{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¬ Neural Network Animations (Real-Time)\n",
    "\n",
    "This notebook provides **real animated visualizations** that work in Jupyter environments:\n",
    "\n",
    "## ğŸš€ What You'll See:\n",
    "1. **ğŸ¬ Animated Forward Propagation** - Watch data flow through the network in real-time\n",
    "2. **ğŸ”„ Animated Backward Propagation** - See gradients flowing backward step by step  \n",
    "3. **ğŸƒâ€â™‚ï¸ Animated Training Progress** - Live training simulation with loss reduction\n",
    "4. **âš–ï¸ Animated Weight Updates** - Watch weights change during optimization\n",
    "5. **ğŸ“Š Complete Animation Sequence** - Full neural network training cycle\n",
    "\n",
    "**Real animations with progress bars, timed updates, and visual feedback!**\n",
    "\n",
    "## ğŸ® How to Use:\n",
    "- Each animation runs automatically with timing\n",
    "- Uses `clear_output()` and `time.sleep()` for smooth transitions\n",
    "- Press Ctrl+C to stop any animation\n",
    "- Call `run_complete_animation_sequence()` for the full experience\n",
    "\n",
    "## âš¡ Features:\n",
    "- âœ… Real-time progress bars\n",
    "- âœ… Animated network diagrams  \n",
    "- âœ… Step-by-step value updates\n",
    "- âœ… Visual gradient flow\n",
    "- âœ… Training loss curves\n",
    "- âœ… Weight update calculations\n",
    "- âœ… No external dependencies (pure Python + IPython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ Simple animation setup complete!\n",
      "ğŸ“± Real animations with pure Python and IPython!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"ğŸ¬ Simple animation setup complete!\")\n",
    "print(\"ğŸ“± Real animations with pure Python and IPython!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Network created successfully!\n"
     ]
    }
   ],
   "source": [
    "class AnimatedNetwork:\n",
    "    def __init__(self):\n",
    "        # Simple 2-3-1 network (using lists instead of numpy)\n",
    "        self.W1 = [[0.5, -0.3], [0.2, 0.7], [-0.4, 0.6]]\n",
    "        self.b1 = [0.1, 0.2, -0.1]\n",
    "        self.W2 = [0.8, -0.5, 0.3]\n",
    "        self.b2 = 0.2\n",
    "        \n",
    "        # Input and target\n",
    "        self.X = [0.8, 0.3]\n",
    "        self.y = 1.0\n",
    "        \n",
    "    def relu(self, z):\n",
    "        if isinstance(z, list):\n",
    "            return [max(0, val) for val in z]\n",
    "        return max(0, z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        if isinstance(z, list):\n",
    "            return [1 / (1 + math.exp(-max(-500, min(500, val)))) for val in z]\n",
    "        return 1 / (1 + math.exp(-max(-500, min(500, z))))\n",
    "    \n",
    "    def matrix_multiply(self, matrix, vector):\n",
    "        \"\"\"Simple matrix-vector multiplication\"\"\"\n",
    "        result = []\n",
    "        for i in range(len(matrix)):\n",
    "            sum_val = 0\n",
    "            for j in range(len(vector)):\n",
    "                sum_val += matrix[i][j] * vector[j]\n",
    "            result.append(sum_val)\n",
    "        return result\n",
    "    \n",
    "    def vector_add(self, vec1, vec2):\n",
    "        \"\"\"Add vector to each element or add bias\"\"\"\n",
    "        if isinstance(vec2, list):\n",
    "            return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "        else:\n",
    "            return [val + vec2 for val in vec1]\n",
    "    \n",
    "    def dot_product(self, vec1, vec2):\n",
    "        \"\"\"Calculate dot product\"\"\"\n",
    "        return sum(vec1[i] * vec2[i] for i in range(len(vec1)))\n",
    "    \n",
    "    def forward_step_by_step(self):\n",
    "        \"\"\"Return each step of forward propagation\"\"\"\n",
    "        steps = {}\n",
    "        \n",
    "        # Step 1: Input\n",
    "        steps['input'] = {'values': self.X, 'description': 'Input values'}\n",
    "        \n",
    "        # Step 2: Hidden layer linear\n",
    "        Z1_temp = self.matrix_multiply(self.W1, self.X)\n",
    "        Z1 = self.vector_add(Z1_temp, self.b1)\n",
    "        steps['hidden_linear'] = {'values': Z1, 'description': 'ZÂ¹ = WÂ¹X + bÂ¹'}\n",
    "        \n",
    "        # Step 3: Hidden layer activation\n",
    "        A1 = self.relu(Z1)\n",
    "        steps['hidden_activation'] = {'values': A1, 'description': 'AÂ¹ = ReLU(ZÂ¹)'}\n",
    "        \n",
    "        # Step 4: Output linear\n",
    "        Z2_temp = self.dot_product(self.W2, A1)\n",
    "        Z2 = Z2_temp + self.b2\n",
    "        steps['output_linear'] = {'values': Z2, 'description': 'ZÂ² = WÂ²AÂ¹ + bÂ²'}\n",
    "        \n",
    "        # Step 5: Output activation\n",
    "        A2 = self.sigmoid(Z2)\n",
    "        steps['output_activation'] = {'values': A2, 'description': 'AÂ² = Ïƒ(ZÂ²)'}\n",
    "        \n",
    "        # Step 6: Loss\n",
    "        loss = -(self.y * math.log(A2 + 1e-8) + (1 - self.y) * math.log(1 - A2 + 1e-8))\n",
    "        steps['loss'] = {'values': loss, 'description': f'Loss = {loss:.4f}'}\n",
    "        \n",
    "        return steps, A1, A2\n",
    "    \n",
    "    def backward_step_by_step(self, A1, A2):\n",
    "        \"\"\"Return each step of backward propagation\"\"\"\n",
    "        steps = {}\n",
    "        \n",
    "        # Step 1: Output gradient\n",
    "        dA2 = -(self.y / (A2 + 1e-8)) + (1 - self.y) / (1 - A2 + 1e-8)\n",
    "        steps['output_grad'] = {'values': dA2, 'description': 'dL/dAÂ² (output gradient)'}\n",
    "        \n",
    "        # Step 2: Output layer backward\n",
    "        dZ2 = dA2 * A2 * (1 - A2)\n",
    "        steps['output_backward'] = {'values': dZ2, 'description': 'dL/dZÂ² = dAÂ² Ã— Ïƒ\\'(ZÂ²)'}\n",
    "        \n",
    "        # Step 3: Hidden gradient\n",
    "        dA1 = [self.W2[i] * dZ2 for i in range(len(self.W2))]\n",
    "        steps['hidden_grad'] = {'values': dA1, 'description': 'dL/dAÂ¹ = WÂ²áµ€ Ã— dZÂ²'}\n",
    "        \n",
    "        # Step 4: Hidden layer backward (simplified)\n",
    "        Z1_temp = self.matrix_multiply(self.W1, self.X)\n",
    "        Z1 = self.vector_add(Z1_temp, self.b1)\n",
    "        dZ1 = [dA1[i] if Z1[i] > 0 else 0 for i in range(len(dA1))]\n",
    "        steps['hidden_backward'] = {'values': dZ1, 'description': 'dL/dZÂ¹ = dAÂ¹ Ã— ReLU\\'(ZÂ¹)'}\n",
    "        \n",
    "        # Step 5: Weight gradients (simplified)\n",
    "        dW2 = [dZ2 * A1[i] for i in range(len(A1))]\n",
    "        dW1 = [[dZ1[i] * self.X[j] for j in range(len(self.X))] for i in range(len(dZ1))]\n",
    "        steps['weight_grads'] = {\n",
    "            'values': {'dW2': dW2, 'dW1': dW1}, \n",
    "            'description': 'Weight gradients computed'\n",
    "        }\n",
    "        \n",
    "        return steps\n",
    "\n",
    "# Create network\n",
    "net = AnimatedNetwork()\n",
    "print(\"ğŸ§  Network created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 1: Forward Propagation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ ANIMATED FORWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 6/6\n",
      "\n",
      "    INPUT       HIDDEN        OUTPUT\n",
      "   ğŸ”µ[X1]  â”€â”€â”€â”€â”€â”€â” ğŸ”µ[H1]  â”€â”€â”€â”€â”€â”\n",
      "         â”Œâ”€â”€â”€â”€â”¼â”€ğŸ”µ[H2]  â”€â”€â”€â”€â”€â”¤ğŸŸ¢[Y]\n",
      "   ğŸ”µ[X2]  â”€â”˜    â””â”€ğŸ”µ[H3]  â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”„ Step 6: Loss Calculation\n",
      "   Loss = 0.5790\n",
      "\n",
      "   Computing: Loss = -[yÃ—ln(A) + (1-y)Ã—ln(1-A)]\n",
      "   Result: Loss=0.5790\n",
      "\n",
      "âœ… Forward propagation complete!\n",
      "\n",
      "ğŸ¯ Final Results:\n",
      "   Prediction: 0.5605\n",
      "   Target: 1.0\n",
      "   Error: 0.4395\n"
     ]
    }
   ],
   "source": [
    "def animate_forward_propagation():\n",
    "    \"\"\"Show animated forward propagation\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¬ ANIMATED FORWARD PROPAGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get forward propagation steps\n",
    "    steps, A1, A2 = net.forward_step_by_step()\n",
    "    \n",
    "    step_names = ['input', 'hidden_linear', 'hidden_activation', 'output_linear', 'output_activation', 'loss']\n",
    "    step_titles = [\n",
    "        'Step 1: Input Data',\n",
    "        'Step 2: Hidden Linear Transform',\n",
    "        'Step 3: Hidden ReLU Activation',\n",
    "        'Step 4: Output Linear Transform', \n",
    "        'Step 5: Output Sigmoid Activation',\n",
    "        'Step 6: Loss Calculation'\n",
    "    ]\n",
    "    \n",
    "    for i, (step_name, title) in enumerate(zip(step_names, step_titles)):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"ğŸ¬ ANIMATED FORWARD PROPAGATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = \"â–ˆ\" * (i + 1) + \"â–‘\" * (len(step_names) - i - 1)\n",
    "        print(f\"Progress: [{progress}] {i+1}/{len(step_names)}\")\n",
    "        print()\n",
    "        \n",
    "        # Network diagram with current step highlighted\n",
    "        print(\"    INPUT       HIDDEN        OUTPUT\")\n",
    "        if step_name == 'input':\n",
    "            print(\"   ğŸŸ¢[X1]  â”€â”€â”€â”€â”€â”€â”  [ H1]  â”€â”€â”€â”€â”€â”\")\n",
    "            print(\"         â”Œâ”€â”€â”€â”€â”¼â”€â”€[ H2]  â”€â”€â”€â”€â”€â”¤ [ Y ]\")\n",
    "            print(\"   ğŸŸ¢[X2]  â”€â”˜    â””â”€â”€[ H3]  â”€â”€â”€â”€â”€â”˜\")\n",
    "        elif step_name in ['hidden_linear', 'hidden_activation']:\n",
    "            print(\"   ğŸ”µ[X1]  â”€â”€â”€â”€â”€â”€â” ğŸŸ¢[H1]  â”€â”€â”€â”€â”€â”\")\n",
    "            print(\"         â”Œâ”€â”€â”€â”€â”¼â”€ğŸŸ¢[H2]  â”€â”€â”€â”€â”€â”¤ [ Y ]\")\n",
    "            print(\"   ğŸ”µ[X2]  â”€â”˜    â””â”€ğŸŸ¢[H3]  â”€â”€â”€â”€â”€â”˜\")\n",
    "        else:\n",
    "            print(\"   ğŸ”µ[X1]  â”€â”€â”€â”€â”€â”€â” ğŸ”µ[H1]  â”€â”€â”€â”€â”€â”\")\n",
    "            print(\"         â”Œâ”€â”€â”€â”€â”¼â”€ğŸ”µ[H2]  â”€â”€â”€â”€â”€â”¤ğŸŸ¢[Y]\")\n",
    "            print(\"   ğŸ”µ[X2]  â”€â”˜    â””â”€ğŸ”µ[H3]  â”€â”€â”€â”€â”€â”˜\")\n",
    "        print()\n",
    "        \n",
    "        # Current step information\n",
    "        print(f\"ğŸ”„ {title}\")\n",
    "        print(f\"   {steps[step_name]['description']}\")\n",
    "        print()\n",
    "        \n",
    "        # Show values\n",
    "        if step_name == 'input':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Input Values: X1={values[0]:.3f}, X2={values[1]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'hidden_linear':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: ZÂ¹ = WÂ¹ Ã— X + bÂ¹\")\n",
    "            print(f\"   Results: Z1={values[0]:.3f}, Z2={values[1]:.3f}, Z3={values[2]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'hidden_activation':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Applying: ReLU(z) = max(0, z)\")\n",
    "            print(f\"   Results: A1={values[0]:.3f}, A2={values[1]:.3f}, A3={values[2]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'output_linear':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: ZÂ² = WÂ² Ã— AÂ¹ + bÂ²\")\n",
    "            print(f\"   Result: Z={values:.3f}\")\n",
    "            \n",
    "        elif step_name == 'output_activation':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Applying: Ïƒ(z) = 1/(1+e^(-z))\")\n",
    "            print(f\"   Result: A={values:.3f}\")\n",
    "            \n",
    "        elif step_name == 'loss':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: Loss = -[yÃ—ln(A) + (1-y)Ã—ln(1-A)]\")\n",
    "            print(f\"   Result: Loss={values:.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"â³ Processing...\" if i < len(step_names) - 1 else \"âœ… Forward propagation complete!\")\n",
    "        \n",
    "        time.sleep(2)  # Animation delay\n",
    "    \n",
    "    print()\n",
    "    print(f\"ğŸ¯ Final Results:\")\n",
    "    print(f\"   Prediction: {A2:.4f}\")\n",
    "    print(f\"   Target: {net.y}\")\n",
    "    print(f\"   Error: {abs(net.y - A2):.4f}\")\n",
    "    \n",
    "    return steps, A1, A2\n",
    "\n",
    "# Run animated forward propagation\n",
    "print(\"ğŸ¬ Starting animated forward propagation...\")\n",
    "forward_steps, A1, A2 = animate_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 2: Backward Propagation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ANIMATED BACKWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 5/5\n",
      "\n",
      "    â†â†â† GRADIENT FLOW â†â†â†\n",
      "\n",
      "    OUTPUT      HIDDEN        INPUT\n",
      "    ğŸ”µ[dY] â†â”€â”€â”€â”€â”€â” ğŸ”µ[dH1] â†â”€â”€â”€â”€â”\n",
      "           â”Œâ”€â”€â”€â”¼â”€ğŸ”µ[dH2] â†â”€â”€â”€â”€â”¤ğŸ”´[dX1]\n",
      "        â†â”€â”€â”˜   â””â”€ğŸ”µ[dH3] â†â”€â”€â”€â”€â”˜ğŸ”´[dX2]\n",
      "\n",
      "ğŸ”„ Step 5: Weight Gradient Computation\n",
      "   Weight gradients computed\n",
      "\n",
      "   Weight Gradients computed!\n",
      "   dW2: [-0.1802, -0.2505, -0.0000]\n",
      "   dW1: Ready for weight updates\n",
      "   Formula: dW = dZ Ã— A_previousáµ€\n",
      "\n",
      "âœ… All gradients computed!\n",
      "\n",
      "ğŸ¯ Backward Propagation Complete!\n",
      "   All weight gradients ready for optimization step\n"
     ]
    }
   ],
   "source": [
    "def animate_backward_propagation():\n",
    "    \"\"\"Show animated backward propagation\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ ANIMATED BACKWARD PROPAGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get backward propagation steps\n",
    "    steps = net.backward_step_by_step(A1, A2)\n",
    "    \n",
    "    step_names = ['output_grad', 'output_backward', 'hidden_grad', 'hidden_backward', 'weight_grads']\n",
    "    step_titles = [\n",
    "        'Step 1: Output Gradient Calculation',\n",
    "        'Step 2: Output Layer Backward Pass',\n",
    "        'Step 3: Hidden Layer Gradient Flow',\n",
    "        'Step 4: Hidden Layer Backward Pass',\n",
    "        'Step 5: Weight Gradient Computation'\n",
    "    ]\n",
    "    \n",
    "    for i, (step_name, title) in enumerate(zip(step_names, step_titles)):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"ğŸ”„ ANIMATED BACKWARD PROPAGATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = \"â–ˆ\" * (i + 1) + \"â–‘\" * (len(step_names) - i - 1)\n",
    "        print(f\"Progress: [{progress}] {i+1}/{len(step_names)}\")\n",
    "        print()\n",
    "        \n",
    "        # Network diagram with gradient flow\n",
    "        print(\"    â†â†â† GRADIENT FLOW â†â†â†\")\n",
    "        print()\n",
    "        print(\"    OUTPUT      HIDDEN        INPUT\")\n",
    "        if step_name in ['output_grad', 'output_backward']:\n",
    "            print(\"    ğŸ”´[dY] â†â”€â”€â”€â”€â”€â” [ dH1] â†â”€â”€â”€â”€â”\")\n",
    "            print(\"           â”Œâ”€â”€â”€â”¼â”€[ dH2] â†â”€â”€â”€â”€â”¤ [dX1]\")\n",
    "            print(\"        â†â”€â”€â”˜   â””â”€[ dH3] â†â”€â”€â”€â”€â”˜ [dX2]\")\n",
    "        elif step_name in ['hidden_grad', 'hidden_backward']:\n",
    "            print(\"    ğŸ”µ[dY] â†â”€â”€â”€â”€â”€â” ğŸ”´[dH1] â†â”€â”€â”€â”€â”\")\n",
    "            print(\"           â”Œâ”€â”€â”€â”¼â”€ğŸ”´[dH2] â†â”€â”€â”€â”€â”¤ [dX1]\")\n",
    "            print(\"        â†â”€â”€â”˜   â””â”€ğŸ”´[dH3] â†â”€â”€â”€â”€â”˜ [dX2]\")\n",
    "        else:\n",
    "            print(\"    ğŸ”µ[dY] â†â”€â”€â”€â”€â”€â” ğŸ”µ[dH1] â†â”€â”€â”€â”€â”\")\n",
    "            print(\"           â”Œâ”€â”€â”€â”¼â”€ğŸ”µ[dH2] â†â”€â”€â”€â”€â”¤ğŸ”´[dX1]\")\n",
    "            print(\"        â†â”€â”€â”˜   â””â”€ğŸ”µ[dH3] â†â”€â”€â”€â”€â”˜ğŸ”´[dX2]\")\n",
    "        print()\n",
    "        \n",
    "        # Current step information\n",
    "        print(f\"ğŸ”„ {title}\")\n",
    "        print(f\"   {steps[step_name]['description']}\")\n",
    "        print()\n",
    "        \n",
    "        # Show gradient values\n",
    "        if step_name == 'output_grad':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Output Gradient: dL/dAÂ² = {values:.4f}\")\n",
    "            print(f\"   This measures how loss changes with output activation\")\n",
    "            \n",
    "        elif step_name == 'output_backward':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Output dZÂ²: {values:.4f}\")\n",
    "            print(f\"   Chain rule applied: dL/dZÂ² = dL/dAÂ² Ã— dAÂ²/dZÂ²\")\n",
    "            \n",
    "        elif step_name == 'hidden_grad':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Hidden Gradients flowing back:\")\n",
    "            print(f\"   dA1={values[0]:.4f}, dA2={values[1]:.4f}, dA3={values[2]:.4f}\")\n",
    "            print(f\"   Computed via: dL/dAÂ¹ = WÂ²áµ€ Ã— dL/dZÂ²\")\n",
    "            \n",
    "        elif step_name == 'hidden_backward':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Hidden dZÂ¹ values:\")\n",
    "            print(f\"   dZ1={values[0]:.4f}, dZ2={values[1]:.4f}, dZ3={values[2]:.4f}\")\n",
    "            print(f\"   ReLU derivative applied: dL/dZÂ¹ = dL/dAÂ¹ Ã— ReLU'(ZÂ¹)\")\n",
    "            \n",
    "        elif step_name == 'weight_grads':\n",
    "            dW2 = steps[step_name]['values']['dW2']\n",
    "            dW1 = steps[step_name]['values']['dW1']\n",
    "            print(f\"   Weight Gradients computed!\")\n",
    "            print(f\"   dW2: [{dW2[0]:.4f}, {dW2[1]:.4f}, {dW2[2]:.4f}]\")\n",
    "            print(f\"   dW1: Ready for weight updates\")\n",
    "            print(f\"   Formula: dW = dZ Ã— A_previousáµ€\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Show gradient flow animation\n",
    "        if i < len(step_names) - 1:\n",
    "            print(\"âš¡ Gradients flowing backward...\")\n",
    "            for j in range(3):\n",
    "                print(\"   \" + \"âš¡\" * (j + 1) + \" \" * (10 - j))\n",
    "                time.sleep(0.3)\n",
    "                if j < 2:\n",
    "                    # Move cursor up to overwrite\n",
    "                    print(\"\\033[A\\033[K\", end=\"\")\n",
    "        else:\n",
    "            print(\"âœ… All gradients computed!\")\n",
    "        \n",
    "        time.sleep(1.5)  # Animation delay\n",
    "    \n",
    "    print()\n",
    "    print(\"ğŸ¯ Backward Propagation Complete!\")\n",
    "    print(\"   All weight gradients ready for optimization step\")\n",
    "    \n",
    "    return steps\n",
    "\n",
    "# Run animated backward propagation\n",
    "print(\"ğŸ”„ Starting animated backward propagation...\")\n",
    "backward_steps = animate_backward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 3: Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒâ€â™‚ï¸ ANIMATED TRAINING SIMULATION\n",
      "==================================================\n",
      "Epoch Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 9/10\n",
      "\n",
      "ğŸ”„ Epoch 9\n",
      "   Current Loss: 1.5491\n",
      "   Gradient: 0.2366\n",
      "   Learning Rate: 0.1\n",
      "\n",
      "Loss Visualization: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1.549\n",
      "\n",
      "Trend: ğŸ“‰ Decreasing (Î” 0.0282)\n",
      "\n",
      "âš¡ Updating weights...\n",
      "   Weights: âš¡     \n",
      "\u001b[A\u001b[K   Weights: âš¡âš¡    \n",
      "\u001b[A\u001b[K   Weights: âš¡âš¡âš¡   \n",
      "\n",
      "   Status: âš ï¸  Slow convergence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def animate_training_progress():\n",
    "    \"\"\"Show animated training simulation\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"ğŸƒâ€â™‚ï¸ ANIMATED TRAINING SIMULATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    epochs = 10\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Initial Loss: calculating...\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Simulate training\n",
    "    current_loss = 2.0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"ğŸƒâ€â™‚ï¸ ANIMATED TRAINING SIMULATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Training progress bar\n",
    "        progress = \"â–ˆ\" * (epoch + 1) + \"â–‘\" * (epochs - epoch - 1)\n",
    "        print(f\"Epoch Progress: [{progress}] {epoch+1}/{epochs}\")\n",
    "        print()\n",
    "        \n",
    "        # Simulate gradient descent step\n",
    "        gradient = current_loss * 0.15  # Simulate gradient\n",
    "        current_loss -= learning_rate * gradient\n",
    "        current_loss = max(0.01, current_loss)\n",
    "        \n",
    "        # Add some training noise\n",
    "        if epoch > 1:\n",
    "            noise = random.uniform(-0.05, 0.02)\n",
    "            current_loss += noise\n",
    "            current_loss = max(0.01, current_loss)\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Show current training state\n",
    "        print(f\"ğŸ”„ Epoch {epoch + 1}\")\n",
    "        print(f\"   Current Loss: {current_loss:.4f}\")\n",
    "        print(f\"   Gradient: {gradient:.4f}\")\n",
    "        print(f\"   Learning Rate: {learning_rate}\")\n",
    "        print()\n",
    "        \n",
    "        # Visual loss representation\n",
    "        loss_bars = int(current_loss * 20)  # Scale for visualization\n",
    "        loss_visual = \"â–ˆ\" * max(1, loss_bars) + \"â–‘\" * max(0, 20 - loss_bars)\n",
    "        print(f\"Loss Visualization: [{loss_visual}] {current_loss:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        # Show loss trend\n",
    "        if epoch > 0:\n",
    "            trend = \"ğŸ“‰ Decreasing\" if losses[epoch] < losses[epoch-1] else \"ğŸ“ˆ Increasing\"\n",
    "            change = abs(losses[epoch] - losses[epoch-1])\n",
    "            print(f\"Trend: {trend} (Î” {change:.4f})\")\n",
    "        else:\n",
    "            print(\"Trend: Starting training...\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Animation of weight updates\n",
    "        print(\"âš¡ Updating weights...\")\n",
    "        for i in range(3):\n",
    "            update_visual = \"âš¡\" * (i + 1) + \" \" * (5 - i)\n",
    "            print(f\"   Weights: {update_visual}\")\n",
    "            time.sleep(0.4)\n",
    "            if i < 2:\n",
    "                print(\"\\033[A\\033[K\", end=\"\")  # Move cursor up and clear line\n",
    "        \n",
    "        # Status message\n",
    "        if current_loss < 0.1:\n",
    "            status = \"ğŸ¯ Converging well!\"\n",
    "        elif current_loss < 0.5:\n",
    "            status = \"ğŸ“ˆ Making progress\"\n",
    "        elif epoch < 3:\n",
    "            status = \"ğŸš€ Starting to learn\"\n",
    "        else:\n",
    "            status = \"âš ï¸  Slow convergence\"\n",
    "        \n",
    "        print(f\"\\n   Status: {status}\")\n",
    "        print()\n",
    "        \n",
    "        time.sleep(1.5)  # Training step delay\n",
    "    \n",
    "    # Final results\n",
    "    clear_output(wait=True)\n",
    "    print(\"ğŸƒâ€â™‚ï¸ TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"ğŸ“Š Training Results:\")\n",
    "    print(f\"   Initial Loss: {losses[0]:.4f}\")\n",
    "    print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"   Total Improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"   Reduction: {((losses[0] - losses[-1]) / losses[0]) * 100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Show loss history\n",
    "    print(\"ğŸ“ˆ Loss History:\")\n",
    "    for i, loss in enumerate(losses):\n",
    "        bar_length = int((1 - loss / losses[0]) * 20)\n",
    "        bar = \"â–ˆ\" * bar_length + \"â–‘\" * (20 - bar_length)\n",
    "        print(f\"   Epoch {i+1:2d}: [{bar}] {loss:.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"âœ… Training animation complete!\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Run animated training\n",
    "print(\"ğŸƒâ€â™‚ï¸ Starting animated training...\")\n",
    "training_losses = animate_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 4: Weight Update Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ WEIGHT UPDATE COMPLETE!\n",
      "==================================================\n",
      "ğŸ“Š BEFORE vs AFTER Comparison:\n",
      "\n",
      "ğŸ”¹ Layer 1 Weights:\n",
      "   BEFORE â†’ AFTER\n",
      "   Row 1: [ 0.500, -0.300] â†’ [ 0.528, -0.289]\n",
      "   Row 2: [ 0.200,  0.700] â†’ [ 0.182,  0.693]\n",
      "   Row 3: [-0.400,  0.600] â†’ [-0.400,  0.600]\n",
      "\n",
      "ğŸ”¸ Layer 2 Weights:\n",
      "   BEFORE â†’ AFTER\n",
      "   [ 0.800, -0.500,  0.300] â†’ [ 0.818, -0.475,  0.300]\n",
      "\n",
      "ğŸ“ˆ Update Statistics:\n",
      "   Total W1 change: 0.0629\n",
      "   Total W2 change: 0.0431\n",
      "   Learning rate: 0.1\n",
      "   ğŸ“Š Significant updates - network is learning!\n",
      "\n",
      "âœ… Weight updates complete! Network ready for next iteration.\n"
     ]
    }
   ],
   "source": [
    "def animate_weight_updates():\n",
    "    \"\"\"Show animated weight update process\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"âš–ï¸ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get gradients\n",
    "    steps = net.backward_step_by_step(A1, A2)\n",
    "    dW1 = steps['weight_grads']['values']['dW1']\n",
    "    dW2 = steps['weight_grads']['values']['dW2']\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    print(\"Starting weight update process...\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 1: Show original weights\n",
    "    clear_output(wait=True)\n",
    "    print(\"âš–ï¸ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 1/3: Original Weights\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”¹ Layer 1 Weights (W1):\")\n",
    "    for i, row in enumerate(net.W1):\n",
    "        print(f\"   Row {i+1}: [{row[0]:6.3f}, {row[1]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Layer 2 Weights (W2):\")\n",
    "    print(f\"   [{net.W2[0]:6.3f}, {net.W2[1]:6.3f}, {net.W2[2]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\nâ³ Preparing gradients...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 2: Show gradients\n",
    "    clear_output(wait=True)\n",
    "    print(\"âš–ï¸ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 2/3: Computed Gradients\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”¹ Gradients for W1 (dW1):\")\n",
    "    for i, row in enumerate(dW1):\n",
    "        # Animate gradient appearance\n",
    "        for j in range(len(row) + 1):\n",
    "            print(f\"\\r   Row {i+1}: [\", end=\"\")\n",
    "            for k in range(j):\n",
    "                if k < len(row):\n",
    "                    print(f\"{row[k]:6.3f}\", end=\"\")\n",
    "                    if k < len(row) - 1:\n",
    "                        print(\", \", end=\"\")\n",
    "            print(\"]\" + \" \" * 10, end=\"\")\n",
    "            time.sleep(0.3)\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Gradients for W2 (dW2):\")\n",
    "    print(\"   [\", end=\"\")\n",
    "    for i, val in enumerate(dW2):\n",
    "        print(f\"{val:6.3f}\", end=\"\")\n",
    "        if i < len(dW2) - 1:\n",
    "            print(\", \", end=\"\")\n",
    "        time.sleep(0.3)\n",
    "    print(\"]\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Update Formula: W_new = W_old - {learning_rate} Ã— gradient\")\n",
    "    print(\"â³ Applying updates...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 3: Animated weight updates\n",
    "    clear_output(wait=True)\n",
    "    print(\"âš–ï¸ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 3/3: Weight Updates in Progress\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate new weights\n",
    "    W1_new = []\n",
    "    W2_new = []\n",
    "    \n",
    "    print(\"ğŸ”¹ Updating W1...\")\n",
    "    for i in range(len(net.W1)):\n",
    "        row_new = []\n",
    "        for j in range(len(net.W1[i])):\n",
    "            old_val = net.W1[i][j]\n",
    "            grad_val = dW1[i][j]\n",
    "            new_val = old_val - learning_rate * grad_val\n",
    "            row_new.append(new_val)\n",
    "            \n",
    "            # Show the update calculation\n",
    "            change = abs(old_val - new_val)\n",
    "            direction = \"â†“\" if old_val > new_val else \"â†‘\"\n",
    "            \n",
    "            print(f\"   W1[{i+1},{j+1}]: {old_val:.3f} - {learning_rate}Ã—{grad_val:.3f} = {new_val:.3f} {direction}{change:.3f}\")\n",
    "            time.sleep(0.5)\n",
    "        W1_new.append(row_new)\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Updating W2...\")\n",
    "    for i in range(len(net.W2)):\n",
    "        old_val = net.W2[i]\n",
    "        grad_val = dW2[i]\n",
    "        new_val = old_val - learning_rate * grad_val\n",
    "        W2_new.append(new_val)\n",
    "        \n",
    "        change = abs(old_val - new_val)\n",
    "        direction = \"â†“\" if old_val > new_val else \"â†‘\"\n",
    "        \n",
    "        print(f\"   W2[{i+1}]: {old_val:.3f} - {learning_rate}Ã—{grad_val:.3f} = {new_val:.3f} {direction}{change:.3f}\")\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Final comparison\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    print(\"âš–ï¸ WEIGHT UPDATE COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"ğŸ“Š BEFORE vs AFTER Comparison:\")\n",
    "    print()\n",
    "    print(\"ğŸ”¹ Layer 1 Weights:\")\n",
    "    print(\"   BEFORE â†’ AFTER\")\n",
    "    for i in range(len(net.W1)):\n",
    "        old_row = net.W1[i]\n",
    "        new_row = W1_new[i]\n",
    "        print(f\"   Row {i+1}: [{old_row[0]:6.3f}, {old_row[1]:6.3f}] â†’ [{new_row[0]:6.3f}, {new_row[1]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Layer 2 Weights:\")\n",
    "    print(\"   BEFORE â†’ AFTER\")\n",
    "    old_W2 = net.W2\n",
    "    print(f\"   [{old_W2[0]:6.3f}, {old_W2[1]:6.3f}, {old_W2[2]:6.3f}] â†’ [{W2_new[0]:6.3f}, {W2_new[1]:6.3f}, {W2_new[2]:6.3f}]\")\n",
    "    \n",
    "    # Calculate update statistics\n",
    "    total_change_W1 = sum(sum(abs(net.W1[i][j] - W1_new[i][j]) for j in range(len(net.W1[i]))) for i in range(len(net.W1)))\n",
    "    total_change_W2 = sum(abs(net.W2[i] - W2_new[i]) for i in range(len(net.W2)))\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Update Statistics:\")\n",
    "    print(f\"   Total W1 change: {total_change_W1:.4f}\")\n",
    "    print(f\"   Total W2 change: {total_change_W2:.4f}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    \n",
    "    if total_change_W1 + total_change_W2 > 0.1:\n",
    "        print(f\"   ğŸ“Š Significant updates - network is learning!\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“Š Small updates - near convergence or low learning rate\")\n",
    "    \n",
    "    print(\"\\nâœ… Weight updates complete! Network ready for next iteration.\")\n",
    "    \n",
    "    return W1_new, W2_new\n",
    "\n",
    "# Run animated weight updates\n",
    "print(\"âš–ï¸ Starting animated weight updates...\")\n",
    "new_W1, new_W2 = animate_weight_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ ANIMATED FORWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 6/6\n",
      "\n",
      "    INPUT       HIDDEN        OUTPUT\n",
      "   ğŸ”µ[X1]  â”€â”€â”€â”€â”€â”€â” ğŸ”µ[H1]  â”€â”€â”€â”€â”€â”\n",
      "         â”Œâ”€â”€â”€â”€â”¼â”€ğŸ”µ[H2]  â”€â”€â”€â”€â”€â”¤ğŸŸ¢[Y]\n",
      "   ğŸ”µ[X2]  â”€â”˜    â””â”€ğŸ”µ[H3]  â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”„ Step 6: Loss Calculation\n",
      "   Loss = 0.5790\n",
      "\n",
      "   Computing: Loss = -[yÃ—ln(A) + (1-y)Ã—ln(1-A)]\n",
      "   Result: Loss=0.5790\n",
      "\n",
      "âœ… Forward propagation complete!\n",
      "\n",
      "ğŸ¯ Final Results:\n",
      "   Prediction: 0.5605\n",
      "   Target: 1.0\n",
      "   Error: 0.4395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input': {'values': [0.8, 0.3], 'description': 'Input values'},\n",
       "  'hidden_linear': {'values': [0.41000000000000003,\n",
       "    0.5700000000000001,\n",
       "    -0.24000000000000007],\n",
       "   'description': 'ZÂ¹ = WÂ¹X + bÂ¹'},\n",
       "  'hidden_activation': {'values': [0.41000000000000003, 0.5700000000000001, 0],\n",
       "   'description': 'AÂ¹ = ReLU(ZÂ¹)'},\n",
       "  'output_linear': {'values': 0.24300000000000005,\n",
       "   'description': 'ZÂ² = WÂ²AÂ¹ + bÂ²'},\n",
       "  'output_activation': {'values': 0.5604528191374981,\n",
       "   'description': 'AÂ² = Ïƒ(ZÂ²)'},\n",
       "  'loss': {'values': 0.5790101985529353, 'description': 'Loss = 0.5790'}},\n",
       " [0.41000000000000003, 0.5700000000000001, 0],\n",
       " 0.5604528191374981)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_complete_animation_sequence():\n",
    "    \"\"\"Run all animations in sequence for a complete neural network demo\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"ğŸ¬ COMPLETE NEURAL NETWORK ANIMATION SEQUENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"This demo will show you:\")\n",
    "    print(\"  1. ğŸ¬ Animated Forward Propagation\")\n",
    "    print(\"  2. ğŸ”„ Animated Backward Propagation\") \n",
    "    print(\"  3. ğŸƒâ€â™‚ï¸ Animated Training Process\")\n",
    "    print(\"  4. âš–ï¸ Animated Weight Updates\")\n",
    "    print(\"  5. ğŸ“Š Final Summary\")\n",
    "    print()\n",
    "    print(\"Each animation will run automatically...\")\n",
    "    print(\"Press Ctrl+C at any time to stop\")\n",
    "    print()\n",
    "    input(\"Press Enter to start the complete animation sequence...\")\n",
    "    \n",
    "    try:\n",
    "        # Animation 1: Forward Propagation\n",
    "        print(\"ğŸ¬ Starting Animation 1/4: Forward Propagation...\")\n",
    "        time.sleep(1)\n",
    "        forward_steps, A1, A2 = animate_forward_propagation()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Backward Propagation...\")\n",
    "        \n",
    "        # Animation 2: Backward Propagation\n",
    "        print(\"ğŸ”„ Starting Animation 2/4: Backward Propagation...\")\n",
    "        time.sleep(1)\n",
    "        backward_steps = animate_backward_propagation()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Training Simulation...\")\n",
    "        \n",
    "        # Animation 3: Training Process\n",
    "        print(\"ğŸƒâ€â™‚ï¸ Starting Animation 3/4: Training Process...\")\n",
    "        time.sleep(1)\n",
    "        training_losses = animate_training_progress()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Weight Updates...\")\n",
    "        \n",
    "        # Animation 4: Weight Updates\n",
    "        print(\"âš–ï¸ Starting Animation 4/4: Weight Updates...\")\n",
    "        time.sleep(1)\n",
    "        new_W1, new_W2 = animate_weight_updates()\n",
    "        \n",
    "        # Final Summary\n",
    "        clear_output(wait=True)\n",
    "        print(\"ğŸ‰ COMPLETE ANIMATION SEQUENCE FINISHED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "        print(\"ğŸ¯ What you just learned:\")\n",
    "        print(\"  âœ… How neural networks process information forward\")\n",
    "        print(\"  âœ… How gradients flow backward through the network\")\n",
    "        print(\"  âœ… How training reduces loss over time\")\n",
    "        print(\"  âœ… How weights get updated to improve performance\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸ“Š Key Takeaways:\")\n",
    "        print(f\"  â€¢ Network processed input [{net.X[0]}, {net.X[1]}] â†’ output {A2:.4f}\")\n",
    "        print(f\"  â€¢ Target was {net.y}, so error was {abs(net.y - A2):.4f}\")\n",
    "        print(f\"  â€¢ Gradients computed automatically via backpropagation\")\n",
    "        print(f\"  â€¢ Training simulation showed loss decreasing over time\")\n",
    "        print(f\"  â€¢ Weights updated using gradient descent optimization\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸš€ Next Steps:\")\n",
    "        print(\"  1. Try different learning rates in the training function\")\n",
    "        print(\"  2. Modify the network architecture (more neurons/layers)\")\n",
    "        print(\"  3. Change the input data and see how it affects propagation\")\n",
    "        print(\"  4. Experiment with different activation functions\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸ¬ All animations complete! Neural network concepts mastered!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        clear_output(wait=True)\n",
    "        print(\"â¹ï¸ Animation sequence stopped by user\")\n",
    "        print(\"You can run individual animations by calling their functions:\")\n",
    "        print(\"  â€¢ animate_forward_propagation()\")\n",
    "        print(\"  â€¢ animate_backward_propagation()\")\n",
    "        print(\"  â€¢ animate_training_progress()\")\n",
    "        print(\"  â€¢ animate_weight_updates()\")\n",
    "\n",
    "# Run complete animation sequence\n",
    "print(\"ğŸ¬ Complete animation sequence ready!\")\n",
    "print(\"Call run_complete_animation_sequence() to see all animations!\")\n",
    "\n",
    "# For immediate testing, run the first animation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ¬ DEMO: Running Forward Propagation Animation\")\n",
    "print(\"=\"*50)\n",
    "animate_forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
