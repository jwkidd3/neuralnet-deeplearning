{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎬 Neural Network Animations (Real-Time)\n",
    "\n",
    "This notebook provides **real animated visualizations** that work in Jupyter environments:\n",
    "\n",
    "## 🚀 What You'll See:\n",
    "1. **🎬 Animated Forward Propagation** - Watch data flow through the network in real-time\n",
    "2. **🔄 Animated Backward Propagation** - See gradients flowing backward step by step  \n",
    "3. **🏃‍♂️ Animated Training Progress** - Live training simulation with loss reduction\n",
    "4. **⚖️ Animated Weight Updates** - Watch weights change during optimization\n",
    "5. **📊 Complete Animation Sequence** - Full neural network training cycle\n",
    "\n",
    "**Real animations with progress bars, timed updates, and visual feedback!**\n",
    "\n",
    "## 🎮 How to Use:\n",
    "- Each animation runs automatically with timing\n",
    "- Uses `clear_output()` and `time.sleep()` for smooth transitions\n",
    "- Press Ctrl+C to stop any animation\n",
    "- Call `run_complete_animation_sequence()` for the full experience\n",
    "\n",
    "## ⚡ Features:\n",
    "- ✅ Real-time progress bars\n",
    "- ✅ Animated network diagrams  \n",
    "- ✅ Step-by-step value updates\n",
    "- ✅ Visual gradient flow\n",
    "- ✅ Training loss curves\n",
    "- ✅ Weight update calculations\n",
    "- ✅ No external dependencies (pure Python + IPython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Simple animation setup complete!\n",
      "📱 Real animations with pure Python and IPython!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"🎬 Simple animation setup complete!\")\n",
    "print(\"📱 Real animations with pure Python and IPython!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Network created successfully!\n"
     ]
    }
   ],
   "source": [
    "class AnimatedNetwork:\n",
    "    def __init__(self):\n",
    "        # Simple 2-3-1 network (using lists instead of numpy)\n",
    "        self.W1 = [[0.5, -0.3], [0.2, 0.7], [-0.4, 0.6]]\n",
    "        self.b1 = [0.1, 0.2, -0.1]\n",
    "        self.W2 = [0.8, -0.5, 0.3]\n",
    "        self.b2 = 0.2\n",
    "        \n",
    "        # Input and target\n",
    "        self.X = [0.8, 0.3]\n",
    "        self.y = 1.0\n",
    "        \n",
    "    def relu(self, z):\n",
    "        if isinstance(z, list):\n",
    "            return [max(0, val) for val in z]\n",
    "        return max(0, z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        if isinstance(z, list):\n",
    "            return [1 / (1 + math.exp(-max(-500, min(500, val)))) for val in z]\n",
    "        return 1 / (1 + math.exp(-max(-500, min(500, z))))\n",
    "    \n",
    "    def matrix_multiply(self, matrix, vector):\n",
    "        \"\"\"Simple matrix-vector multiplication\"\"\"\n",
    "        result = []\n",
    "        for i in range(len(matrix)):\n",
    "            sum_val = 0\n",
    "            for j in range(len(vector)):\n",
    "                sum_val += matrix[i][j] * vector[j]\n",
    "            result.append(sum_val)\n",
    "        return result\n",
    "    \n",
    "    def vector_add(self, vec1, vec2):\n",
    "        \"\"\"Add vector to each element or add bias\"\"\"\n",
    "        if isinstance(vec2, list):\n",
    "            return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "        else:\n",
    "            return [val + vec2 for val in vec1]\n",
    "    \n",
    "    def dot_product(self, vec1, vec2):\n",
    "        \"\"\"Calculate dot product\"\"\"\n",
    "        return sum(vec1[i] * vec2[i] for i in range(len(vec1)))\n",
    "    \n",
    "    def forward_step_by_step(self):\n",
    "        \"\"\"Return each step of forward propagation\"\"\"\n",
    "        steps = {}\n",
    "        \n",
    "        # Step 1: Input\n",
    "        steps['input'] = {'values': self.X, 'description': 'Input values'}\n",
    "        \n",
    "        # Step 2: Hidden layer linear\n",
    "        Z1_temp = self.matrix_multiply(self.W1, self.X)\n",
    "        Z1 = self.vector_add(Z1_temp, self.b1)\n",
    "        steps['hidden_linear'] = {'values': Z1, 'description': 'Z¹ = W¹X + b¹'}\n",
    "        \n",
    "        # Step 3: Hidden layer activation\n",
    "        A1 = self.relu(Z1)\n",
    "        steps['hidden_activation'] = {'values': A1, 'description': 'A¹ = ReLU(Z¹)'}\n",
    "        \n",
    "        # Step 4: Output linear\n",
    "        Z2_temp = self.dot_product(self.W2, A1)\n",
    "        Z2 = Z2_temp + self.b2\n",
    "        steps['output_linear'] = {'values': Z2, 'description': 'Z² = W²A¹ + b²'}\n",
    "        \n",
    "        # Step 5: Output activation\n",
    "        A2 = self.sigmoid(Z2)\n",
    "        steps['output_activation'] = {'values': A2, 'description': 'A² = σ(Z²)'}\n",
    "        \n",
    "        # Step 6: Loss\n",
    "        loss = -(self.y * math.log(A2 + 1e-8) + (1 - self.y) * math.log(1 - A2 + 1e-8))\n",
    "        steps['loss'] = {'values': loss, 'description': f'Loss = {loss:.4f}'}\n",
    "        \n",
    "        return steps, A1, A2\n",
    "    \n",
    "    def backward_step_by_step(self, A1, A2):\n",
    "        \"\"\"Return each step of backward propagation\"\"\"\n",
    "        steps = {}\n",
    "        \n",
    "        # Step 1: Output gradient\n",
    "        dA2 = -(self.y / (A2 + 1e-8)) + (1 - self.y) / (1 - A2 + 1e-8)\n",
    "        steps['output_grad'] = {'values': dA2, 'description': 'dL/dA² (output gradient)'}\n",
    "        \n",
    "        # Step 2: Output layer backward\n",
    "        dZ2 = dA2 * A2 * (1 - A2)\n",
    "        steps['output_backward'] = {'values': dZ2, 'description': 'dL/dZ² = dA² × σ\\'(Z²)'}\n",
    "        \n",
    "        # Step 3: Hidden gradient\n",
    "        dA1 = [self.W2[i] * dZ2 for i in range(len(self.W2))]\n",
    "        steps['hidden_grad'] = {'values': dA1, 'description': 'dL/dA¹ = W²ᵀ × dZ²'}\n",
    "        \n",
    "        # Step 4: Hidden layer backward (simplified)\n",
    "        Z1_temp = self.matrix_multiply(self.W1, self.X)\n",
    "        Z1 = self.vector_add(Z1_temp, self.b1)\n",
    "        dZ1 = [dA1[i] if Z1[i] > 0 else 0 for i in range(len(dA1))]\n",
    "        steps['hidden_backward'] = {'values': dZ1, 'description': 'dL/dZ¹ = dA¹ × ReLU\\'(Z¹)'}\n",
    "        \n",
    "        # Step 5: Weight gradients (simplified)\n",
    "        dW2 = [dZ2 * A1[i] for i in range(len(A1))]\n",
    "        dW1 = [[dZ1[i] * self.X[j] for j in range(len(self.X))] for i in range(len(dZ1))]\n",
    "        steps['weight_grads'] = {\n",
    "            'values': {'dW2': dW2, 'dW1': dW1}, \n",
    "            'description': 'Weight gradients computed'\n",
    "        }\n",
    "        \n",
    "        return steps\n",
    "\n",
    "# Create network\n",
    "net = AnimatedNetwork()\n",
    "print(\"🧠 Network created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 1: Forward Propagation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 ANIMATED FORWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [██████] 6/6\n",
      "\n",
      "    INPUT       HIDDEN        OUTPUT\n",
      "   🔵[X1]  ──────┐ 🔵[H1]  ─────┐\n",
      "         ┌────┼─🔵[H2]  ─────┤🟢[Y]\n",
      "   🔵[X2]  ─┘    └─🔵[H3]  ─────┘\n",
      "\n",
      "🔄 Step 6: Loss Calculation\n",
      "   Loss = 0.5790\n",
      "\n",
      "   Computing: Loss = -[y×ln(A) + (1-y)×ln(1-A)]\n",
      "   Result: Loss=0.5790\n",
      "\n",
      "✅ Forward propagation complete!\n",
      "\n",
      "🎯 Final Results:\n",
      "   Prediction: 0.5605\n",
      "   Target: 1.0\n",
      "   Error: 0.4395\n"
     ]
    }
   ],
   "source": [
    "def animate_forward_propagation():\n",
    "    \"\"\"Show animated forward propagation\"\"\"\n",
    "    \n",
    "    print(\"🎬 ANIMATED FORWARD PROPAGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get forward propagation steps\n",
    "    steps, A1, A2 = net.forward_step_by_step()\n",
    "    \n",
    "    step_names = ['input', 'hidden_linear', 'hidden_activation', 'output_linear', 'output_activation', 'loss']\n",
    "    step_titles = [\n",
    "        'Step 1: Input Data',\n",
    "        'Step 2: Hidden Linear Transform',\n",
    "        'Step 3: Hidden ReLU Activation',\n",
    "        'Step 4: Output Linear Transform', \n",
    "        'Step 5: Output Sigmoid Activation',\n",
    "        'Step 6: Loss Calculation'\n",
    "    ]\n",
    "    \n",
    "    for i, (step_name, title) in enumerate(zip(step_names, step_titles)):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"🎬 ANIMATED FORWARD PROPAGATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = \"█\" * (i + 1) + \"░\" * (len(step_names) - i - 1)\n",
    "        print(f\"Progress: [{progress}] {i+1}/{len(step_names)}\")\n",
    "        print()\n",
    "        \n",
    "        # Network diagram with current step highlighted\n",
    "        print(\"    INPUT       HIDDEN        OUTPUT\")\n",
    "        if step_name == 'input':\n",
    "            print(\"   🟢[X1]  ──────┐  [ H1]  ─────┐\")\n",
    "            print(\"         ┌────┼──[ H2]  ─────┤ [ Y ]\")\n",
    "            print(\"   🟢[X2]  ─┘    └──[ H3]  ─────┘\")\n",
    "        elif step_name in ['hidden_linear', 'hidden_activation']:\n",
    "            print(\"   🔵[X1]  ──────┐ 🟢[H1]  ─────┐\")\n",
    "            print(\"         ┌────┼─🟢[H2]  ─────┤ [ Y ]\")\n",
    "            print(\"   🔵[X2]  ─┘    └─🟢[H3]  ─────┘\")\n",
    "        else:\n",
    "            print(\"   🔵[X1]  ──────┐ 🔵[H1]  ─────┐\")\n",
    "            print(\"         ┌────┼─🔵[H2]  ─────┤🟢[Y]\")\n",
    "            print(\"   🔵[X2]  ─┘    └─🔵[H3]  ─────┘\")\n",
    "        print()\n",
    "        \n",
    "        # Current step information\n",
    "        print(f\"🔄 {title}\")\n",
    "        print(f\"   {steps[step_name]['description']}\")\n",
    "        print()\n",
    "        \n",
    "        # Show values\n",
    "        if step_name == 'input':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Input Values: X1={values[0]:.3f}, X2={values[1]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'hidden_linear':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: Z¹ = W¹ × X + b¹\")\n",
    "            print(f\"   Results: Z1={values[0]:.3f}, Z2={values[1]:.3f}, Z3={values[2]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'hidden_activation':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Applying: ReLU(z) = max(0, z)\")\n",
    "            print(f\"   Results: A1={values[0]:.3f}, A2={values[1]:.3f}, A3={values[2]:.3f}\")\n",
    "            \n",
    "        elif step_name == 'output_linear':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: Z² = W² × A¹ + b²\")\n",
    "            print(f\"   Result: Z={values:.3f}\")\n",
    "            \n",
    "        elif step_name == 'output_activation':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Applying: σ(z) = 1/(1+e^(-z))\")\n",
    "            print(f\"   Result: A={values:.3f}\")\n",
    "            \n",
    "        elif step_name == 'loss':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Computing: Loss = -[y×ln(A) + (1-y)×ln(1-A)]\")\n",
    "            print(f\"   Result: Loss={values:.4f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"⏳ Processing...\" if i < len(step_names) - 1 else \"✅ Forward propagation complete!\")\n",
    "        \n",
    "        time.sleep(2)  # Animation delay\n",
    "    \n",
    "    print()\n",
    "    print(f\"🎯 Final Results:\")\n",
    "    print(f\"   Prediction: {A2:.4f}\")\n",
    "    print(f\"   Target: {net.y}\")\n",
    "    print(f\"   Error: {abs(net.y - A2):.4f}\")\n",
    "    \n",
    "    return steps, A1, A2\n",
    "\n",
    "# Run animated forward propagation\n",
    "print(\"🎬 Starting animated forward propagation...\")\n",
    "forward_steps, A1, A2 = animate_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 2: Backward Propagation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ANIMATED BACKWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [█████] 5/5\n",
      "\n",
      "    ←←← GRADIENT FLOW ←←←\n",
      "\n",
      "    OUTPUT      HIDDEN        INPUT\n",
      "    🔵[dY] ←─────┐ 🔵[dH1] ←────┐\n",
      "           ┌───┼─🔵[dH2] ←────┤🔴[dX1]\n",
      "        ←──┘   └─🔵[dH3] ←────┘🔴[dX2]\n",
      "\n",
      "🔄 Step 5: Weight Gradient Computation\n",
      "   Weight gradients computed\n",
      "\n",
      "   Weight Gradients computed!\n",
      "   dW2: [-0.1802, -0.2505, -0.0000]\n",
      "   dW1: Ready for weight updates\n",
      "   Formula: dW = dZ × A_previousᵀ\n",
      "\n",
      "✅ All gradients computed!\n",
      "\n",
      "🎯 Backward Propagation Complete!\n",
      "   All weight gradients ready for optimization step\n"
     ]
    }
   ],
   "source": [
    "def animate_backward_propagation():\n",
    "    \"\"\"Show animated backward propagation\"\"\"\n",
    "    \n",
    "    print(\"🔄 ANIMATED BACKWARD PROPAGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get backward propagation steps\n",
    "    steps = net.backward_step_by_step(A1, A2)\n",
    "    \n",
    "    step_names = ['output_grad', 'output_backward', 'hidden_grad', 'hidden_backward', 'weight_grads']\n",
    "    step_titles = [\n",
    "        'Step 1: Output Gradient Calculation',\n",
    "        'Step 2: Output Layer Backward Pass',\n",
    "        'Step 3: Hidden Layer Gradient Flow',\n",
    "        'Step 4: Hidden Layer Backward Pass',\n",
    "        'Step 5: Weight Gradient Computation'\n",
    "    ]\n",
    "    \n",
    "    for i, (step_name, title) in enumerate(zip(step_names, step_titles)):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"🔄 ANIMATED BACKWARD PROPAGATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = \"█\" * (i + 1) + \"░\" * (len(step_names) - i - 1)\n",
    "        print(f\"Progress: [{progress}] {i+1}/{len(step_names)}\")\n",
    "        print()\n",
    "        \n",
    "        # Network diagram with gradient flow\n",
    "        print(\"    ←←← GRADIENT FLOW ←←←\")\n",
    "        print()\n",
    "        print(\"    OUTPUT      HIDDEN        INPUT\")\n",
    "        if step_name in ['output_grad', 'output_backward']:\n",
    "            print(\"    🔴[dY] ←─────┐ [ dH1] ←────┐\")\n",
    "            print(\"           ┌───┼─[ dH2] ←────┤ [dX1]\")\n",
    "            print(\"        ←──┘   └─[ dH3] ←────┘ [dX2]\")\n",
    "        elif step_name in ['hidden_grad', 'hidden_backward']:\n",
    "            print(\"    🔵[dY] ←─────┐ 🔴[dH1] ←────┐\")\n",
    "            print(\"           ┌───┼─🔴[dH2] ←────┤ [dX1]\")\n",
    "            print(\"        ←──┘   └─🔴[dH3] ←────┘ [dX2]\")\n",
    "        else:\n",
    "            print(\"    🔵[dY] ←─────┐ 🔵[dH1] ←────┐\")\n",
    "            print(\"           ┌───┼─🔵[dH2] ←────┤🔴[dX1]\")\n",
    "            print(\"        ←──┘   └─🔵[dH3] ←────┘🔴[dX2]\")\n",
    "        print()\n",
    "        \n",
    "        # Current step information\n",
    "        print(f\"🔄 {title}\")\n",
    "        print(f\"   {steps[step_name]['description']}\")\n",
    "        print()\n",
    "        \n",
    "        # Show gradient values\n",
    "        if step_name == 'output_grad':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Output Gradient: dL/dA² = {values:.4f}\")\n",
    "            print(f\"   This measures how loss changes with output activation\")\n",
    "            \n",
    "        elif step_name == 'output_backward':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Output dZ²: {values:.4f}\")\n",
    "            print(f\"   Chain rule applied: dL/dZ² = dL/dA² × dA²/dZ²\")\n",
    "            \n",
    "        elif step_name == 'hidden_grad':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Hidden Gradients flowing back:\")\n",
    "            print(f\"   dA1={values[0]:.4f}, dA2={values[1]:.4f}, dA3={values[2]:.4f}\")\n",
    "            print(f\"   Computed via: dL/dA¹ = W²ᵀ × dL/dZ²\")\n",
    "            \n",
    "        elif step_name == 'hidden_backward':\n",
    "            values = steps[step_name]['values']\n",
    "            print(f\"   Hidden dZ¹ values:\")\n",
    "            print(f\"   dZ1={values[0]:.4f}, dZ2={values[1]:.4f}, dZ3={values[2]:.4f}\")\n",
    "            print(f\"   ReLU derivative applied: dL/dZ¹ = dL/dA¹ × ReLU'(Z¹)\")\n",
    "            \n",
    "        elif step_name == 'weight_grads':\n",
    "            dW2 = steps[step_name]['values']['dW2']\n",
    "            dW1 = steps[step_name]['values']['dW1']\n",
    "            print(f\"   Weight Gradients computed!\")\n",
    "            print(f\"   dW2: [{dW2[0]:.4f}, {dW2[1]:.4f}, {dW2[2]:.4f}]\")\n",
    "            print(f\"   dW1: Ready for weight updates\")\n",
    "            print(f\"   Formula: dW = dZ × A_previousᵀ\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Show gradient flow animation\n",
    "        if i < len(step_names) - 1:\n",
    "            print(\"⚡ Gradients flowing backward...\")\n",
    "            for j in range(3):\n",
    "                print(\"   \" + \"⚡\" * (j + 1) + \" \" * (10 - j))\n",
    "                time.sleep(0.3)\n",
    "                if j < 2:\n",
    "                    # Move cursor up to overwrite\n",
    "                    print(\"\\033[A\\033[K\", end=\"\")\n",
    "        else:\n",
    "            print(\"✅ All gradients computed!\")\n",
    "        \n",
    "        time.sleep(1.5)  # Animation delay\n",
    "    \n",
    "    print()\n",
    "    print(\"🎯 Backward Propagation Complete!\")\n",
    "    print(\"   All weight gradients ready for optimization step\")\n",
    "    \n",
    "    return steps\n",
    "\n",
    "# Run animated backward propagation\n",
    "print(\"🔄 Starting animated backward propagation...\")\n",
    "backward_steps = animate_backward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 3: Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃‍♂️ ANIMATED TRAINING SIMULATION\n",
      "==================================================\n",
      "Epoch Progress: [█████████░] 9/10\n",
      "\n",
      "🔄 Epoch 9\n",
      "   Current Loss: 1.5491\n",
      "   Gradient: 0.2366\n",
      "   Learning Rate: 0.1\n",
      "\n",
      "Loss Visualization: [██████████████████████████████] 1.549\n",
      "\n",
      "Trend: 📉 Decreasing (Δ 0.0282)\n",
      "\n",
      "⚡ Updating weights...\n",
      "   Weights: ⚡     \n",
      "\u001b[A\u001b[K   Weights: ⚡⚡    \n",
      "\u001b[A\u001b[K   Weights: ⚡⚡⚡   \n",
      "\n",
      "   Status: ⚠️  Slow convergence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def animate_training_progress():\n",
    "    \"\"\"Show animated training simulation\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"🏃‍♂️ ANIMATED TRAINING SIMULATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    epochs = 10\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Initial Loss: calculating...\")\n",
    "    print()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Simulate training\n",
    "    current_loss = 2.0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"🏃‍♂️ ANIMATED TRAINING SIMULATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Training progress bar\n",
    "        progress = \"█\" * (epoch + 1) + \"░\" * (epochs - epoch - 1)\n",
    "        print(f\"Epoch Progress: [{progress}] {epoch+1}/{epochs}\")\n",
    "        print()\n",
    "        \n",
    "        # Simulate gradient descent step\n",
    "        gradient = current_loss * 0.15  # Simulate gradient\n",
    "        current_loss -= learning_rate * gradient\n",
    "        current_loss = max(0.01, current_loss)\n",
    "        \n",
    "        # Add some training noise\n",
    "        if epoch > 1:\n",
    "            noise = random.uniform(-0.05, 0.02)\n",
    "            current_loss += noise\n",
    "            current_loss = max(0.01, current_loss)\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Show current training state\n",
    "        print(f\"🔄 Epoch {epoch + 1}\")\n",
    "        print(f\"   Current Loss: {current_loss:.4f}\")\n",
    "        print(f\"   Gradient: {gradient:.4f}\")\n",
    "        print(f\"   Learning Rate: {learning_rate}\")\n",
    "        print()\n",
    "        \n",
    "        # Visual loss representation\n",
    "        loss_bars = int(current_loss * 20)  # Scale for visualization\n",
    "        loss_visual = \"█\" * max(1, loss_bars) + \"░\" * max(0, 20 - loss_bars)\n",
    "        print(f\"Loss Visualization: [{loss_visual}] {current_loss:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        # Show loss trend\n",
    "        if epoch > 0:\n",
    "            trend = \"📉 Decreasing\" if losses[epoch] < losses[epoch-1] else \"📈 Increasing\"\n",
    "            change = abs(losses[epoch] - losses[epoch-1])\n",
    "            print(f\"Trend: {trend} (Δ {change:.4f})\")\n",
    "        else:\n",
    "            print(\"Trend: Starting training...\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Animation of weight updates\n",
    "        print(\"⚡ Updating weights...\")\n",
    "        for i in range(3):\n",
    "            update_visual = \"⚡\" * (i + 1) + \" \" * (5 - i)\n",
    "            print(f\"   Weights: {update_visual}\")\n",
    "            time.sleep(0.4)\n",
    "            if i < 2:\n",
    "                print(\"\\033[A\\033[K\", end=\"\")  # Move cursor up and clear line\n",
    "        \n",
    "        # Status message\n",
    "        if current_loss < 0.1:\n",
    "            status = \"🎯 Converging well!\"\n",
    "        elif current_loss < 0.5:\n",
    "            status = \"📈 Making progress\"\n",
    "        elif epoch < 3:\n",
    "            status = \"🚀 Starting to learn\"\n",
    "        else:\n",
    "            status = \"⚠️  Slow convergence\"\n",
    "        \n",
    "        print(f\"\\n   Status: {status}\")\n",
    "        print()\n",
    "        \n",
    "        time.sleep(1.5)  # Training step delay\n",
    "    \n",
    "    # Final results\n",
    "    clear_output(wait=True)\n",
    "    print(\"🏃‍♂️ TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"📊 Training Results:\")\n",
    "    print(f\"   Initial Loss: {losses[0]:.4f}\")\n",
    "    print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"   Total Improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"   Reduction: {((losses[0] - losses[-1]) / losses[0]) * 100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Show loss history\n",
    "    print(\"📈 Loss History:\")\n",
    "    for i, loss in enumerate(losses):\n",
    "        bar_length = int((1 - loss / losses[0]) * 20)\n",
    "        bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "        print(f\"   Epoch {i+1:2d}: [{bar}] {loss:.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"✅ Training animation complete!\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Run animated training\n",
    "print(\"🏃‍♂️ Starting animated training...\")\n",
    "training_losses = animate_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation 4: Weight Update Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ WEIGHT UPDATE COMPLETE!\n",
      "==================================================\n",
      "📊 BEFORE vs AFTER Comparison:\n",
      "\n",
      "🔹 Layer 1 Weights:\n",
      "   BEFORE → AFTER\n",
      "   Row 1: [ 0.500, -0.300] → [ 0.528, -0.289]\n",
      "   Row 2: [ 0.200,  0.700] → [ 0.182,  0.693]\n",
      "   Row 3: [-0.400,  0.600] → [-0.400,  0.600]\n",
      "\n",
      "🔸 Layer 2 Weights:\n",
      "   BEFORE → AFTER\n",
      "   [ 0.800, -0.500,  0.300] → [ 0.818, -0.475,  0.300]\n",
      "\n",
      "📈 Update Statistics:\n",
      "   Total W1 change: 0.0629\n",
      "   Total W2 change: 0.0431\n",
      "   Learning rate: 0.1\n",
      "   📊 Significant updates - network is learning!\n",
      "\n",
      "✅ Weight updates complete! Network ready for next iteration.\n"
     ]
    }
   ],
   "source": [
    "def animate_weight_updates():\n",
    "    \"\"\"Show animated weight update process\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"⚖️ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get gradients\n",
    "    steps = net.backward_step_by_step(A1, A2)\n",
    "    dW1 = steps['weight_grads']['values']['dW1']\n",
    "    dW2 = steps['weight_grads']['values']['dW2']\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    print(\"Starting weight update process...\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 1: Show original weights\n",
    "    clear_output(wait=True)\n",
    "    print(\"⚖️ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 1/3: Original Weights\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔹 Layer 1 Weights (W1):\")\n",
    "    for i, row in enumerate(net.W1):\n",
    "        print(f\"   Row {i+1}: [{row[0]:6.3f}, {row[1]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\n🔸 Layer 2 Weights (W2):\")\n",
    "    print(f\"   [{net.W2[0]:6.3f}, {net.W2[1]:6.3f}, {net.W2[2]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\n⏳ Preparing gradients...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 2: Show gradients\n",
    "    clear_output(wait=True)\n",
    "    print(\"⚖️ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 2/3: Computed Gradients\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔹 Gradients for W1 (dW1):\")\n",
    "    for i, row in enumerate(dW1):\n",
    "        # Animate gradient appearance\n",
    "        for j in range(len(row) + 1):\n",
    "            print(f\"\\r   Row {i+1}: [\", end=\"\")\n",
    "            for k in range(j):\n",
    "                if k < len(row):\n",
    "                    print(f\"{row[k]:6.3f}\", end=\"\")\n",
    "                    if k < len(row) - 1:\n",
    "                        print(\", \", end=\"\")\n",
    "            print(\"]\" + \" \" * 10, end=\"\")\n",
    "            time.sleep(0.3)\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n🔸 Gradients for W2 (dW2):\")\n",
    "    print(\"   [\", end=\"\")\n",
    "    for i, val in enumerate(dW2):\n",
    "        print(f\"{val:6.3f}\", end=\"\")\n",
    "        if i < len(dW2) - 1:\n",
    "            print(\", \", end=\"\")\n",
    "        time.sleep(0.3)\n",
    "    print(\"]\")\n",
    "    \n",
    "    print(f\"\\n📐 Update Formula: W_new = W_old - {learning_rate} × gradient\")\n",
    "    print(\"⏳ Applying updates...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Phase 3: Animated weight updates\n",
    "    clear_output(wait=True)\n",
    "    print(\"⚖️ ANIMATED WEIGHT UPDATES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 3/3: Weight Updates in Progress\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate new weights\n",
    "    W1_new = []\n",
    "    W2_new = []\n",
    "    \n",
    "    print(\"🔹 Updating W1...\")\n",
    "    for i in range(len(net.W1)):\n",
    "        row_new = []\n",
    "        for j in range(len(net.W1[i])):\n",
    "            old_val = net.W1[i][j]\n",
    "            grad_val = dW1[i][j]\n",
    "            new_val = old_val - learning_rate * grad_val\n",
    "            row_new.append(new_val)\n",
    "            \n",
    "            # Show the update calculation\n",
    "            change = abs(old_val - new_val)\n",
    "            direction = \"↓\" if old_val > new_val else \"↑\"\n",
    "            \n",
    "            print(f\"   W1[{i+1},{j+1}]: {old_val:.3f} - {learning_rate}×{grad_val:.3f} = {new_val:.3f} {direction}{change:.3f}\")\n",
    "            time.sleep(0.5)\n",
    "        W1_new.append(row_new)\n",
    "    \n",
    "    print(\"\\n🔸 Updating W2...\")\n",
    "    for i in range(len(net.W2)):\n",
    "        old_val = net.W2[i]\n",
    "        grad_val = dW2[i]\n",
    "        new_val = old_val - learning_rate * grad_val\n",
    "        W2_new.append(new_val)\n",
    "        \n",
    "        change = abs(old_val - new_val)\n",
    "        direction = \"↓\" if old_val > new_val else \"↑\"\n",
    "        \n",
    "        print(f\"   W2[{i+1}]: {old_val:.3f} - {learning_rate}×{grad_val:.3f} = {new_val:.3f} {direction}{change:.3f}\")\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Final comparison\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    print(\"⚖️ WEIGHT UPDATE COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"📊 BEFORE vs AFTER Comparison:\")\n",
    "    print()\n",
    "    print(\"🔹 Layer 1 Weights:\")\n",
    "    print(\"   BEFORE → AFTER\")\n",
    "    for i in range(len(net.W1)):\n",
    "        old_row = net.W1[i]\n",
    "        new_row = W1_new[i]\n",
    "        print(f\"   Row {i+1}: [{old_row[0]:6.3f}, {old_row[1]:6.3f}] → [{new_row[0]:6.3f}, {new_row[1]:6.3f}]\")\n",
    "    \n",
    "    print(\"\\n🔸 Layer 2 Weights:\")\n",
    "    print(\"   BEFORE → AFTER\")\n",
    "    old_W2 = net.W2\n",
    "    print(f\"   [{old_W2[0]:6.3f}, {old_W2[1]:6.3f}, {old_W2[2]:6.3f}] → [{W2_new[0]:6.3f}, {W2_new[1]:6.3f}, {W2_new[2]:6.3f}]\")\n",
    "    \n",
    "    # Calculate update statistics\n",
    "    total_change_W1 = sum(sum(abs(net.W1[i][j] - W1_new[i][j]) for j in range(len(net.W1[i]))) for i in range(len(net.W1)))\n",
    "    total_change_W2 = sum(abs(net.W2[i] - W2_new[i]) for i in range(len(net.W2)))\n",
    "    \n",
    "    print(f\"\\n📈 Update Statistics:\")\n",
    "    print(f\"   Total W1 change: {total_change_W1:.4f}\")\n",
    "    print(f\"   Total W2 change: {total_change_W2:.4f}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    \n",
    "    if total_change_W1 + total_change_W2 > 0.1:\n",
    "        print(f\"   📊 Significant updates - network is learning!\")\n",
    "    else:\n",
    "        print(f\"   📊 Small updates - near convergence or low learning rate\")\n",
    "    \n",
    "    print(\"\\n✅ Weight updates complete! Network ready for next iteration.\")\n",
    "    \n",
    "    return W1_new, W2_new\n",
    "\n",
    "# Run animated weight updates\n",
    "print(\"⚖️ Starting animated weight updates...\")\n",
    "new_W1, new_W2 = animate_weight_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 ANIMATED FORWARD PROPAGATION\n",
      "==================================================\n",
      "Progress: [██████] 6/6\n",
      "\n",
      "    INPUT       HIDDEN        OUTPUT\n",
      "   🔵[X1]  ──────┐ 🔵[H1]  ─────┐\n",
      "         ┌────┼─🔵[H2]  ─────┤🟢[Y]\n",
      "   🔵[X2]  ─┘    └─🔵[H3]  ─────┘\n",
      "\n",
      "🔄 Step 6: Loss Calculation\n",
      "   Loss = 0.5790\n",
      "\n",
      "   Computing: Loss = -[y×ln(A) + (1-y)×ln(1-A)]\n",
      "   Result: Loss=0.5790\n",
      "\n",
      "✅ Forward propagation complete!\n",
      "\n",
      "🎯 Final Results:\n",
      "   Prediction: 0.5605\n",
      "   Target: 1.0\n",
      "   Error: 0.4395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input': {'values': [0.8, 0.3], 'description': 'Input values'},\n",
       "  'hidden_linear': {'values': [0.41000000000000003,\n",
       "    0.5700000000000001,\n",
       "    -0.24000000000000007],\n",
       "   'description': 'Z¹ = W¹X + b¹'},\n",
       "  'hidden_activation': {'values': [0.41000000000000003, 0.5700000000000001, 0],\n",
       "   'description': 'A¹ = ReLU(Z¹)'},\n",
       "  'output_linear': {'values': 0.24300000000000005,\n",
       "   'description': 'Z² = W²A¹ + b²'},\n",
       "  'output_activation': {'values': 0.5604528191374981,\n",
       "   'description': 'A² = σ(Z²)'},\n",
       "  'loss': {'values': 0.5790101985529353, 'description': 'Loss = 0.5790'}},\n",
       " [0.41000000000000003, 0.5700000000000001, 0],\n",
       " 0.5604528191374981)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_complete_animation_sequence():\n",
    "    \"\"\"Run all animations in sequence for a complete neural network demo\"\"\"\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"🎬 COMPLETE NEURAL NETWORK ANIMATION SEQUENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"This demo will show you:\")\n",
    "    print(\"  1. 🎬 Animated Forward Propagation\")\n",
    "    print(\"  2. 🔄 Animated Backward Propagation\") \n",
    "    print(\"  3. 🏃‍♂️ Animated Training Process\")\n",
    "    print(\"  4. ⚖️ Animated Weight Updates\")\n",
    "    print(\"  5. 📊 Final Summary\")\n",
    "    print()\n",
    "    print(\"Each animation will run automatically...\")\n",
    "    print(\"Press Ctrl+C at any time to stop\")\n",
    "    print()\n",
    "    input(\"Press Enter to start the complete animation sequence...\")\n",
    "    \n",
    "    try:\n",
    "        # Animation 1: Forward Propagation\n",
    "        print(\"🎬 Starting Animation 1/4: Forward Propagation...\")\n",
    "        time.sleep(1)\n",
    "        forward_steps, A1, A2 = animate_forward_propagation()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Backward Propagation...\")\n",
    "        \n",
    "        # Animation 2: Backward Propagation\n",
    "        print(\"🔄 Starting Animation 2/4: Backward Propagation...\")\n",
    "        time.sleep(1)\n",
    "        backward_steps = animate_backward_propagation()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Training Simulation...\")\n",
    "        \n",
    "        # Animation 3: Training Process\n",
    "        print(\"🏃‍♂️ Starting Animation 3/4: Training Process...\")\n",
    "        time.sleep(1)\n",
    "        training_losses = animate_training_progress()\n",
    "        \n",
    "        input(\"\\nPress Enter to continue to Weight Updates...\")\n",
    "        \n",
    "        # Animation 4: Weight Updates\n",
    "        print(\"⚖️ Starting Animation 4/4: Weight Updates...\")\n",
    "        time.sleep(1)\n",
    "        new_W1, new_W2 = animate_weight_updates()\n",
    "        \n",
    "        # Final Summary\n",
    "        clear_output(wait=True)\n",
    "        print(\"🎉 COMPLETE ANIMATION SEQUENCE FINISHED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "        print(\"🎯 What you just learned:\")\n",
    "        print(\"  ✅ How neural networks process information forward\")\n",
    "        print(\"  ✅ How gradients flow backward through the network\")\n",
    "        print(\"  ✅ How training reduces loss over time\")\n",
    "        print(\"  ✅ How weights get updated to improve performance\")\n",
    "        print()\n",
    "        \n",
    "        print(\"📊 Key Takeaways:\")\n",
    "        print(f\"  • Network processed input [{net.X[0]}, {net.X[1]}] → output {A2:.4f}\")\n",
    "        print(f\"  • Target was {net.y}, so error was {abs(net.y - A2):.4f}\")\n",
    "        print(f\"  • Gradients computed automatically via backpropagation\")\n",
    "        print(f\"  • Training simulation showed loss decreasing over time\")\n",
    "        print(f\"  • Weights updated using gradient descent optimization\")\n",
    "        print()\n",
    "        \n",
    "        print(\"🚀 Next Steps:\")\n",
    "        print(\"  1. Try different learning rates in the training function\")\n",
    "        print(\"  2. Modify the network architecture (more neurons/layers)\")\n",
    "        print(\"  3. Change the input data and see how it affects propagation\")\n",
    "        print(\"  4. Experiment with different activation functions\")\n",
    "        print()\n",
    "        \n",
    "        print(\"🎬 All animations complete! Neural network concepts mastered!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        clear_output(wait=True)\n",
    "        print(\"⏹️ Animation sequence stopped by user\")\n",
    "        print(\"You can run individual animations by calling their functions:\")\n",
    "        print(\"  • animate_forward_propagation()\")\n",
    "        print(\"  • animate_backward_propagation()\")\n",
    "        print(\"  • animate_training_progress()\")\n",
    "        print(\"  • animate_weight_updates()\")\n",
    "\n",
    "# Run complete animation sequence\n",
    "print(\"🎬 Complete animation sequence ready!\")\n",
    "print(\"Call run_complete_animation_sequence() to see all animations!\")\n",
    "\n",
    "# For immediate testing, run the first animation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎬 DEMO: Running Forward Propagation Animation\")\n",
    "print(\"=\"*50)\n",
    "animate_forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
