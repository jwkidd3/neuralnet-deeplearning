{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: Network Architecture Design\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the components of shallow neural network architectures\n",
    "- Design network architectures for different problem types\n",
    "- Implement modular neural network components\n",
    "- Visualize network architectures and data flow\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 2.1-2.7\n",
    "- Understanding of binary classification\n",
    "- Familiarity with NumPy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Network Architecture Components (10 minutes)\n",
    "\n",
    "### Network Layer Structure\n",
    "A shallow neural network consists of:\n",
    "1. **Input layer**: Receives features\n",
    "2. **Hidden layer(s)**: Process and transform data\n",
    "3. **Output layer**: Produces predictions\n",
    "\n",
    "Let's create a class to represent our network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkArchitecture:\n",
    "    \"\"\"\n",
    "    A class to define and visualize neural network architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation_functions=None):\n",
    "        \"\"\"\n",
    "        Initialize network architecture\n",
    "        \n",
    "        Parameters:\n",
    "        layer_dims: list of integers representing the number of units in each layer\n",
    "        activation_functions: list of activation function names for each layer\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.n_layers = len(layer_dims)\n",
    "        \n",
    "        # Default activation functions\n",
    "        if activation_functions is None:\n",
    "            # ReLU for hidden layers, sigmoid for output\n",
    "            self.activation_functions = ['relu'] * (self.n_layers - 2) + ['sigmoid']\n",
    "        else:\n",
    "            self.activation_functions = activation_functions\n",
    "            \n",
    "        # Calculate parameter counts\n",
    "        self._calculate_parameters()\n",
    "    \n",
    "    def _calculate_parameters(self):\n",
    "        \"\"\"Calculate the number of parameters (weights and biases) in the network\"\"\"\n",
    "        self.parameter_count = 0\n",
    "        self.layer_parameters = []\n",
    "        \n",
    "        for i in range(1, self.n_layers):\n",
    "            # Weights: current_layer_size * previous_layer_size\n",
    "            weights = self.layer_dims[i] * self.layer_dims[i-1]\n",
    "            # Biases: current_layer_size\n",
    "            biases = self.layer_dims[i]\n",
    "            layer_params = weights + biases\n",
    "            \n",
    "            self.layer_parameters.append({\n",
    "                'layer': i,\n",
    "                'weights': weights,\n",
    "                'biases': biases,\n",
    "                'total': layer_params\n",
    "            })\n",
    "            \n",
    "            self.parameter_count += layer_params\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print a summary of the network architecture\"\"\"\n",
    "        print(\"Neural Network Architecture Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total Layers: {self.n_layers}\")\n",
    "        print(f\"Input Features: {self.layer_dims[0]}\")\n",
    "        print(f\"Output Units: {self.layer_dims[-1]}\")\n",
    "        print(f\"Total Parameters: {self.parameter_count:,}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Layer-by-Layer Breakdown:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Layer':<8} {'Type':<12} {'Units':<8} {'Activation':<12} {'Parameters':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Input layer\n",
    "        print(f\"{'0':<8} {'Input':<12} {self.layer_dims[0]:<8} {'None':<12} {'0':<12}\")\n",
    "        \n",
    "        # Hidden and output layers\n",
    "        for i, params in enumerate(self.layer_parameters, 1):\n",
    "            layer_type = 'Hidden' if i < self.n_layers - 1 else 'Output'\n",
    "            activation = self.activation_functions[i-1]\n",
    "            units = self.layer_dims[i]\n",
    "            param_count = params['total']\n",
    "            \n",
    "            print(f\"{i:<8} {layer_type:<12} {units:<8} {activation:<12} {param_count:<12,}\")\n",
    "    \n",
    "    def visualize_architecture(self):\n",
    "        \"\"\"Create a visual representation of the network architecture\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Calculate positions for layers\n",
    "        layer_positions = np.linspace(0, 10, self.n_layers)\n",
    "        max_units = max(self.layer_dims)\n",
    "        \n",
    "        # Draw nodes for each layer\n",
    "        for i, (pos, units) in enumerate(zip(layer_positions, self.layer_dims)):\n",
    "            # Calculate vertical positions for nodes in this layer\n",
    "            if units == 1:\n",
    "                node_positions = [max_units / 2]\n",
    "            else:\n",
    "                node_positions = np.linspace(0, max_units, units)\n",
    "            \n",
    "            # Draw nodes\n",
    "            for node_pos in node_positions:\n",
    "                color = 'lightblue' if i == 0 else 'lightgreen' if i == self.n_layers - 1 else 'lightcoral'\n",
    "                circle = plt.Circle((pos, node_pos), 0.3, color=color, ec='black', linewidth=1)\n",
    "                ax.add_patch(circle)\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if i < self.n_layers - 1:\n",
    "                next_units = self.layer_dims[i + 1]\n",
    "                next_pos = layer_positions[i + 1]\n",
    "                \n",
    "                if next_units == 1:\n",
    "                    next_node_positions = [max_units / 2]\n",
    "                else:\n",
    "                    next_node_positions = np.linspace(0, max_units, next_units)\n",
    "                \n",
    "                # Draw connections (sample only to avoid clutter)\n",
    "                for current_node in node_positions[::max(1, len(node_positions)//3)]:\n",
    "                    for next_node in next_node_positions[::max(1, len(next_node_positions)//3)]:\n",
    "                        ax.plot([pos + 0.3, next_pos - 0.3], [current_node, next_node], \n",
    "                               'k-', alpha=0.3, linewidth=0.5)\n",
    "        \n",
    "        # Add labels\n",
    "        layer_names = ['Input'] + [f'Hidden {i}' for i in range(1, self.n_layers-1)] + ['Output']\n",
    "        for i, (pos, name, units) in enumerate(zip(layer_positions, layer_names, self.layer_dims)):\n",
    "            ax.text(pos, max_units + 0.5, f'{name}\\n({units} units)', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            # Add activation function labels (except for input layer)\n",
    "            if i > 0:\n",
    "                activation = self.activation_functions[i-1]\n",
    "                ax.text(pos, -0.8, f'{activation}', \n",
    "                       ha='center', va='top', fontsize=9, style='italic')\n",
    "        \n",
    "        ax.set_xlim(-1, 11)\n",
    "        ax.set_ylim(-1.5, max_units + 1)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Neural Network Architecture Visualization', fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test the architecture class\n",
    "print(\"NeuralNetworkArchitecture class created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Design Architectures for Different Problems (15 minutes)\n",
    "\n",
    "### Problem 1: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design architecture for binary classification\n",
    "# Input: 2 features, Hidden: 4 units, Output: 1 unit\n",
    "binary_arch = NeuralNetworkArchitecture(\n",
    "    layer_dims=[2, 4, 1],\n",
    "    activation_functions=['relu', 'sigmoid']\n",
    ")\n",
    "\n",
    "print(\"=== BINARY CLASSIFICATION ARCHITECTURE ===\")\n",
    "binary_arch.summary()\n",
    "print(\"\\nVisualization:\")\n",
    "binary_arch.visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design architecture for multi-class classification (3 classes)\n",
    "# Input: 4 features, Hidden: 6 units, Output: 3 units\n",
    "multiclass_arch = NeuralNetworkArchitecture(\n",
    "    layer_dims=[4, 6, 3],\n",
    "    activation_functions=['relu', 'softmax']\n",
    ")\n",
    "\n",
    "print(\"=== MULTI-CLASS CLASSIFICATION ARCHITECTURE ===\")\n",
    "multiclass_arch.summary()\n",
    "print(\"\\nVisualization:\")\n",
    "multiclass_arch.visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Complex Non-linear Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design architecture for complex non-linear problem\n",
    "# Input: 10 features, Hidden1: 20 units, Hidden2: 10 units, Output: 1 unit\n",
    "complex_arch = NeuralNetworkArchitecture(\n",
    "    layer_dims=[10, 20, 10, 1],\n",
    "    activation_functions=['relu', 'relu', 'sigmoid']\n",
    ")\n",
    "\n",
    "print(\"=== COMPLEX NON-LINEAR PROBLEM ARCHITECTURE ===\")\n",
    "complex_arch.summary()\n",
    "print(\"\\nVisualization:\")\n",
    "complex_arch.visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hands-on Architecture Design (15 minutes)\n",
    "\n",
    "### Exercise 1: Design Your Own Architecture\n",
    "Create an architecture for a specific problem scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Design an architecture for predicting house prices\n",
    "# Requirements:\n",
    "# - 8 input features (size, bedrooms, bathrooms, age, location_score, etc.)\n",
    "# - 1 hidden layer with appropriate number of units\n",
    "# - 1 output unit (price prediction - regression problem)\n",
    "\n",
    "# TODO: Create your architecture here\n",
    "# Hint: For regression, use 'linear' activation for output layer\n",
    "\n",
    "house_price_arch = NeuralNetworkArchitecture(\n",
    "    layer_dims=[8, 12, 1],  # You can modify these dimensions\n",
    "    activation_functions=['relu', 'linear']  # Linear for regression output\n",
    ")\n",
    "\n",
    "print(\"=== HOUSE PRICE PREDICTION ARCHITECTURE ===\")\n",
    "house_price_arch.summary()\n",
    "house_price_arch.visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different architectures for the same problem\n",
    "# Problem: Binary classification with 5 input features\n",
    "\n",
    "architectures = {\n",
    "    'Small': NeuralNetworkArchitecture([5, 3, 1], ['relu', 'sigmoid']),\n",
    "    'Medium': NeuralNetworkArchitecture([5, 8, 1], ['relu', 'sigmoid']),\n",
    "    'Large': NeuralNetworkArchitecture([5, 16, 1], ['relu', 'sigmoid']),\n",
    "    'Deep': NeuralNetworkArchitecture([5, 8, 4, 1], ['relu', 'relu', 'sigmoid'])\n",
    "}\n",
    "\n",
    "print(\"=== ARCHITECTURE COMPARISON ===\")\n",
    "print(f\"{'Architecture':<12} {'Parameters':<12} {'Layers':<8} {'Hidden Units':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, arch in architectures.items():\n",
    "    hidden_units = ' -> '.join(map(str, arch.layer_dims[1:-1]))\n",
    "    print(f\"{name:<12} {arch.parameter_count:<12,} {arch.n_layers:<8} {hidden_units:<15}\")\n",
    "\n",
    "# Visualize one of them\n",
    "print(\"\\nDetailed view of 'Deep' architecture:\")\n",
    "architectures['Deep'].visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Architecture Selection Guidelines (5 minutes)\n",
    "\n",
    "### Key Principles for Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture_guidelines():\n",
    "    \"\"\"\n",
    "    Guidelines for selecting neural network architectures\n",
    "    \"\"\"\n",
    "    guidelines = {\n",
    "        \"Problem Type\": {\n",
    "            \"Binary Classification\": \"1 output unit with sigmoid activation\",\n",
    "            \"Multi-class Classification\": \"N output units with softmax activation\",\n",
    "            \"Regression\": \"1 output unit with linear activation\",\n",
    "            \"Multi-output Regression\": \"N output units with linear activation\"\n",
    "        },\n",
    "        \"Hidden Layer Size\": {\n",
    "            \"Rule of Thumb\": \"Between input size and output size\",\n",
    "            \"Common Practice\": \"Start with 2/3 * (input + output) size\",\n",
    "            \"Experimentation\": \"Try powers of 2: 4, 8, 16, 32, 64, etc.\"\n",
    "        },\n",
    "        \"Number of Hidden Layers\": {\n",
    "            \"Simple Problems\": \"1 hidden layer often sufficient\",\n",
    "            \"Complex Problems\": \"2-3 hidden layers for non-linear patterns\",\n",
    "            \"Avoid Overfitting\": \"More layers = more parameters = higher overfitting risk\"\n",
    "        },\n",
    "        \"Activation Functions\": {\n",
    "            \"Hidden Layers\": \"ReLU (most common), tanh, or LeakyReLU\",\n",
    "            \"Output Layer\": \"Sigmoid (binary), Softmax (multiclass), Linear (regression)\",\n",
    "            \"Avoid\": \"Sigmoid in hidden layers (vanishing gradient)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, items in guidelines.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"=\" * (len(category) + 1))\n",
    "        for key, value in items.items():\n",
    "            print(f\"â€¢ {key}: {value}\")\n",
    "\n",
    "architecture_guidelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Architecture Class**: Understood NeuralNetworkArchitecture class components\n",
    "- [ ] **Binary Classification**: Designed and visualized binary classification architecture\n",
    "- [ ] **Multi-class Classification**: Designed and visualized multi-class architecture\n",
    "- [ ] **Complex Architecture**: Designed and visualized multi-layer architecture\n",
    "- [ ] **Custom Design**: Created house price prediction architecture\n",
    "- [ ] **Architecture Comparison**: Compared different architectures for same problem\n",
    "- [ ] **Guidelines**: Reviewed architecture selection guidelines\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Network Components**: Input, hidden, and output layers\n",
    "2. **Parameter Calculation**: Weights and biases counting\n",
    "3. **Architecture Design**: Matching network structure to problem type\n",
    "4. **Visualization**: Understanding network topology\n",
    "5. **Design Guidelines**: Best practices for architecture selection\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Problem-Specific Design**: Different problems require different architectures\n",
    "- **Parameter Efficiency**: More parameters â‰  better performance\n",
    "- **Activation Functions**: Critical for network capability and training\n",
    "- **Layer Sizing**: Balance between capacity and overfitting\n",
    "- **Experimentation**: Architecture design is iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps\n",
    "\n",
    "Run the following cells to validate your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Architecture Creation\n",
    "def test_architecture_creation():\n",
    "    \"\"\"Test if you can create architectures correctly\"\"\"\n",
    "    try:\n",
    "        # Create a test architecture\n",
    "        test_arch = NeuralNetworkArchitecture([3, 5, 2], ['relu', 'softmax'])\n",
    "        \n",
    "        # Check basic properties\n",
    "        assert test_arch.n_layers == 3, \"Layer count incorrect\"\n",
    "        assert test_arch.layer_dims == [3, 5, 2], \"Layer dimensions incorrect\"\n",
    "        assert test_arch.parameter_count == 27, \"Parameter count incorrect\"  # (3*5 + 5) + (5*2 + 2) = 32\n",
    "        \n",
    "        print(\"âœ… Architecture creation test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Architecture creation test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_architecture_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Parameter Calculation\n",
    "def test_parameter_calculation():\n",
    "    \"\"\"Test parameter calculation accuracy\"\"\"\n",
    "    try:\n",
    "        # Test different architectures\n",
    "        arch1 = NeuralNetworkArchitecture([2, 3, 1])  # (2*3 + 3) + (3*1 + 1) = 9 + 4 = 13\n",
    "        arch2 = NeuralNetworkArchitecture([4, 8, 8, 3])  # (4*8 + 8) + (8*8 + 8) + (8*3 + 3) = 40 + 72 + 27 = 139\n",
    "        \n",
    "        assert arch1.parameter_count == 13, f\"Expected 13 parameters, got {arch1.parameter_count}\"\n",
    "        assert arch2.parameter_count == 139, f\"Expected 139 parameters, got {arch2.parameter_count}\"\n",
    "        \n",
    "        print(\"âœ… Parameter calculation test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Parameter calculation test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_parameter_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 3: Architecture Appropriateness\n",
    "def test_architecture_appropriateness():\n",
    "    \"\"\"Test if architectures are appropriate for their problems\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            'name': 'Binary Classification',\n",
    "            'arch': NeuralNetworkArchitecture([5, 8, 1], ['relu', 'sigmoid']),\n",
    "            'expected_output_units': 1,\n",
    "            'expected_output_activation': 'sigmoid'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Multi-class Classification',\n",
    "            'arch': NeuralNetworkArchitecture([10, 15, 5], ['relu', 'softmax']),\n",
    "            'expected_output_units': 5,\n",
    "            'expected_output_activation': 'softmax'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for test in test_cases:\n",
    "        arch = test['arch']\n",
    "        output_units = arch.layer_dims[-1]\n",
    "        output_activation = arch.activation_functions[-1]\n",
    "        \n",
    "        if output_units != test['expected_output_units']:\n",
    "            print(f\"âŒ {test['name']}: Wrong output units ({output_units} vs {test['expected_output_units']})\")\n",
    "            all_passed = False\n",
    "        \n",
    "        if output_activation != test['expected_output_activation']:\n",
    "            print(f\"âŒ {test['name']}: Wrong output activation ({output_activation} vs {test['expected_output_activation']})\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"âœ… Architecture appropriateness test passed!\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "test_architecture_appropriateness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Visualization not displaying**\n",
    "- **Cause**: Matplotlib backend issues\n",
    "- **Solution**: Run `%matplotlib inline` in a cell, restart kernel if needed\n",
    "\n",
    "**Issue 2: Parameter count seems wrong**\n",
    "- **Cause**: Misunderstanding parameter calculation\n",
    "- **Solution**: Remember parameters = weights + biases = (current_units Ã— previous_units) + current_units\n",
    "\n",
    "**Issue 3: Architecture class errors**\n",
    "- **Cause**: Incorrect layer dimensions or activation function lists\n",
    "- **Solution**: Ensure layer_dims length matches activation_functions length + 1\n",
    "\n",
    "**Issue 4: Import errors**\n",
    "- **Cause**: Missing packages\n",
    "- **Solution**: Install required packages: `pip install numpy matplotlib pandas scikit-learn`\n",
    "\n",
    "### Getting Help:\n",
    "- Check the error message carefully\n",
    "- Review the architecture requirements for your problem type\n",
    "- Verify input dimensions match your data\n",
    "- Ask instructor for clarification on architecture design principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your custom architectures\n",
    "2. **Clear output**: Cell â†’ All Output â†’ Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up variables\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Lab 3.1 completed successfully! ðŸŽ‰\")\n",
    "print(\"You're ready to move on to Lab 3.2: Forward Propagation Vectorization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}