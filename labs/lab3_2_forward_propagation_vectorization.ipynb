{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Forward Propagation Vectorization\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement forward propagation using vectorized operations\n",
    "- Understand the mathematics behind forward propagation\n",
    "- Optimize neural network computations using NumPy\n",
    "- Handle multiple training examples simultaneously\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Lab 3.1\n",
    "- Understanding of matrix operations\n",
    "- Basic knowledge of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Activation Functions (10 minutes)\n",
    "\n",
    "First, let's implement the activation functions we'll need for forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"\n",
    "    Collection of activation functions and their derivatives\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh function\"\"\"\n",
    "        return 1 - np.tanh(z)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        \"\"\"Linear activation function (identity)\"\"\"\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        \"\"\"Derivative of linear function\"\"\"\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax activation function for multi-class classification\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "# Test activation functions\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Testing activation functions:\")\n",
    "print(f\"Input: {test_values}\")\n",
    "print(f\"ReLU: {ActivationFunctions.relu(test_values)}\")\n",
    "print(f\"Sigmoid: {ActivationFunctions.sigmoid(test_values)}\")\n",
    "print(f\"Tanh: {ActivationFunctions.tanh(test_values)}\")\n",
    "print(\"\\nActivation functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Forward Propagation Implementation (15 minutes)\n",
    "\n",
    "### Understanding Forward Propagation Mathematics\n",
    "\n",
    "For each layer l:\n",
    "- Linear transformation: Z^[l] = W^[l] * A^[l-1] + b^[l]\n",
    "- Activation: A^[l] = g^[l](Z^[l])\n",
    "\n",
    "Where:\n",
    "- W^[l]: Weight matrix for layer l\n",
    "- b^[l]: Bias vector for layer l\n",
    "- A^[l-1]: Activations from previous layer\n",
    "- g^[l]: Activation function for layer l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardPropagation:\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = ActivationFunctions()\n",
    "        self.cache = {}  # Store intermediate values for backpropagation\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for all layers\n",
    "        \n",
    "        Parameters:\n",
    "        layer_dims: list of layer dimensions [input_size, hidden1, hidden2, ..., output_size]\n",
    "        \n",
    "        Returns:\n",
    "        parameters: dictionary containing weights and biases\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            # Xavier/Glorot initialization\n",
    "            parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def linear_forward(self, A_prev, W, b):\n",
    "        \"\"\"\n",
    "        Implement the linear part of forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev: activations from previous layer (n_prev, m)\n",
    "        W: weights matrix (n_current, n_prev)\n",
    "        b: bias vector (n_current, 1)\n",
    "        \n",
    "        Returns:\n",
    "        Z: linear transformation result\n",
    "        cache: tuple containing (A_prev, W, b) for backpropagation\n",
    "        \"\"\"\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        cache = (A_prev, W, b)\n",
    "        \n",
    "        return Z, cache\n",
    "    \n",
    "    def linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        \"\"\"\n",
    "        Implement forward propagation for one layer (linear + activation)\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev: activations from previous layer\n",
    "        W: weights matrix\n",
    "        b: bias vector\n",
    "        activation: activation function name\n",
    "        \n",
    "        Returns:\n",
    "        A: post-activation values\n",
    "        cache: tuple containing linear_cache and activation_cache\n",
    "        \"\"\"\n",
    "        # Linear forward\n",
    "        Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "        \n",
    "        # Activation forward\n",
    "        if activation == 'sigmoid':\n",
    "            A = self.activations.sigmoid(Z)\n",
    "        elif activation == 'relu':\n",
    "            A = self.activations.relu(Z)\n",
    "        elif activation == 'tanh':\n",
    "            A = self.activations.tanh(Z)\n",
    "        elif activation == 'linear':\n",
    "            A = self.activations.linear(Z)\n",
    "        elif activation == 'softmax':\n",
    "            A = self.activations.softmax(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        cache = (linear_cache, Z)  # Store both linear cache and Z for backprop\n",
    "        \n",
    "        return A, cache\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, activation_functions):\n",
    "        \"\"\"\n",
    "        Implement forward propagation for entire network\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data (input_size, m)\n",
    "        parameters: dictionary containing weights and biases\n",
    "        activation_functions: list of activation functions for each layer\n",
    "        \n",
    "        Returns:\n",
    "        AL: final layer activations (predictions)\n",
    "        caches: list of caches for each layer\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        \n",
    "        # Number of layers (excluding input layer)\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        # Forward propagation through all layers\n",
    "        for l in range(1, L + 1):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            activation = activation_functions[l-1]\n",
    "            \n",
    "            A, cache = self.linear_activation_forward(A_prev, W, b, activation)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return A, caches\n",
    "    \n",
    "    def predict(self, X, parameters, activation_functions, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained network\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data\n",
    "        parameters: network parameters\n",
    "        activation_functions: activation functions\n",
    "        threshold: decision threshold for binary classification\n",
    "        \n",
    "        Returns:\n",
    "        predictions: predicted labels\n",
    "        probabilities: prediction probabilities\n",
    "        \"\"\"\n",
    "        probabilities, _ = self.forward_propagation(X, parameters, activation_functions)\n",
    "        \n",
    "        if probabilities.shape[0] == 1:  # Binary classification\n",
    "            predictions = (probabilities > threshold).astype(int)\n",
    "        else:  # Multi-class classification\n",
    "            predictions = np.argmax(probabilities, axis=0)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "\n",
    "print(\"ForwardPropagation class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Forward Propagation (10 minutes)\n",
    "\n",
    "### Test 1: Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test network\n",
    "forward_prop = ForwardPropagation()\n",
    "\n",
    "# Network architecture: 3 inputs -> 4 hidden (ReLU) -> 1 output (sigmoid)\n",
    "layer_dims = [3, 4, 1]\n",
    "activation_functions = ['relu', 'sigmoid']\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = forward_prop.initialize_parameters(layer_dims)\n",
    "\n",
    "print(\"Network Architecture:\")\n",
    "print(f\"Layer dimensions: {layer_dims}\")\n",
    "print(f\"Activation functions: {activation_functions}\")\n",
    "print(\"\\nInitialized Parameters:\")\n",
    "for key, value in parameters.items():\n",
    "    print(f\"{key}: shape {value.shape}\")\n",
    "\n",
    "# Create test input\n",
    "m = 5  # Number of examples\n",
    "X = np.random.randn(3, m)  # 3 features, 5 examples\n",
    "\n",
    "print(f\"\\nInput shape: {X.shape}\")\n",
    "print(f\"Input data (first 3 examples):\\n{X[:, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward propagation\n",
    "predictions, caches = forward_prop.forward_propagation(X, parameters, activation_functions)\n",
    "\n",
    "print(\"Forward Propagation Results:\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions: {predictions.flatten()}\")\n",
    "print(f\"Number of cached layers: {len(caches)}\")\n",
    "\n",
    "# Test predictions\n",
    "pred_labels, pred_probs = forward_prop.predict(X, parameters, activation_functions)\n",
    "print(f\"\\nPredicted labels: {pred_labels.flatten()}\")\n",
    "print(f\"Predicted probabilities: {pred_probs.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Multi-class Classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-class classification network\n",
    "# Architecture: 2 inputs -> 5 hidden (ReLU) -> 3 outputs (softmax)\n",
    "multiclass_dims = [2, 5, 3]\n",
    "multiclass_activations = ['relu', 'softmax']\n",
    "\n",
    "# Initialize parameters\n",
    "mc_parameters = forward_prop.initialize_parameters(multiclass_dims)\n",
    "\n",
    "# Create test data\n",
    "X_mc = np.random.randn(2, 8)  # 2 features, 8 examples\n",
    "\n",
    "print(\"Multi-class Network:\")\n",
    "print(f\"Architecture: {multiclass_dims}\")\n",
    "print(f\"Activations: {multiclass_activations}\")\n",
    "print(f\"Input shape: {X_mc.shape}\")\n",
    "\n",
    "# Forward propagation\n",
    "mc_predictions, mc_caches = forward_prop.forward_propagation(X_mc, mc_parameters, multiclass_activations)\n",
    "\n",
    "print(f\"\\nOutput shape: {mc_predictions.shape}\")\n",
    "print(f\"Sum of probabilities (should be ~1.0): {np.sum(mc_predictions, axis=0)}\")\n",
    "print(f\"Predictions (first 4 examples):\\n{mc_predictions[:, :4]}\")\n",
    "\n",
    "# Make predictions\n",
    "mc_labels, mc_probs = forward_prop.predict(X_mc, mc_parameters, multiclass_activations)\n",
    "print(f\"\\nPredicted classes: {mc_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Comparison (5 minutes)\n",
    "\n",
    "Let's compare vectorized vs non-vectorized implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_loop(X, parameters, activation_functions):\n",
    "    \"\"\"\n",
    "    Non-vectorized forward propagation (for comparison)\n",
    "    Processes examples one by one\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Initialize output array\n",
    "    if activation_functions[-1] == 'softmax':\n",
    "        n_output = parameters[f'W{L}'].shape[0]\n",
    "    else:\n",
    "        n_output = parameters[f'W{L}'].shape[0]\n",
    "    \n",
    "    AL_loop = np.zeros((n_output, m))\n",
    "    \n",
    "    # Process each example individually\n",
    "    for i in range(m):\n",
    "        x_i = X[:, i:i+1]  # Single example\n",
    "        a = x_i\n",
    "        \n",
    "        # Forward through all layers\n",
    "        for l in range(1, L + 1):\n",
    "            w = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            z = np.dot(w, a) + b\n",
    "            \n",
    "            # Apply activation\n",
    "            if activation_functions[l-1] == 'relu':\n",
    "                a = np.maximum(0, z)\n",
    "            elif activation_functions[l-1] == 'sigmoid':\n",
    "                a = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "            elif activation_functions[l-1] == 'softmax':\n",
    "                exp_z = np.exp(z - np.max(z))\n",
    "                a = exp_z / np.sum(exp_z)\n",
    "        \n",
    "        AL_loop[:, i:i+1] = a\n",
    "    \n",
    "    return AL_loop\n",
    "\n",
    "# Performance comparison\n",
    "# Create larger test data\n",
    "large_X = np.random.randn(10, 1000)  # 10 features, 1000 examples\n",
    "large_dims = [10, 20, 5, 1]\n",
    "large_activations = ['relu', 'relu', 'sigmoid']\n",
    "large_params = forward_prop.initialize_parameters(large_dims)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Test data: {large_X.shape[0]} features, {large_X.shape[1]} examples\")\n",
    "print(f\"Network: {large_dims}\")\n",
    "\n",
    "# Vectorized version\n",
    "start_time = time.time()\n",
    "vectorized_result, _ = forward_prop.forward_propagation(large_X, large_params, large_activations)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "# Loop version\n",
    "start_time = time.time()\n",
    "loop_result = forward_propagation_loop(large_X, large_params, large_activations)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nVectorized time: {vectorized_time:.4f} seconds\")\n",
    "print(f\"Loop time: {loop_time:.4f} seconds\")\n",
    "print(f\"Speedup: {loop_time/vectorized_time:.1f}x faster\")\n",
    "\n",
    "# Verify results are the same\n",
    "diff = np.max(np.abs(vectorized_result - loop_result))\n",
    "print(f\"Maximum difference: {diff:.10f}\")\n",
    "print(\"‚úÖ Results match!\" if diff < 1e-10 else \"‚ùå Results don't match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real Data Example (5 minutes)\n",
    "\n",
    "Let's test our forward propagation on a real dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a realistic dataset\n",
    "X_real, y_real = make_moons(n_samples=200, noise=0.3, random_state=42)\n",
    "\n",
    "# Prepare data\n",
    "X_real = X_real.T  # Transpose to (features, examples)\n",
    "y_real = y_real.reshape(1, -1)  # Reshape to (1, examples)\n",
    "\n",
    "# Split into train/test\n",
    "X_train = X_real[:, :150]\n",
    "X_test = X_real[:, 150:]\n",
    "y_train = y_real[:, :150]\n",
    "y_test = y_real[:, 150:]\n",
    "\n",
    "print(\"Real Dataset:\")\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Training labels: {y_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")\n",
    "print(f\"Test labels: {y_test.shape}\")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_train.flatten() == i\n",
    "    plt.scatter(X_train[0, mask], X_train[1, mask], c=colors[i], label=f'Class {i}', alpha=0.7)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(2):\n",
    "    mask = y_test.flatten() == i\n",
    "    plt.scatter(X_test[0, mask], X_test[1, mask], c=colors[i], label=f'Class {i}', alpha=0.7)\n",
    "plt.title('Test Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network for the real dataset\n",
    "real_dims = [2, 4, 1]  # 2 features -> 4 hidden -> 1 output\n",
    "real_activations = ['tanh', 'sigmoid']\n",
    "real_params = forward_prop.initialize_parameters(real_dims)\n",
    "\n",
    "# Forward propagation on training data\n",
    "train_predictions, train_caches = forward_prop.forward_propagation(X_train, real_params, real_activations)\n",
    "\n",
    "# Forward propagation on test data\n",
    "test_predictions, test_caches = forward_prop.forward_propagation(X_test, real_params, real_activations)\n",
    "\n",
    "print(\"Forward Propagation on Real Data:\")\n",
    "print(f\"Training predictions shape: {train_predictions.shape}\")\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "\n",
    "# Make binary predictions\n",
    "train_pred_labels, train_probs = forward_prop.predict(X_train, real_params, real_activations)\n",
    "test_pred_labels, test_probs = forward_prop.predict(X_test, real_params, real_activations)\n",
    "\n",
    "print(f\"\\nTraining prediction summary:\")\n",
    "print(f\"Predicted class 0: {np.sum(train_pred_labels == 0)} examples\")\n",
    "print(f\"Predicted class 1: {np.sum(train_pred_labels == 1)} examples\")\n",
    "print(f\"Actual class 0: {np.sum(y_train == 0)} examples\")\n",
    "print(f\"Actual class 1: {np.sum(y_train == 1)} examples\")\n",
    "\n",
    "print(f\"\\nTest prediction summary:\")\n",
    "print(f\"Predicted class 0: {np.sum(test_pred_labels == 0)} examples\")\n",
    "print(f\"Predicted class 1: {np.sum(test_pred_labels == 1)} examples\")\n",
    "print(f\"Actual class 0: {np.sum(y_test == 0)} examples\")\n",
    "print(f\"Actual class 1: {np.sum(y_test == 1)} examples\")\n",
    "\n",
    "# Note: Random weights won't give good accuracy - this is just testing forward propagation\n",
    "train_accuracy = np.mean(train_pred_labels == y_train) * 100\n",
    "test_accuracy = np.mean(test_pred_labels == y_test) * 100\n",
    "\n",
    "print(f\"\\nRandom weights performance:\")\n",
    "print(f\"Training accuracy: {train_accuracy:.1f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy:.1f}%\")\n",
    "print(\"Note: Low accuracy expected with random weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Activation Functions**: Implemented and tested activation functions\n",
    "- [ ] **Forward Propagation Class**: Created vectorized forward propagation\n",
    "- [ ] **Parameter Initialization**: Implemented weight and bias initialization\n",
    "- [ ] **Linear Forward**: Implemented linear transformation step\n",
    "- [ ] **Activation Forward**: Implemented activation step\n",
    "- [ ] **Full Forward Pass**: Implemented complete forward propagation\n",
    "- [ ] **Simple Network Test**: Tested on simple binary classification network\n",
    "- [ ] **Multi-class Test**: Tested on multi-class classification network\n",
    "- [ ] **Performance Comparison**: Compared vectorized vs loop implementation\n",
    "- [ ] **Real Data Test**: Applied forward propagation to real dataset\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Vectorization Benefits**: Massive performance improvements over loops\n",
    "2. **Forward Propagation Steps**: Linear transformation ‚Üí Activation\n",
    "3. **Matrix Operations**: Understanding of weight-input multiplication\n",
    "4. **Batch Processing**: Handling multiple examples simultaneously\n",
    "5. **Activation Functions**: Different functions for different layer types\n",
    "\n",
    "### Key Mathematical Concepts:\n",
    "- **Linear Transformation**: Z^[l] = W^[l] * A^[l-1] + b^[l]\n",
    "- **Activation**: A^[l] = g^[l](Z^[l])\n",
    "- **Vectorization**: Processing multiple examples in parallel\n",
    "- **Parameter Shapes**: Ensuring compatible matrix dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps\n",
    "\n",
    "Run the following cells to validate your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Shape Consistency\n",
    "def test_shape_consistency():\n",
    "    \"\"\"Test if forward propagation maintains correct shapes\"\"\"\n",
    "    try:\n",
    "        # Test different architectures\n",
    "        test_cases = [\n",
    "            {'dims': [3, 5, 1], 'examples': 10},\n",
    "            {'dims': [10, 20, 15, 5], 'examples': 50},\n",
    "            {'dims': [2, 8, 3], 'examples': 25}\n",
    "        ]\n",
    "        \n",
    "        for case in test_cases:\n",
    "            dims = case['dims']\n",
    "            m = case['examples']\n",
    "            \n",
    "            # Create test data\n",
    "            X_test = np.random.randn(dims[0], m)\n",
    "            params_test = forward_prop.initialize_parameters(dims)\n",
    "            activations_test = ['relu'] * (len(dims) - 2) + ['sigmoid']\n",
    "            \n",
    "            # Forward propagation\n",
    "            output, caches = forward_prop.forward_propagation(X_test, params_test, activations_test)\n",
    "            \n",
    "            # Check output shape\n",
    "            expected_shape = (dims[-1], m)\n",
    "            assert output.shape == expected_shape, f\"Expected {expected_shape}, got {output.shape}\"\n",
    "            \n",
    "            # Check cache length\n",
    "            expected_layers = len(dims) - 1\n",
    "            assert len(caches) == expected_layers, f\"Expected {expected_layers} caches, got {len(caches)}\"\n",
    "        \n",
    "        print(\"‚úÖ Shape consistency test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Shape consistency test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_shape_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Activation Function Correctness\n",
    "def test_activation_functions():\n",
    "    \"\"\"Test activation functions work correctly\"\"\"\n",
    "    try:\n",
    "        # Test specific values\n",
    "        z = np.array([[-1, 0, 1], [2, -2, 0.5]])\n",
    "        \n",
    "        # ReLU test\n",
    "        relu_result = ActivationFunctions.relu(z)\n",
    "        expected_relu = np.array([[0, 0, 1], [2, 0, 0.5]])\n",
    "        assert np.allclose(relu_result, expected_relu), \"ReLU test failed\"\n",
    "        \n",
    "        # Sigmoid test (check range)\n",
    "        sigmoid_result = ActivationFunctions.sigmoid(z)\n",
    "        assert np.all((sigmoid_result >= 0) & (sigmoid_result <= 1)), \"Sigmoid range test failed\"\n",
    "        \n",
    "        # Softmax test (check probabilities sum to 1)\n",
    "        softmax_result = ActivationFunctions.softmax(z)\n",
    "        column_sums = np.sum(softmax_result, axis=0)\n",
    "        assert np.allclose(column_sums, 1.0), \"Softmax probability sum test failed\"\n",
    "        \n",
    "        print(\"‚úÖ Activation functions test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Activation functions test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_activation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 3: Prediction Consistency\n",
    "def test_prediction_consistency():\n",
    "    \"\"\"Test if predictions are consistent with forward propagation\"\"\"\n",
    "    try:\n",
    "        # Create test network\n",
    "        dims = [4, 6, 1]\n",
    "        X_test = np.random.randn(4, 20)\n",
    "        params = forward_prop.initialize_parameters(dims)\n",
    "        activations = ['relu', 'sigmoid']\n",
    "        \n",
    "        # Forward propagation\n",
    "        output, _ = forward_prop.forward_propagation(X_test, params, activations)\n",
    "        \n",
    "        # Predictions\n",
    "        pred_labels, pred_probs = forward_prop.predict(X_test, params, activations)\n",
    "        \n",
    "        # Check consistency\n",
    "        assert np.allclose(output, pred_probs), \"Prediction probabilities don't match forward propagation\"\n",
    "        \n",
    "        # Check binary predictions\n",
    "        expected_labels = (output > 0.5).astype(int)\n",
    "        assert np.array_equal(pred_labels, expected_labels), \"Prediction labels incorrect\"\n",
    "        \n",
    "        print(\"‚úÖ Prediction consistency test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction consistency test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_prediction_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Shape mismatch errors**\n",
    "- **Cause**: Incompatible matrix dimensions\n",
    "- **Solution**: Check that W^[l] has shape (n^[l], n^[l-1]) and A^[l-1] has shape (n^[l-1], m)\n",
    "\n",
    "**Issue 2: Numerical overflow in sigmoid/softmax**\n",
    "- **Cause**: Very large input values\n",
    "- **Solution**: Use np.clip() to limit input range, implemented in our functions\n",
    "\n",
    "**Issue 3: Softmax probabilities don't sum to 1**\n",
    "- **Cause**: Numerical instability\n",
    "- **Solution**: Subtract max value before exponential (implemented)\n",
    "\n",
    "**Issue 4: Slow performance**\n",
    "- **Cause**: Using loops instead of vectorization\n",
    "- **Solution**: Use matrix operations with NumPy, process all examples together\n",
    "\n",
    "**Issue 5: Cache errors in backpropagation**\n",
    "- **Cause**: Incorrect cache structure\n",
    "- **Solution**: Ensure each cache contains (linear_cache, activation_cache)\n",
    "\n",
    "### Getting Help:\n",
    "- Check input/output shapes carefully\n",
    "- Verify activation function implementations\n",
    "- Test with small examples first\n",
    "- Print intermediate values for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell ‚Üí All Output ‚Üí Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéâ Lab 3.2: Forward Propagation Vectorization Completed!\")\n",
    "print(\"\\nüìã What you accomplished:\")\n",
    "print(\"‚úÖ Implemented vectorized forward propagation\")\n",
    "print(\"‚úÖ Created efficient activation functions\")\n",
    "print(\"‚úÖ Tested on multiple network architectures\")\n",
    "print(\"‚úÖ Demonstrated massive performance improvements\")\n",
    "print(\"‚úÖ Applied to real datasets\")\n",
    "print(\"\\nüéØ Next: Lab 3.3 - Backward Propagation Implementation\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nüßπ Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
