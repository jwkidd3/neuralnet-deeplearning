{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3: Backward Propagation Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematics behind backward propagation\n",
    "- Implement backward propagation using vectorized operations\n",
    "- Calculate gradients for weights and biases in all layers\n",
    "- Verify gradient calculations using numerical methods\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 3.1 and 3.2\n",
    "- Understanding of chain rule in calculus\n",
    "- Knowledge of forward propagation mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Backward Propagation Theory (8 minutes)\n",
    "\n",
    "### The Chain Rule in Neural Networks\n",
    "\n",
    "Backward propagation uses the chain rule to compute gradients:\n",
    "\n",
    "For layer l:\n",
    "- **dW^[l]** = (1/m) * dZ^[l] * A^[l-1].T\n",
    "- **db^[l]** = (1/m) * sum(dZ^[l], axis=1, keepdims=True)\n",
    "- **dA^[l-1]** = W^[l].T * dZ^[l]\n",
    "\n",
    "Where:\n",
    "- **dZ^[l]** = dA^[l] * g'^[l](Z^[l])\n",
    "- g'^[l] is the derivative of the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the activation functions from Lab 3.2\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        return np.ones_like(z)\n",
    "\n",
    "print(\"Activation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cost Functions (7 minutes)\n",
    "\n",
    "Before implementing backward propagation, we need cost functions to compute initial gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunctions:\n",
    "    \"\"\"Cost functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(AL, Y):\n",
    "        \"\"\"\n",
    "        Binary cross-entropy cost function\n",
    "        \n",
    "        Parameters:\n",
    "        AL: predictions (1, m)\n",
    "        Y: true labels (1, m)\n",
    "        \n",
    "        Returns:\n",
    "        cost: scalar cost\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Avoid log(0) by clipping predictions\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "        cost = np.squeeze(cost)  # Remove extra dimensions\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy_derivative(AL, Y):\n",
    "        \"\"\"\n",
    "        Derivative of binary cross-entropy\n",
    "        \n",
    "        Returns:\n",
    "        dAL: gradient with respect to final layer activations\n",
    "        \"\"\"\n",
    "        # Avoid division by zero\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        dAL = -(Y / AL) + (1 - Y) / (1 - AL)\n",
    "        \n",
    "        return dAL\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error(AL, Y):\n",
    "        \"\"\"\n",
    "        Mean squared error cost function\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = 1/(2*m) * np.sum(np.square(AL - Y))\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error_derivative(AL, Y):\n",
    "        \"\"\"\n",
    "        Derivative of mean squared error\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        dAL = 1/m * (AL - Y)\n",
    "        return dAL\n",
    "\n",
    "# Test cost functions\n",
    "# Generate test data\n",
    "test_predictions = np.array([[0.8, 0.2, 0.9, 0.1]])\n",
    "test_labels = np.array([[1, 0, 1, 0]])\n",
    "\n",
    "cost = CostFunctions.binary_cross_entropy(test_predictions, test_labels)\n",
    "dAL = CostFunctions.binary_cross_entropy_derivative(test_predictions, test_labels)\n",
    "\n",
    "print(\"Cost Functions Test:\")\n",
    "print(f\"Predictions: {test_predictions.flatten()}\")\n",
    "print(f\"True labels: {test_labels.flatten()}\")\n",
    "print(f\"Binary cross-entropy cost: {cost:.4f}\")\n",
    "print(f\"Cost derivative: {dAL.flatten()}\")\n",
    "print(\"Cost functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Backward Propagation Implementation (20 minutes)\n",
    "\n",
    "Now let's implement the complete backward propagation algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardPropagation:\n",
    "    \"\"\"\n",
    "    Complete backward propagation implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = ActivationFunctions()\n",
    "        self.cost_functions = CostFunctions()\n",
    "    \n",
    "    def linear_backward(self, dZ, cache):\n",
    "        \"\"\"\n",
    "        Linear portion of backward propagation for one layer\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of cost with respect to linear output of layer l\n",
    "        cache: tuple of (A_prev, W, b) from forward propagation\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev: gradient of cost with respect to activation of layer l-1\n",
    "        dW: gradient of cost with respect to weights of layer l\n",
    "        db: gradient of cost with respect to bias of layer l\n",
    "        \"\"\"\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        # Compute gradients\n",
    "        dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def linear_activation_backward(self, dA, cache, activation):\n",
    "        \"\"\"\n",
    "        Backward propagation for one layer (linear + activation)\n",
    "        \n",
    "        Parameters:\n",
    "        dA: gradient of cost with respect to activation of layer l\n",
    "        cache: tuple of (linear_cache, activation_cache)\n",
    "        activation: activation function name\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev: gradient of cost with respect to activation of layer l-1\n",
    "        dW: gradient of cost with respect to weights of layer l\n",
    "        db: gradient of cost with respect to bias of layer l\n",
    "        \"\"\"\n",
    "        linear_cache, activation_cache = cache\n",
    "        \n",
    "        # Compute dZ based on activation function\n",
    "        if activation == 'relu':\n",
    "            dZ = dA * self.activations.relu_derivative(activation_cache)\n",
    "        elif activation == 'sigmoid':\n",
    "            dZ = dA * self.activations.sigmoid_derivative(activation_cache)\n",
    "        elif activation == 'tanh':\n",
    "            dZ = dA * self.activations.tanh_derivative(activation_cache)\n",
    "        elif activation == 'linear':\n",
    "            dZ = dA * self.activations.linear_derivative(activation_cache)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Linear backward\n",
    "        dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches, activation_functions, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Complete backward propagation for the entire network\n",
    "        \n",
    "        Parameters:\n",
    "        AL: probability vector, output of forward propagation (output_size, m)\n",
    "        Y: true \"label\" vector (output_size, m)\n",
    "        caches: list of caches from forward propagation\n",
    "        activation_functions: list of activation functions for each layer\n",
    "        cost_function: name of cost function to use\n",
    "        \n",
    "        Returns:\n",
    "        gradients: dictionary with gradients for each parameter\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        L = len(caches)  # Number of layers\n",
    "        Y = Y.reshape(AL.shape)  # Ensure Y has same shape as AL\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        if cost_function == 'binary_cross_entropy':\n",
    "            dAL = self.cost_functions.binary_cross_entropy_derivative(AL, Y)\n",
    "        elif cost_function == 'mean_squared_error':\n",
    "            dAL = self.cost_functions.mean_squared_error_derivative(AL, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported cost function: {cost_function}\")\n",
    "        \n",
    "        # Backward propagation through all layers\n",
    "        dA = dAL\n",
    "        \n",
    "        for l in reversed(range(L)):\n",
    "            cache = caches[l]\n",
    "            activation = activation_functions[l]\n",
    "            \n",
    "            dA_prev, dW, db = self.linear_activation_backward(dA, cache, activation)\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients[f'dW{l+1}'] = dW\n",
    "            gradients[f'db{l+1}'] = db\n",
    "            \n",
    "            # Update dA for next iteration\n",
    "            dA = dA_prev\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Compute the cost\n",
    "        \"\"\"\n",
    "        if cost_function == 'binary_cross_entropy':\n",
    "            return self.cost_functions.binary_cross_entropy(AL, Y)\n",
    "        elif cost_function == 'mean_squared_error':\n",
    "            return self.cost_functions.mean_squared_error(AL, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported cost function: {cost_function}\")\n",
    "\n",
    "print(\"BackwardPropagation class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Integration with Forward Propagation (5 minutes)\n",
    "\n",
    "Let's combine forward and backward propagation in a complete neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import/recreate the forward propagation from Lab 3.2\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Complete neural network with forward and backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = ActivationFunctions()\n",
    "        self.backward_prop = BackwardPropagation()\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims):\n",
    "        \"\"\"Initialize weights and biases\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            # Xavier initialization\n",
    "            parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, activation_functions):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            \n",
    "            # Linear forward\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            linear_cache = (A_prev, W, b)\n",
    "            \n",
    "            # Activation forward\n",
    "            activation = activation_functions[l-1]\n",
    "            if activation == 'sigmoid':\n",
    "                A = self.activations.sigmoid(Z)\n",
    "            elif activation == 'relu':\n",
    "                A = self.activations.relu(Z)\n",
    "            elif activation == 'tanh':\n",
    "                A = self.activations.tanh(Z)\n",
    "            elif activation == 'linear':\n",
    "                A = self.activations.linear(Z)\n",
    "            \n",
    "            cache = (linear_cache, Z)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return A, caches\n",
    "    \n",
    "    def compute_gradients(self, X, Y, parameters, activation_functions, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Compute gradients using forward and backward propagation\n",
    "        \n",
    "        Returns:\n",
    "        cost: computed cost\n",
    "        gradients: gradients for all parameters\n",
    "        \"\"\"\n",
    "        # Forward propagation\n",
    "        AL, caches = self.forward_propagation(X, parameters, activation_functions)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = self.backward_prop.compute_cost(AL, Y, cost_function)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = self.backward_prop.backward_propagation(AL, Y, caches, activation_functions, cost_function)\n",
    "        \n",
    "        return cost, gradients, AL\n",
    "\n",
    "print(\"Complete NeuralNetwork class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Testing Backward Propagation (5 minutes)\n",
    "\n",
    "### Test 1: Simple Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test case\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Network architecture\n",
    "layer_dims = [2, 3, 1]  # 2 inputs -> 3 hidden -> 1 output\n",
    "activation_functions = ['relu', 'sigmoid']\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = nn.initialize_parameters(layer_dims)\n",
    "\n",
    "# Create test data\n",
    "X_test = np.array([[1.5, 2.0], [0.5, -1.0], [2.5, 1.5], [-0.5, 0.8]]).T  # (2, 4)\n",
    "Y_test = np.array([[1, 0, 1, 0]])  # (1, 4)\n",
    "\n",
    "print(\"Test Setup:\")\n",
    "print(f\"Network: {layer_dims}\")\n",
    "print(f\"Activations: {activation_functions}\")\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Labels shape: {Y_test.shape}\")\n",
    "print(f\"Test data:\\nX =\\n{X_test}\")\n",
    "print(f\"Y = {Y_test.flatten()}\")\n",
    "\n",
    "# Compute gradients\n",
    "cost, gradients, predictions = nn.compute_gradients(X_test, Y_test, parameters, activation_functions)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Cost: {cost:.6f}\")\n",
    "print(f\"Predictions: {predictions.flatten()}\")\n",
    "print(\"\\nGradient shapes:\")\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"{key}: {grad.shape}\")\n",
    "\n",
    "print(\"\\nGradient values:\")\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Gradient Checking (Numerical Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, Y, parameters, activation_functions, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Gradient checking using numerical differentiation\n",
    "    \n",
    "    Parameters:\n",
    "    epsilon: small value for numerical differentiation\n",
    "    \n",
    "    Returns:\n",
    "    difference: relative difference between analytical and numerical gradients\n",
    "    \"\"\"\n",
    "    # Compute analytical gradients\n",
    "    cost, gradients, _ = nn.compute_gradients(X, Y, parameters, activation_functions)\n",
    "    \n",
    "    # Convert gradients to vector\n",
    "    grad_vector = []\n",
    "    param_names = []\n",
    "    \n",
    "    for key in sorted(gradients.keys()):\n",
    "        grad_vector.extend(gradients[key].flatten())\n",
    "        param_names.extend([key] * gradients[key].size)\n",
    "    \n",
    "    grad_vector = np.array(grad_vector)\n",
    "    \n",
    "    # Compute numerical gradients\n",
    "    num_grad_vector = np.zeros_like(grad_vector)\n",
    "    \n",
    "    # Convert parameters to vector\n",
    "    param_vector = []\n",
    "    for l in range(1, len(parameters)//2 + 1):\n",
    "        param_vector.extend(parameters[f'W{l}'].flatten())\n",
    "        param_vector.extend(parameters[f'b{l}'].flatten())\n",
    "    \n",
    "    param_vector = np.array(param_vector)\n",
    "    \n",
    "    # Check only a subset of parameters for efficiency\n",
    "    check_indices = np.random.choice(len(param_vector), min(20, len(param_vector)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(check_indices):\n",
    "        # Reconstruct parameters with theta + epsilon\n",
    "        param_plus = param_vector.copy()\n",
    "        param_plus[idx] += epsilon\n",
    "        params_plus = vector_to_parameters(param_plus, parameters)\n",
    "        \n",
    "        # Reconstruct parameters with theta - epsilon\n",
    "        param_minus = param_vector.copy()\n",
    "        param_minus[idx] -= epsilon\n",
    "        params_minus = vector_to_parameters(param_minus, parameters)\n",
    "        \n",
    "        # Compute costs\n",
    "        AL_plus, _ = nn.forward_propagation(X, params_plus, activation_functions)\n",
    "        cost_plus = nn.backward_prop.compute_cost(AL_plus, Y)\n",
    "        \n",
    "        AL_minus, _ = nn.forward_propagation(X, params_minus, activation_functions)\n",
    "        cost_minus = nn.backward_prop.compute_cost(AL_minus, Y)\n",
    "        \n",
    "        # Numerical gradient\n",
    "        num_grad_vector[idx] = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Check only the indices we computed\n",
    "    analytical_subset = grad_vector[check_indices]\n",
    "    numerical_subset = num_grad_vector[check_indices]\n",
    "    \n",
    "    # Compute relative difference\n",
    "    numerator = np.linalg.norm(analytical_subset - numerical_subset)\n",
    "    denominator = np.linalg.norm(analytical_subset) + np.linalg.norm(numerical_subset)\n",
    "    difference = numerator / denominator if denominator != 0 else 0\n",
    "    \n",
    "    return difference, analytical_subset, numerical_subset\n",
    "\n",
    "def vector_to_parameters(param_vector, template_params):\n",
    "    \"\"\"Convert parameter vector back to parameter dictionary\"\"\"\n",
    "    parameters = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for l in range(1, len(template_params)//2 + 1):\n",
    "        W_shape = template_params[f'W{l}'].shape\n",
    "        b_shape = template_params[f'b{l}'].shape\n",
    "        \n",
    "        W_size = np.prod(W_shape)\n",
    "        b_size = np.prod(b_shape)\n",
    "        \n",
    "        parameters[f'W{l}'] = param_vector[idx:idx+W_size].reshape(W_shape)\n",
    "        idx += W_size\n",
    "        \n",
    "        parameters[f'b{l}'] = param_vector[idx:idx+b_size].reshape(b_shape)\n",
    "        idx += b_size\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Perform gradient check on our test case\n",
    "print(\"Performing Gradient Check...\")\n",
    "difference, analytical, numerical = gradient_check(nn, X_test, Y_test, parameters, activation_functions)\n",
    "\n",
    "print(f\"\\nGradient Check Results:\")\n",
    "print(f\"Relative difference: {difference:.2e}\")\n",
    "print(f\"Sample analytical gradients: {analytical[:5]}\")\n",
    "print(f\"Sample numerical gradients: {numerical[:5]}\")\n",
    "\n",
    "if difference < 1e-5:\n",
    "    print(\"âœ… Gradient check PASSED! Backpropagation is correct.\")\n",
    "elif difference < 1e-3:\n",
    "    print(\"âš ï¸ Gradient check WARNING: Small discrepancy detected.\")\n",
    "else:\n",
    "    print(\"âŒ Gradient check FAILED: Large discrepancy detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Theory Understanding**: Reviewed backward propagation mathematics\n",
    "- [ ] **Activation Derivatives**: Implemented activation function derivatives\n",
    "- [ ] **Cost Functions**: Implemented cost functions and their derivatives\n",
    "- [ ] **Linear Backward**: Implemented linear portion of backward propagation\n",
    "- [ ] **Activation Backward**: Implemented activation portion of backward propagation\n",
    "- [ ] **Full Backward**: Implemented complete backward propagation\n",
    "- [ ] **Integration**: Combined forward and backward propagation\n",
    "- [ ] **Simple Test**: Tested on simple binary classification\n",
    "- [ ] **Gradient Check**: Verified gradients using numerical methods\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Chain Rule Application**: How calculus enables gradient computation\n",
    "2. **Gradient Flow**: How gradients propagate backward through layers\n",
    "3. **Vectorized Implementation**: Efficient gradient computation for multiple examples\n",
    "4. **Numerical Verification**: Using gradient checking to verify implementations\n",
    "5. **Cost Function Integration**: Connecting loss to gradient computation\n",
    "\n",
    "### Mathematical Foundation:\n",
    "- **Weight Gradients**: dW^[l] = (1/m) * dZ^[l] * A^[l-1].T\n",
    "- **Bias Gradients**: db^[l] = (1/m) * sum(dZ^[l])\n",
    "- **Activation Gradients**: dA^[l-1] = W^[l].T * dZ^[l]\n",
    "- **Chain Rule**: dZ^[l] = dA^[l] * g'^[l](Z^[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Gradient Shapes\n",
    "def test_gradient_shapes():\n",
    "    \"\"\"Test if gradients have correct shapes\"\"\"\n",
    "    try:\n",
    "        test_nn = NeuralNetwork()\n",
    "        test_dims = [3, 5, 2, 1]\n",
    "        test_activations = ['relu', 'tanh', 'sigmoid']\n",
    "        test_params = test_nn.initialize_parameters(test_dims)\n",
    "        \n",
    "        X_test = np.random.randn(3, 10)\n",
    "        Y_test = np.random.randint(0, 2, (1, 10))\n",
    "        \n",
    "        _, gradients, _ = test_nn.compute_gradients(X_test, Y_test, test_params, test_activations)\n",
    "        \n",
    "        # Check gradient shapes match parameter shapes\n",
    "        for l in range(1, len(test_dims)):\n",
    "            W_shape = test_params[f'W{l}'].shape\n",
    "            b_shape = test_params[f'b{l}'].shape\n",
    "            \n",
    "            assert gradients[f'dW{l}'].shape == W_shape, f\"dW{l} shape mismatch\"\n",
    "            assert gradients[f'db{l}'].shape == b_shape, f\"db{l} shape mismatch\"\n",
    "        \n",
    "        print(\"âœ… Gradient shapes test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gradient shapes test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_gradient_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Cost Decreases\n",
    "def test_cost_behavior():\n",
    "    \"\"\"Test if cost behaves correctly\"\"\"\n",
    "    try:\n",
    "        test_nn = NeuralNetwork()\n",
    "        \n",
    "        # Perfect predictions should have low cost\n",
    "        Y_perfect = np.array([[1, 0, 1, 0]])\n",
    "        AL_perfect = np.array([[0.99, 0.01, 0.98, 0.02]])\n",
    "        cost_perfect = test_nn.backward_prop.compute_cost(AL_perfect, Y_perfect)\n",
    "        \n",
    "        # Random predictions should have higher cost\n",
    "        AL_random = np.array([[0.5, 0.5, 0.5, 0.5]])\n",
    "        cost_random = test_nn.backward_prop.compute_cost(AL_random, Y_perfect)\n",
    "        \n",
    "        # Wrong predictions should have highest cost\n",
    "        AL_wrong = np.array([[0.01, 0.99, 0.02, 0.98]])\n",
    "        cost_wrong = test_nn.backward_prop.compute_cost(AL_wrong, Y_perfect)\n",
    "        \n",
    "        assert cost_perfect < cost_random < cost_wrong, \"Cost ordering incorrect\"\n",
    "        \n",
    "        print(f\"Perfect predictions cost: {cost_perfect:.4f}\")\n",
    "        print(f\"Random predictions cost: {cost_random:.4f}\")\n",
    "        print(f\"Wrong predictions cost: {cost_wrong:.4f}\")\n",
    "        print(\"âœ… Cost behavior test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cost behavior test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_cost_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Gradient explosion/vanishing**\n",
    "- **Cause**: Poor weight initialization or activation functions\n",
    "- **Solution**: Use Xavier initialization, avoid sigmoid in deep networks\n",
    "\n",
    "**Issue 2: Gradient check fails**\n",
    "- **Cause**: Implementation errors in backward propagation\n",
    "- **Solution**: Check activation derivatives, matrix dimensions, chain rule application\n",
    "\n",
    "**Issue 3: Shape mismatch errors**\n",
    "- **Cause**: Incorrect matrix operations\n",
    "- **Solution**: Verify cache structure, gradient shapes match parameter shapes\n",
    "\n",
    "**Issue 4: NaN or infinite values**\n",
    "- **Cause**: Numerical instability in activation/cost functions\n",
    "- **Solution**: Use clipping in sigmoid/log functions, check for divide by zero\n",
    "\n",
    "**Issue 5: Slow convergence**\n",
    "- **Cause**: Poor gradient computation or learning rate\n",
    "- **Solution**: Verify backward propagation implementation, tune hyperparameters\n",
    "\n",
    "### Debugging Tips:\n",
    "- Use gradient checking on small networks first\n",
    "- Print intermediate gradient values\n",
    "- Check gradient norms (should not be too large or small)\n",
    "- Verify cost decreases over iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell â†’ All Output â†’ Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"ðŸŽ‰ Lab 3.3: Backward Propagation Implementation Completed!\")\n",
    "print(\"\\nðŸ“‹ What you accomplished:\")\n",
    "print(\"âœ… Implemented complete backward propagation algorithm\")\n",
    "print(\"âœ… Created cost functions and their derivatives\")\n",
    "print(\"âœ… Integrated forward and backward propagation\")\n",
    "print(\"âœ… Verified implementation with gradient checking\")\n",
    "print(\"âœ… Tested on real neural network architectures\")\n",
    "print(\"\\nðŸŽ¯ Next: Lab 3.4 - Multi-class Classification Setup\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nðŸ§¹ Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
