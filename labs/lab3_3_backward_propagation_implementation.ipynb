{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3: Backward Propagation Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematics behind backward propagation\n",
    "- Implement backward propagation using vectorized operations\n",
    "- Calculate gradients for weights and biases in all layers\n",
    "- Verify gradient calculations using numerical methods\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 3.1 and 3.2\n",
    "- Understanding of chain rule in calculus\n",
    "- Knowledge of forward propagation mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Backward Propagation Theory (8 minutes)\n",
    "\n",
    "### The Chain Rule in Neural Networks\n",
    "\n",
    "Backward propagation uses the chain rule to compute gradients:\n",
    "\n",
    "For layer l:\n",
    "- **dW^[l]** = (1/m) * dZ^[l] * A^[l-1].T\n",
    "- **db^[l]** = (1/m) * sum(dZ^[l], axis=1, keepdims=True)\n",
    "- **dA^[l-1]** = W^[l].T * dZ^[l]\n",
    "\n",
    "Where:\n",
    "- **dZ^[l]** = dA^[l] * g'^[l](Z^[l])\n",
    "- g'^[l] is the derivative of the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the activation functions from Lab 3.2\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        return np.ones_like(z)\n",
    "\n",
    "print(\"Activation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cost Functions (7 minutes)\n",
    "\n",
    "Before implementing backward propagation, we need cost functions to compute initial gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunctions:\n",
    "    \"\"\"Cost functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(AL, Y):\n",
    "        \"\"\"\n",
    "        Binary cross-entropy cost function\n",
    "        \n",
    "        Parameters:\n",
    "        AL: predictions (1, m)\n",
    "        Y: true labels (1, m)\n",
    "        \n",
    "        Returns:\n",
    "        cost: scalar cost\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Avoid log(0) by clipping predictions\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "        cost = np.squeeze(cost)  # Remove extra dimensions\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy_derivative(AL, Y):\n",
    "        \"\"\"\n",
    "        Derivative of binary cross-entropy\n",
    "        \n",
    "        Returns:\n",
    "        dAL: gradient with respect to final layer activations\n",
    "        \"\"\"\n",
    "        # Avoid division by zero\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        dAL = -(Y / AL) + (1 - Y) / (1 - AL)\n",
    "        \n",
    "        return dAL\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error(AL, Y):\n",
    "        \"\"\"\n",
    "        Mean squared error cost function\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = 1/(2*m) * np.sum(np.square(AL - Y))\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error_derivative(AL, Y):\n",
    "        \"\"\"\n",
    "        Derivative of mean squared error\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        dAL = 1/m * (AL - Y)\n",
    "        return dAL\n",
    "\n",
    "# Test cost functions\n",
    "# Generate test data\n",
    "test_predictions = np.array([[0.8, 0.2, 0.9, 0.1]])\n",
    "test_labels = np.array([[1, 0, 1, 0]])\n",
    "\n",
    "cost = CostFunctions.binary_cross_entropy(test_predictions, test_labels)\n",
    "dAL = CostFunctions.binary_cross_entropy_derivative(test_predictions, test_labels)\n",
    "\n",
    "print(\"Cost Functions Test:\")\n",
    "print(f\"Predictions: {test_predictions.flatten()}\")\n",
    "print(f\"True labels: {test_labels.flatten()}\")\n",
    "print(f\"Binary cross-entropy cost: {cost:.4f}\")\n",
    "print(f\"Cost derivative: {dAL.flatten()}\")\n",
    "print(\"Cost functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Backward Propagation Implementation (20 minutes)\n",
    "\n",
    "Now let's implement the complete backward propagation algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardPropagation:\n",
    "    \"\"\"\n",
    "    Complete backward propagation implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = ActivationFunctions()\n",
    "        self.cost_functions = CostFunctions()\n",
    "    \n",
    "    def linear_backward(self, dZ, cache):\n",
    "        \"\"\"\n",
    "        Linear portion of backward propagation for one layer\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: gradient of cost with respect to linear output of layer l\n",
    "        cache: tuple of (A_prev, W, b) from forward propagation\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev: gradient of cost with respect to activation of layer l-1\n",
    "        dW: gradient of cost with respect to weights of layer l\n",
    "        db: gradient of cost with respect to bias of layer l\n",
    "        \"\"\"\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        # Compute gradients\n",
    "        dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def linear_activation_backward(self, dA, cache, activation):\n",
    "        \"\"\"\n",
    "        Backward propagation for one layer (linear + activation)\n",
    "        \n",
    "        Parameters:\n",
    "        dA: gradient of cost with respect to activation of layer l\n",
    "        cache: tuple of (linear_cache, activation_cache)\n",
    "        activation: activation function name\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev: gradient of cost with respect to activation of layer l-1\n",
    "        dW: gradient of cost with respect to weights of layer l\n",
    "        db: gradient of cost with respect to bias of layer l\n",
    "        \"\"\"\n",
    "        linear_cache, activation_cache = cache\n",
    "        \n",
    "        # Compute dZ based on activation function\n",
    "        if activation == 'relu':\n",
    "            dZ = dA * self.activations.relu_derivative(activation_cache)\n",
    "        elif activation == 'sigmoid':\n",
    "            dZ = dA * self.activations.sigmoid_derivative(activation_cache)\n",
    "        elif activation == 'tanh':\n",
    "            dZ = dA * self.activations.tanh_derivative(activation_cache)\n",
    "        elif activation == 'linear':\n",
    "            dZ = dA * self.activations.linear_derivative(activation_cache)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Linear backward\n",
    "        dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches, activation_functions, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Complete backward propagation for the entire network\n",
    "        \n",
    "        Parameters:\n",
    "        AL: probability vector, output of forward propagation (output_size, m)\n",
    "        Y: true \"label\" vector (output_size, m)\n",
    "        caches: list of caches from forward propagation\n",
    "        activation_functions: list of activation functions for each layer\n",
    "        cost_function: name of cost function to use\n",
    "        \n",
    "        Returns:\n",
    "        gradients: dictionary with gradients for each parameter\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        L = len(caches)  # Number of layers\n",
    "        Y = Y.reshape(AL.shape)  # Ensure Y has same shape as AL\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        if cost_function == 'binary_cross_entropy':\n",
    "            dAL = self.cost_functions.binary_cross_entropy_derivative(AL, Y)\n",
    "        elif cost_function == 'mean_squared_error':\n",
    "            dAL = self.cost_functions.mean_squared_error_derivative(AL, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported cost function: {cost_function}\")\n",
    "        \n",
    "        # Backward propagation through all layers\n",
    "        dA = dAL\n",
    "        \n",
    "        for l in reversed(range(L)):\n",
    "            cache = caches[l]\n",
    "            activation = activation_functions[l]\n",
    "            \n",
    "            dA_prev, dW, db = self.linear_activation_backward(dA, cache, activation)\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients[f'dW{l+1}'] = dW\n",
    "            gradients[f'db{l+1}'] = db\n",
    "            \n",
    "            # Update dA for next iteration\n",
    "            dA = dA_prev\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Compute the cost\n",
    "        \"\"\"\n",
    "        if cost_function == 'binary_cross_entropy':\n",
    "            return self.cost_functions.binary_cross_entropy(AL, Y)\n",
    "        elif cost_function == 'mean_squared_error':\n",
    "            return self.cost_functions.mean_squared_error(AL, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported cost function: {cost_function}\")\n",
    "\n",
    "print(\"BackwardPropagation class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Integration with Forward Propagation (5 minutes)\n",
    "\n",
    "Let's combine forward and backward propagation in a complete neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import/recreate the forward propagation from Lab 3.2\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Complete neural network with forward and backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = ActivationFunctions()\n",
    "        self.backward_prop = BackwardPropagation()\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims):\n",
    "        \"\"\"Initialize weights and biases\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            # Xavier initialization\n",
    "            parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, activation_functions):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            \n",
    "            # Linear forward\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            linear_cache = (A_prev, W, b)\n",
    "            \n",
    "            # Activation forward\n",
    "            activation = activation_functions[l-1]\n",
    "            if activation == 'sigmoid':\n",
    "                A = self.activations.sigmoid(Z)\n",
    "            elif activation == 'relu':\n",
    "                A = self.activations.relu(Z)\n",
    "            elif activation == 'tanh':\n",
    "                A = self.activations.tanh(Z)\n",
    "            elif activation == 'linear':\n",
    "                A = self.activations.linear(Z)\n",
    "            \n",
    "            cache = (linear_cache, Z)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return A, caches\n",
    "    \n",
    "    def compute_gradients(self, X, Y, parameters, activation_functions, cost_function='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Compute gradients using forward and backward propagation\n",
    "        \n",
    "        Returns:\n",
    "        cost: computed cost\n",
    "        gradients: gradients for all parameters\n",
    "        \"\"\"\n",
    "        # Forward propagation\n",
    "        AL, caches = self.forward_propagation(X, parameters, activation_functions)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = self.backward_prop.compute_cost(AL, Y, cost_function)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = self.backward_prop.backward_propagation(AL, Y, caches, activation_functions, cost_function)\n",
    "        \n",
    "        return cost, gradients, AL\n",
    "\n",
    "print(\"Complete NeuralNetwork class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Testing Backward Propagation (5 minutes)\n",
    "\n",
    "### Test 1: Simple Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test case\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Network architecture\n",
    "layer_dims = [2, 3, 1]  # 2 inputs -> 3 hidden -> 1 output\n",
    "activation_functions = ['relu', 'sigmoid']\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = nn.initialize_parameters(layer_dims)\n",
    "\n",
    "# Create test data\n",
    "X_test = np.array([[1.5, 2.0], [0.5, -1.0], [2.5, 1.5], [-0.5, 0.8]]).T  # (2, 4)\n",
    "Y_test = np.array([[1, 0, 1, 0]])  # (1, 4)\n",
    "\n",
    "print(\"Test Setup:\")\n",
    "print(f\"Network: {layer_dims}\")\n",
    "print(f\"Activations: {activation_functions}\")\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Labels shape: {Y_test.shape}\")\n",
    "print(f\"Test data:\\nX =\\n{X_test}\")\n",
    "print(f\"Y = {Y_test.flatten()}\")\n",
    "\n",
    "# Compute gradients\n",
    "cost, gradients, predictions = nn.compute_gradients(X_test, Y_test, parameters, activation_functions)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Cost: {cost:.6f}\")\n",
    "print(f\"Predictions: {predictions.flatten()}\")\n",
    "print(\"\\nGradient shapes:\")\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"{key}: {grad.shape}\")\n",
    "\n",
    "print(\"\\nGradient values:\")\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Gradient Checking (Numerical Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, Y, parameters, activation_functions, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Gradient checking using numerical differentiation\n",
    "    \n",
    "    Parameters:\n",
    "    epsilon: small value for numerical differentiation\n",
    "    \n",
    "    Returns:\n",
    "    difference: relative difference between analytical and numerical gradients\n",
    "    \"\"\"\n",
    "    # Compute analytical gradients\n",
    "    cost, gradients, _ = nn.compute_gradients(X, Y, parameters, activation_functions)\n",
    "    \n",
    "    # Convert gradients to vector\n",
    "    grad_vector = []\n",
    "    param_names = []\n",
    "    \n",
    "    for key in sorted(gradients.keys()):\n",
    "        grad_vector.extend(gradients[key].flatten())\n",
    "        param_names.extend([key] * gradients[key].size)\n",
    "    \n",
    "    grad_vector = np.array(grad_vector)\n",
    "    \n",
    "    # Compute numerical gradients\n",
    "    num_grad_vector = np.zeros_like(grad_vector)\n",
    "    \n",
    "    # Convert parameters to vector\n",
    "    param_vector = []\n",
    "    for l in range(1, len(parameters)//2 + 1):\n",
    "        param_vector.extend(parameters[f'W{l}'].flatten())\n",
    "        param_vector.extend(parameters[f'b{l}'].flatten())\n",
    "    \n",
    "    param_vector = np.array(param_vector)\n",
    "    \n",
    "    # Check only a subset of parameters for efficiency\n",
    "    check_indices = np.random.choice(len(param_vector), min(20, len(param_vector)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(check_indices):\n",
    "        # Reconstruct parameters with theta + epsilon\n",
    "        param_plus = param_vector.copy()\n",
    "        param_plus[idx] += epsilon\n",
    "        params_plus = vector_to_parameters(param_plus, parameters)\n",
    "        \n",
    "        # Reconstruct parameters with theta - epsilon\n",
    "        param_minus = param_vector.copy()\n",
    "        param_minus[idx] -= epsilon\n",
    "        params_minus = vector_to_parameters(param_minus, parameters)\n",
    "        \n",
    "        # Compute costs\n",
    "        AL_plus, _ = nn.forward_propagation(X, params_plus, activation_functions)\n",
    "        cost_plus = nn.backward_prop.compute_cost(AL_plus, Y)\n",
    "        \n",
    "        AL_minus, _ = nn.forward_propagation(X, params_minus, activation_functions)\n",
    "        cost_minus = nn.backward_prop.compute_cost(AL_minus, Y)\n",
    "        \n",
    "        # Numerical gradient\n",
    "        num_grad_vector[idx] = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Check only the indices we computed\n",
    "    analytical_subset = grad_vector[check_indices]\n",
    "    numerical_subset = num_grad_vector[check_indices]\n",
    "    \n",
    "    # Compute relative difference\n",
    "    numerator = np.linalg.norm(analytical_subset - numerical_subset)\n",
    "    denominator = np.linalg.norm(analytical_subset) + np.linalg.norm(numerical_subset)\n",
    "    difference = numerator / denominator if denominator != 0 else 0\n",
    "    \n",
    "    return difference, analytical_subset, numerical_subset\n",
    "\n",
    "def vector_to_parameters(param_vector, template_params):\n",
    "    \"\"\"Convert parameter vector back to parameter dictionary\"\"\"\n",
    "    parameters = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for l in range(1, len(template_params)//2 + 1):\n",
    "        W_shape = template_params[f'W{l}'].shape\n",
    "        b_shape = template_params[f'b{l}'].shape\n",
    "        \n",
    "        W_size = np.prod(W_shape)\n",
    "        b_size = np.prod(b_shape)\n",
    "        \n",
    "        parameters[f'W{l}'] = param_vector[idx:idx+W_size].reshape(W_shape)\n",
    "        idx += W_size\n",
    "        \n",
    "        parameters[f'b{l}'] = param_vector[idx:idx+b_size].reshape(b_shape)\n",
    "        idx += b_size\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Perform gradient check on our test case\n",
    "print(\"Performing Gradient Check...\")\n",
    "difference, analytical, numerical = gradient_check(nn, X_test, Y_test, parameters, activation_functions)\n",
    "\n",
    "print(f\"\\nGradient Check Results:\")\n",
    "print(f\"Relative difference: {difference:.2e}\")\n",
    "print(f\"Sample analytical gradients: {analytical[:5]}\")\n",
    "print(f\"Sample numerical gradients: {numerical[:5]}\")\n",
    "\n",
    "if difference < 1e-5:\n",
    "    print(\"✅ Gradient check PASSED! Backpropagation is correct.\")\n",
    "elif difference < 1e-3:\n",
    "    print(\"⚠️ Gradient check WARNING: Small discrepancy detected.\")\n",
    "else:\n",
    "    print(\"❌ Gradient check FAILED: Large discrepancy detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Theory Understanding**: Reviewed backward propagation mathematics\n",
    "- [ ] **Activation Derivatives**: Implemented activation function derivatives\n",
    "- [ ] **Cost Functions**: Implemented cost functions and their derivatives\n",
    "- [ ] **Linear Backward**: Implemented linear portion of backward propagation\n",
    "- [ ] **Activation Backward**: Implemented activation portion of backward propagation\n",
    "- [ ] **Full Backward**: Implemented complete backward propagation\n",
    "- [ ] **Integration**: Combined forward and backward propagation\n",
    "- [ ] **Simple Test**: Tested on simple binary classification\n",
    "- [ ] **Gradient Check**: Verified gradients using numerical methods\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Chain Rule Application**: How calculus enables gradient computation\n",
    "2. **Gradient Flow**: How gradients propagate backward through layers\n",
    "3. **Vectorized Implementation**: Efficient gradient computation for multiple examples\n",
    "4. **Numerical Verification**: Using gradient checking to verify implementations\n",
    "5. **Cost Function Integration**: Connecting loss to gradient computation\n",
    "\n",
    "### Mathematical Foundation:\n",
    "- **Weight Gradients**: dW^[l] = (1/m) * dZ^[l] * A^[l-1].T\n",
    "- **Bias Gradients**: db^[l] = (1/m) * sum(dZ^[l])\n",
    "- **Activation Gradients**: dA^[l-1] = W^[l].T * dZ^[l]\n",
    "- **Chain Rule**: dZ^[l] = dA^[l] * g'^[l](Z^[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Gradient Shapes\n",
    "def test_gradient_shapes():\n",
    "    \"\"\"Test if gradients have correct shapes\"\"\"\n",
    "    try:\n",
    "        test_nn = NeuralNetwork()\n",
    "        test_dims = [3, 5, 2, 1]\n",
    "        test_activations = ['relu', 'tanh', 'sigmoid']\n",
    "        test_params = test_nn.initialize_parameters(test_dims)\n",
    "        \n",
    "        X_test = np.random.randn(3, 10)\n",
    "        Y_test = np.random.randint(0, 2, (1, 10))\n",
    "        \n",
    "        _, gradients, _ = test_nn.compute_gradients(X_test, Y_test, test_params, test_activations)\n",
    "        \n",
    "        # Check gradient shapes match parameter shapes\n",
    "        for l in range(1, len(test_dims)):\n",
    "            W_shape = test_params[f'W{l}'].shape\n",
    "            b_shape = test_params[f'b{l}'].shape\n",
    "            \n",
    "            assert gradients[f'dW{l}'].shape == W_shape, f\"dW{l} shape mismatch\"\n",
    "            assert gradients[f'db{l}'].shape == b_shape, f\"db{l} shape mismatch\"\n",
    "        \n",
    "        print(\"✅ Gradient shapes test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Gradient shapes test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_gradient_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Cost Decreases\n",
    "def test_cost_behavior():\n",
    "    \"\"\"Test if cost behaves correctly\"\"\"\n",
    "    try:\n",
    "        test_nn = NeuralNetwork()\n",
    "        \n",
    "        # Perfect predictions should have low cost\n",
    "        Y_perfect = np.array([[1, 0, 1, 0]])\n",
    "        AL_perfect = np.array([[0.99, 0.01, 0.98, 0.02]])\n",
    "        cost_perfect = test_nn.backward_prop.compute_cost(AL_perfect, Y_perfect)\n",
    "        \n",
    "        # Random predictions should have higher cost\n",
    "        AL_random = np.array([[0.5, 0.5, 0.5, 0.5]])\n",
    "        cost_random = test_nn.backward_prop.compute_cost(AL_random, Y_perfect)\n",
    "        \n",
    "        # Wrong predictions should have highest cost\n",
    "        AL_wrong = np.array([[0.01, 0.99, 0.02, 0.98]])\n",
    "        cost_wrong = test_nn.backward_prop.compute_cost(AL_wrong, Y_perfect)\n",
    "        \n",
    "        assert cost_perfect < cost_random < cost_wrong, \"Cost ordering incorrect\"\n",
    "        \n",
    "        print(f\"Perfect predictions cost: {cost_perfect:.4f}\")\n",
    "        print(f\"Random predictions cost: {cost_random:.4f}\")\n",
    "        print(f\"Wrong predictions cost: {cost_wrong:.4f}\")\n",
    "        print(\"✅ Cost behavior test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cost behavior test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_cost_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Gradient explosion/vanishing**\n",
    "- **Cause**: Poor weight initialization or activation functions\n",
    "- **Solution**: Use Xavier initialization, avoid sigmoid in deep networks\n",
    "\n",
    "**Issue 2: Gradient check fails**\n",
    "- **Cause**: Implementation errors in backward propagation\n",
    "- **Solution**: Check activation derivatives, matrix dimensions, chain rule application\n",
    "\n",
    "**Issue 3: Shape mismatch errors**\n",
    "- **Cause**: Incorrect matrix operations\n",
    "- **Solution**: Verify cache structure, gradient shapes match parameter shapes\n",
    "\n",
    "**Issue 4: NaN or infinite values**\n",
    "- **Cause**: Numerical instability in activation/cost functions\n",
    "- **Solution**: Use clipping in sigmoid/log functions, check for divide by zero\n",
    "\n",
    "**Issue 5: Slow convergence**\n",
    "- **Cause**: Poor gradient computation or learning rate\n",
    "- **Solution**: Verify backward propagation implementation, tune hyperparameters\n",
    "\n",
    "### Debugging Tips:\n",
    "- Use gradient checking on small networks first\n",
    "- Print intermediate gradient values\n",
    "- Check gradient norms (should not be too large or small)\n",
    "- Verify cost decreases over iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell → All Output → Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"🎉 Lab 3.3: Backward Propagation Implementation Completed!\")\n",
    "print(\"\\n📋 What you accomplished:\")\n",
    "print(\"✅ Implemented complete backward propagation algorithm\")\n",
    "print(\"✅ Created cost functions and their derivatives\")\n",
    "print(\"✅ Integrated forward and backward propagation\")\n",
    "print(\"✅ Verified implementation with gradient checking\")\n",
    "print(\"✅ Tested on real neural network architectures\")\n",
    "print(\"\\n🎯 Next: Lab 3.4 - Multi-class Classification Setup\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\n🧹 Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
