{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4: Multi-class Classification Setup\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement multi-class classification using softmax activation\n",
    "- Understand categorical cross-entropy loss function\n",
    "- Handle multi-class datasets and one-hot encoding\n",
    "- Build and train multi-class neural networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 3.1-3.3\n",
    "- Understanding of forward and backward propagation\n",
    "- Knowledge of softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, load_iris, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Multi-class Classification Theory (8 minutes)\n",
    "\n",
    "### Key Differences from Binary Classification:\n",
    "1. **Output Layer**: Multiple neurons (one per class)\n",
    "2. **Activation**: Softmax instead of sigmoid\n",
    "3. **Loss Function**: Categorical cross-entropy\n",
    "4. **Labels**: One-hot encoded vectors\n",
    "\n",
    "### Softmax Function:\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "### Categorical Cross-Entropy:\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassActivations:\n",
    "    \"\"\"Activation functions for multi-class classification\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Stable softmax implementation\n",
    "        \n",
    "        Parameters:\n",
    "        z: input logits (n_classes, m)\n",
    "        \n",
    "        Returns:\n",
    "        softmax probabilities (n_classes, m)\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        z_stable = z - np.max(z, axis=0, keepdims=True)\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(z):\n",
    "        \"\"\"\n",
    "        Note: For categorical cross-entropy + softmax,\n",
    "        the derivative simplifies to: dZ = A - Y\n",
    "        This is handled in the loss function derivative.\n",
    "        \"\"\"\n",
    "        s = MultiClassActivations.softmax(z)\n",
    "        return s * (1 - s)  # Simplified version\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "# Test softmax function\n",
    "test_logits = np.array([[2.0, 1.0, 0.1], \n",
    "                       [1.0, 3.0, 0.2], \n",
    "                       [0.2, 0.2, 2.9]])\n",
    "\n",
    "print(\"Testing Softmax Function:\")\n",
    "print(f\"Input logits:\\n{test_logits}\")\n",
    "\n",
    "softmax_result = MultiClassActivations.softmax(test_logits)\n",
    "print(f\"\\nSoftmax output:\\n{softmax_result}\")\n",
    "print(f\"\\nColumn sums (should be 1.0): {np.sum(softmax_result, axis=0)}\")\n",
    "print(f\"All probabilities positive: {np.all(softmax_result >= 0)}\")\n",
    "print(\"✅ Softmax function working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preparation for Multi-class Problems (10 minutes)\n",
    "\n",
    "### Utility Functions for Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassDataHandler:\n",
    "    \"\"\"Handle multi-class data preprocessing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_one_hot(labels, n_classes=None):\n",
    "        \"\"\"\n",
    "        Convert integer labels to one-hot encoded format\n",
    "        \n",
    "        Parameters:\n",
    "        labels: array of integer labels (m,)\n",
    "        n_classes: number of classes (auto-detected if None)\n",
    "        \n",
    "        Returns:\n",
    "        one_hot: one-hot encoded labels (n_classes, m)\n",
    "        \"\"\"\n",
    "        if n_classes is None:\n",
    "            n_classes = len(np.unique(labels))\n",
    "        \n",
    "        m = len(labels)\n",
    "        one_hot = np.zeros((n_classes, m))\n",
    "        one_hot[labels.astype(int), np.arange(m)] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_one_hot(one_hot):\n",
    "        \"\"\"\n",
    "        Convert one-hot encoded labels back to integer labels\n",
    "        \n",
    "        Parameters:\n",
    "        one_hot: one-hot encoded labels (n_classes, m)\n",
    "        \n",
    "        Returns:\n",
    "        labels: integer labels (m,)\n",
    "        \"\"\"\n",
    "        return np.argmax(one_hot, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_multiclass_data(X, y, test_size=0.2, standardize=True):\n",
    "        \"\"\"\n",
    "        Prepare data for multi-class classification\n",
    "        \n",
    "        Returns:\n",
    "        X_train, X_test: feature matrices (n_features, m)\n",
    "        Y_train, Y_test: one-hot encoded labels (n_classes, m)\n",
    "        y_train, y_test: integer labels (m,)\n",
    "        scaler: fitted StandardScaler (if standardize=True)\n",
    "        n_classes: number of classes\n",
    "        \"\"\"\n",
    "        # Ensure labels start from 0\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        n_classes = len(le.classes_)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = None\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Transpose to (n_features, m) format\n",
    "        X_train = X_train.T\n",
    "        X_test = X_test.T\n",
    "        \n",
    "        # Convert to one-hot encoding\n",
    "        Y_train = MultiClassDataHandler.to_one_hot(y_train, n_classes)\n",
    "        Y_test = MultiClassDataHandler.to_one_hot(y_test, n_classes)\n",
    "        \n",
    "        return X_train, X_test, Y_train, Y_test, y_train, y_test, scaler, n_classes\n",
    "\n",
    "# Test with synthetic data\n",
    "print(\"Testing Data Preparation:\")\n",
    "\n",
    "# Create synthetic 3-class dataset\n",
    "X_synthetic, y_synthetic = make_classification(\n",
    "    n_samples=300, n_features=4, n_classes=3, n_clusters_per_class=1,\n",
    "    n_informative=3, n_redundant=1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Original data shape: X={X_synthetic.shape}, y={y_synthetic.shape}\")\n",
    "print(f\"Classes in dataset: {np.unique(y_synthetic)}\")\n",
    "\n",
    "# Prepare data\n",
    "data_handler = MultiClassDataHandler()\n",
    "X_train, X_test, Y_train, Y_test, y_train, y_test, scaler, n_classes = data_handler.prepare_multiclass_data(\n",
    "    X_synthetic, y_synthetic\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter preparation:\")\n",
    "print(f\"X_train shape: {X_train.shape} (features, examples)\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape} (classes, examples)\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Test one-hot encoding\n",
    "test_labels = np.array([0, 1, 2, 0, 1])\n",
    "test_one_hot = data_handler.to_one_hot(test_labels)\n",
    "test_back = data_handler.from_one_hot(test_one_hot)\n",
    "\n",
    "print(f\"\\nOne-hot encoding test:\")\n",
    "print(f\"Original labels: {test_labels}\")\n",
    "print(f\"One-hot encoded:\\n{test_one_hot}\")\n",
    "print(f\"Converted back: {test_back}\")\n",
    "print(f\"Conversion successful: {np.array_equal(test_labels, test_back)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-class Loss Functions (7 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLoss:\n",
    "    \"\"\"Loss functions for multi-class classification\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Categorical cross-entropy loss\n",
    "        \n",
    "        Parameters:\n",
    "        Y_pred: predicted probabilities (n_classes, m)\n",
    "        Y_true: true one-hot labels (n_classes, m)\n",
    "        \n",
    "        Returns:\n",
    "        loss: scalar loss value\n",
    "        \"\"\"\n",
    "        m = Y_true.shape[1]\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        Y_pred_clipped = np.clip(Y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -1/m * np.sum(Y_true * np.log(Y_pred_clipped))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy_derivative(Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Derivative of categorical cross-entropy\n",
    "        \n",
    "        For softmax + categorical cross-entropy, the derivative simplifies to:\n",
    "        dZ = Y_pred - Y_true\n",
    "        \n",
    "        Returns:\n",
    "        dZ: gradient with respect to logits (before softmax)\n",
    "        \"\"\"\n",
    "        return Y_pred - Y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Compute accuracy for multi-class classification\n",
    "        \n",
    "        Parameters:\n",
    "        Y_pred: predicted probabilities (n_classes, m)\n",
    "        Y_true: true one-hot labels (n_classes, m)\n",
    "        \n",
    "        Returns:\n",
    "        accuracy: percentage of correct predictions\n",
    "        \"\"\"\n",
    "        pred_labels = np.argmax(Y_pred, axis=0)\n",
    "        true_labels = np.argmax(Y_true, axis=0)\n",
    "        \n",
    "        return np.mean(pred_labels == true_labels) * 100\n",
    "\n",
    "# Test loss functions\n",
    "print(\"Testing Multi-class Loss Functions:\")\n",
    "\n",
    "# Create test data\n",
    "n_classes, m = 3, 5\n",
    "Y_true = np.array([[1, 0, 0, 1, 0],\n",
    "                   [0, 1, 0, 0, 1],\n",
    "                   [0, 0, 1, 0, 0]])  # One-hot encoded\n",
    "\n",
    "# Perfect predictions\n",
    "Y_pred_perfect = Y_true.copy()\n",
    "\n",
    "# Random predictions\n",
    "Y_pred_random = np.array([[0.33, 0.4, 0.2, 0.5, 0.3],\n",
    "                         [0.33, 0.3, 0.3, 0.2, 0.4],\n",
    "                         [0.34, 0.3, 0.5, 0.3, 0.3]])\n",
    "\n",
    "# Ensure probabilities sum to 1\n",
    "Y_pred_random = Y_pred_random / np.sum(Y_pred_random, axis=0, keepdims=True)\n",
    "\n",
    "# Compute losses\n",
    "loss_perfect = MultiClassLoss.categorical_cross_entropy(Y_pred_perfect, Y_true)\n",
    "loss_random = MultiClassLoss.categorical_cross_entropy(Y_pred_random, Y_true)\n",
    "\n",
    "# Compute accuracies\n",
    "acc_perfect = MultiClassLoss.accuracy(Y_pred_perfect, Y_true)\n",
    "acc_random = MultiClassLoss.accuracy(Y_pred_random, Y_true)\n",
    "\n",
    "print(f\"True labels (one-hot):\\n{Y_true}\")\n",
    "print(f\"Perfect predictions loss: {loss_perfect:.6f}\")\n",
    "print(f\"Random predictions loss: {loss_random:.6f}\")\n",
    "print(f\"Perfect predictions accuracy: {acc_perfect:.1f}%\")\n",
    "print(f\"Random predictions accuracy: {acc_random:.1f}%\")\n",
    "print(f\"Loss ordering correct: {loss_perfect < loss_random}\")\n",
    "print(\"✅ Loss functions working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Complete Multi-class Neural Network (15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNeuralNetwork:\n",
    "    \"\"\"Complete neural network for multi-class classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = MultiClassActivations()\n",
    "        self.loss = MultiClassLoss()\n",
    "        self.parameters = {}\n",
    "        self.costs = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims):\n",
    "        \"\"\"\n",
    "        Initialize network parameters using Xavier initialization\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, activation_functions):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        # Forward through hidden layers\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            \n",
    "            # Linear transformation\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            \n",
    "            # Activation (ReLU for hidden layers)\n",
    "            A = self.activations.relu(Z)\n",
    "            \n",
    "            cache = ((A_prev, W, b), Z)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        # Output layer with softmax\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{L}']\n",
    "        b = parameters[f'b{L}']\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = self.activations.softmax(Z)  # Softmax for output layer\n",
    "        \n",
    "        cache = ((A_prev, W, b), Z)\n",
    "        caches.append(cache)\n",
    "        \n",
    "        return A, caches\n",
    "    \n",
    "    def backward_propagation(self, Y_pred, Y_true, caches, activation_functions):\n",
    "        \"\"\"\n",
    "        Backward propagation through the network\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        L = len(caches)\n",
    "        m = Y_true.shape[1]\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        # For softmax + categorical cross-entropy, dZ = Y_pred - Y_true\n",
    "        dZ = Y_pred - Y_true\n",
    "        \n",
    "        # Output layer gradients\n",
    "        (A_prev, W, b), _ = caches[L-1]\n",
    "        gradients[f'dW{L}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "        gradients[f'db{L}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        # Hidden layers gradients\n",
    "        for l in reversed(range(L-1)):\n",
    "            (A_prev, W, b), Z = caches[l]\n",
    "            \n",
    "            # dZ for ReLU activation\n",
    "            dZ = dA_prev * self.activations.relu_derivative(Z)\n",
    "            \n",
    "            # Gradients\n",
    "            gradients[f'dW{l+1}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "            gradients[f'db{l+1}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 0:  # Not the first layer\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, parameters, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "        \"\"\"\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "            parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def train(self, X, Y, layer_dims, learning_rate=0.01, num_iterations=1000, print_cost=True):\n",
    "        \"\"\"\n",
    "        Train the multi-class neural network\n",
    "        \n",
    "        Parameters:\n",
    "        X: training features (n_features, m)\n",
    "        Y: training labels one-hot encoded (n_classes, m)\n",
    "        layer_dims: list of layer dimensions\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        num_iterations: number of training iterations\n",
    "        \n",
    "        Returns:\n",
    "        parameters: trained parameters\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters(layer_dims)\n",
    "        activation_functions = ['relu'] * (len(layer_dims) - 2) + ['softmax']\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(num_iterations):\n",
    "            # Forward propagation\n",
    "            Y_pred, caches = self.forward_propagation(X, parameters, activation_functions)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.loss.categorical_cross_entropy(Y_pred, Y)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_propagation(Y_pred, Y, caches, activation_functions)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = self.update_parameters(parameters, gradients, learning_rate)\n",
    "            \n",
    "            # Record cost and accuracy\n",
    "            if i % 100 == 0:\n",
    "                accuracy = self.loss.accuracy(Y_pred, Y)\n",
    "                self.costs.append(cost)\n",
    "                self.accuracies.append(accuracy)\n",
    "                \n",
    "                if print_cost:\n",
    "                    print(f\"Iteration {i}: Cost = {cost:.6f}, Accuracy = {accuracy:.2f}%\")\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        return parameters\n",
    "    \n",
    "    def predict(self, X, parameters=None):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters\n",
    "        \n",
    "        activation_functions = ['relu'] * (len(parameters)//2 - 1) + ['softmax']\n",
    "        Y_pred, _ = self.forward_propagation(X, parameters, activation_functions)\n",
    "        \n",
    "        return Y_pred, np.argmax(Y_pred, axis=0)\n",
    "\n",
    "print(\"MultiClassNeuralNetwork class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real Dataset Application (10 minutes)\n",
    "\n",
    "### Test on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Iris dataset\n",
    "print(\"Loading Iris Dataset...\")\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(f\"Iris dataset: {X_iris.shape[0]} samples, {X_iris.shape[1]} features, {len(np.unique(y_iris))} classes\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "\n",
    "# Prepare data\n",
    "data_handler = MultiClassDataHandler()\n",
    "X_train, X_test, Y_train, Y_test, y_train, y_test, scaler, n_classes = data_handler.prepare_multiclass_data(\n",
    "    X_iris, y_iris, test_size=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[1]} samples\")\n",
    "print(f\"Test set: {X_test.shape[1]} samples\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(unique, counts, alpha=0.7, color=['red', 'green', 'blue'])\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(unique, [iris.target_names[i] for i in unique], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "plt.bar(unique, counts, alpha=0.7, color=['red', 'green', 'blue'])\n",
    "plt.title('Test Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(unique, [iris.target_names[i] for i in unique], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Feature correlation heatmap\n",
    "correlation_matrix = np.corrcoef(X_train)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "           xticklabels=iris.feature_names, yticklabels=iris.feature_names)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network on Iris dataset\n",
    "print(\"Training Multi-class Neural Network on Iris Dataset...\")\n",
    "\n",
    "# Create and train network\n",
    "nn = MultiClassNeuralNetwork()\n",
    "\n",
    "# Network architecture: 4 features -> 8 hidden -> 3 classes\n",
    "layer_dims = [X_train.shape[0], 8, n_classes]  # [4, 8, 3]\n",
    "\n",
    "print(f\"Network architecture: {layer_dims}\")\n",
    "\n",
    "# Train the network\n",
    "parameters = nn.train(\n",
    "    X_train, Y_train, \n",
    "    layer_dims=layer_dims,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=2000,\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"Evaluating Model Performance...\")\n",
    "\n",
    "# Training set predictions\n",
    "Y_pred_train, y_pred_train = nn.predict(X_train)\n",
    "train_accuracy = nn.loss.accuracy(Y_pred_train, Y_train)\n",
    "\n",
    "# Test set predictions\n",
    "Y_pred_test, y_pred_test = nn.predict(X_test)\n",
    "test_accuracy = nn.loss.accuracy(Y_pred_test, Y_test)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training progress\n",
    "plt.subplot(1, 3, 1)\n",
    "iterations = np.arange(0, len(nn.costs)) * 100\n",
    "plt.plot(iterations, nn.costs, 'b-', linewidth=2)\n",
    "plt.title('Training Cost Over Time')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(iterations, nn.accuracies, 'g-', linewidth=2)\n",
    "plt.title('Training Accuracy Over Time')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Custom Multi-class Problem (5 minutes)\n",
    "\n",
    "### Create and solve a custom 4-class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate custom 4-class dataset\n",
    "print(\"Creating Custom 4-Class Problem...\")\n",
    "\n",
    "X_custom, y_custom = make_blobs(\n",
    "    n_samples=400, centers=4, cluster_std=1.5,\n",
    "    n_features=2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Custom dataset: {X_custom.shape[0]} samples, {X_custom.shape[1]} features, {len(np.unique(y_custom))} classes\")\n",
    "\n",
    "# Visualize the custom dataset\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(4):\n",
    "    mask = y_custom == i\n",
    "    plt.scatter(X_custom[mask, 0], X_custom[mask, 1], \n",
    "               c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n",
    "\n",
    "plt.title('Custom 4-Class Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Prepare custom data\n",
    "X_train_c, X_test_c, Y_train_c, Y_test_c, y_train_c, y_test_c, scaler_c, n_classes_c = data_handler.prepare_multiclass_data(\n",
    "    X_custom, y_custom, test_size=0.25\n",
    ")\n",
    "\n",
    "# Quick training on custom dataset\n",
    "print(\"\\nTraining on Custom 4-Class Problem...\")\n",
    "nn_custom = MultiClassNeuralNetwork()\n",
    "\n",
    "# Larger network for 4 classes: 2 features -> 12 hidden -> 8 hidden -> 4 classes\n",
    "layer_dims_custom = [2, 12, 8, 4]\n",
    "\n",
    "parameters_custom = nn_custom.train(\n",
    "    X_train_c, Y_train_c,\n",
    "    layer_dims=layer_dims_custom,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1500,\n",
    "    print_cost=False  # Silent training\n",
    ")\n",
    "\n",
    "# Test custom model\n",
    "Y_pred_custom, y_pred_custom = nn_custom.predict(X_test_c)\n",
    "custom_accuracy = nn_custom.loss.accuracy(Y_pred_custom, Y_test_c)\n",
    "\n",
    "print(f\"Custom 4-Class Problem Results:\")\n",
    "print(f\"Test Accuracy: {custom_accuracy:.2f}%\")\n",
    "\n",
    "# Visualize decision boundary (simplified)\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Plot test data with predictions\n",
    "for i in range(4):\n",
    "    mask = y_pred_custom == i\n",
    "    plt.scatter(X_test_c[0, mask], X_test_c[1, mask], \n",
    "               c=colors[i], label=f'Predicted Class {i}', alpha=0.7, s=50, marker='x')\n",
    "\n",
    "plt.title(f'Custom Dataset Predictions\\n(Accuracy: {custom_accuracy:.1f}%)')\n",
    "plt.xlabel('Feature 1 (Standardized)')\n",
    "plt.ylabel('Feature 2 (Standardized)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCustom dataset training completed with {custom_accuracy:.1f}% test accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Theory Understanding**: Reviewed multi-class classification concepts\n",
    "- [ ] **Softmax Implementation**: Implemented stable softmax activation\n",
    "- [ ] **Data Preparation**: Created utilities for one-hot encoding and data prep\n",
    "- [ ] **Loss Functions**: Implemented categorical cross-entropy and accuracy\n",
    "- [ ] **Neural Network**: Built complete multi-class neural network\n",
    "- [ ] **Iris Dataset**: Successfully trained and tested on Iris dataset\n",
    "- [ ] **Custom Problem**: Created and solved 4-class classification problem\n",
    "- [ ] **Evaluation**: Analyzed model performance with confusion matrix\n",
    "- [ ] **Visualization**: Created plots showing training progress and results\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Multi-class Architecture**: Output layer with multiple neurons and softmax\n",
    "2. **One-hot Encoding**: Converting integer labels to vector format\n",
    "3. **Categorical Cross-entropy**: Appropriate loss function for multi-class problems\n",
    "4. **Softmax Properties**: Outputs sum to 1, suitable for probability distributions\n",
    "5. **Evaluation Metrics**: Accuracy, confusion matrix, classification reports\n",
    "\n",
    "### Technical Insights:\n",
    "- **Softmax Stability**: Subtract max before exponential for numerical stability\n",
    "- **Gradient Simplification**: For softmax + cross-entropy, dZ = Y_pred - Y_true\n",
    "- **Data Preprocessing**: Standardization crucial for multi-class problems\n",
    "- **Architecture Design**: More classes may need larger hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Softmax Properties\n",
    "def test_softmax_properties():\n",
    "    \"\"\"Test softmax function properties\"\"\"\n",
    "    try:\n",
    "        # Test data\n",
    "        test_logits = np.random.randn(5, 10)  # 5 classes, 10 examples\n",
    "        \n",
    "        # Apply softmax\n",
    "        probs = MultiClassActivations.softmax(test_logits)\n",
    "        \n",
    "        # Check properties\n",
    "        # 1. All probabilities are non-negative\n",
    "        assert np.all(probs >= 0), \"Negative probabilities found\"\n",
    "        \n",
    "        # 2. Each column sums to 1\n",
    "        column_sums = np.sum(probs, axis=0)\n",
    "        assert np.allclose(column_sums, 1.0, atol=1e-10), \"Probabilities don't sum to 1\"\n",
    "        \n",
    "        # 3. Shape preservation\n",
    "        assert probs.shape == test_logits.shape, \"Shape not preserved\"\n",
    "        \n",
    "        print(\"✅ Softmax properties test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Softmax properties test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_softmax_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: One-hot Encoding\n",
    "def test_one_hot_encoding():\n",
    "    \"\"\"Test one-hot encoding conversion\"\"\"\n",
    "    try:\n",
    "        handler = MultiClassDataHandler()\n",
    "        \n",
    "        # Test cases\n",
    "        test_labels = [np.array([0, 1, 2, 0, 1]), np.array([3, 1, 0, 2, 3, 1])]\n",
    "        \n",
    "        for labels in test_labels:\n",
    "            # Convert to one-hot\n",
    "            one_hot = handler.to_one_hot(labels)\n",
    "            \n",
    "            # Convert back\n",
    "            labels_back = handler.from_one_hot(one_hot)\n",
    "            \n",
    "            # Check consistency\n",
    "            assert np.array_equal(labels, labels_back), \"One-hot conversion inconsistent\"\n",
    "            \n",
    "            # Check one-hot properties\n",
    "            assert np.all(np.sum(one_hot, axis=0) == 1), \"One-hot encoding incorrect\"\n",
    "            assert one_hot.shape[0] == len(np.unique(labels)), \"Wrong number of classes\"\n",
    "        \n",
    "        print(\"✅ One-hot encoding test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ One-hot encoding test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_one_hot_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 3: Loss Function Behavior\n",
    "def test_loss_behavior():\n",
    "    \"\"\"Test loss function decreases with better predictions\"\"\"\n",
    "    try:\n",
    "        # Create test data\n",
    "        Y_true = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 1]])\n",
    "        \n",
    "        # Perfect predictions\n",
    "        Y_perfect = Y_true.copy()\n",
    "        \n",
    "        # Good predictions\n",
    "        Y_good = np.array([[0.8, 0.1, 0.1],\n",
    "                          [0.1, 0.8, 0.1],\n",
    "                          [0.1, 0.1, 0.8]])\n",
    "        \n",
    "        # Random predictions\n",
    "        Y_random = np.array([[0.33, 0.33, 0.33],\n",
    "                            [0.33, 0.33, 0.33],\n",
    "                            [0.34, 0.34, 0.34]])\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_perfect = MultiClassLoss.categorical_cross_entropy(Y_perfect, Y_true)\n",
    "        loss_good = MultiClassLoss.categorical_cross_entropy(Y_good, Y_true)\n",
    "        loss_random = MultiClassLoss.categorical_cross_entropy(Y_random, Y_true)\n",
    "        \n",
    "        # Check ordering\n",
    "        assert loss_perfect < loss_good < loss_random, \"Loss ordering incorrect\"\n",
    "        assert loss_perfect < 0.1, \"Perfect prediction loss too high\"\n",
    "        \n",
    "        print(f\"Perfect loss: {loss_perfect:.6f}\")\n",
    "        print(f\"Good loss: {loss_good:.6f}\")\n",
    "        print(f\"Random loss: {loss_random:.6f}\")\n",
    "        print(\"✅ Loss function behavior test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Loss function behavior test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_loss_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Softmax overflow/underflow**\n",
    "- **Cause**: Very large input values causing exp() overflow\n",
    "- **Solution**: Subtract max value before applying exponential (implemented in our softmax)\n",
    "\n",
    "**Issue 2: Loss becomes NaN**\n",
    "- **Cause**: Taking log(0) in categorical cross-entropy\n",
    "- **Solution**: Clip predictions to avoid exactly 0 or 1 values\n",
    "\n",
    "**Issue 3: Poor convergence**\n",
    "- **Cause**: Inappropriate learning rate or network architecture\n",
    "- **Solution**: Try different learning rates (0.01-0.1), add more hidden units\n",
    "\n",
    "**Issue 4: Imbalanced class predictions**\n",
    "- **Cause**: Dataset class imbalance or poor initialization\n",
    "- **Solution**: Use stratified sampling, adjust class weights, or use different initialization\n",
    "\n",
    "**Issue 5: One-hot encoding errors**\n",
    "- **Cause**: Labels not starting from 0 or gaps in label sequence\n",
    "- **Solution**: Use LabelEncoder to ensure labels are 0, 1, 2, ..., n-1\n",
    "\n",
    "### Performance Tips:\n",
    "- Start with smaller networks and increase complexity gradually\n",
    "- Monitor both training and validation accuracy\n",
    "- Use appropriate regularization for overfitting\n",
    "- Ensure balanced class representation in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell → All Output → Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"🎉 Lab 3.4: Multi-class Classification Setup Completed!\")\n",
    "print(\"\\n📋 What you accomplished:\")\n",
    "print(\"✅ Implemented softmax activation and categorical cross-entropy loss\")\n",
    "print(\"✅ Created data preprocessing utilities for multi-class problems\")\n",
    "print(\"✅ Built complete multi-class neural network from scratch\")\n",
    "print(\"✅ Successfully trained on Iris dataset with high accuracy\")\n",
    "print(\"✅ Solved custom 4-class classification problem\")\n",
    "print(\"✅ Analyzed model performance with confusion matrices and reports\")\n",
    "print(\"\\n🎯 Next: Lab 3.5 - Performance Optimization Techniques\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\n🧹 Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}