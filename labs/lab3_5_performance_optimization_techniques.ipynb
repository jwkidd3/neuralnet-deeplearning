{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5: Performance Optimization Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement various optimization techniques for neural networks\n",
    "- Compare different optimization algorithms (SGD, Momentum, Adam)\n",
    "- Apply regularization techniques to prevent overfitting\n",
    "- Optimize training procedures and hyperparameters\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 3.1-3.4\n",
    "- Understanding of gradient descent and backpropagation\n",
    "- Knowledge of overfitting and underfitting concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Optimization Algorithms (15 minutes)\n",
    "\n",
    "### 1.1 Base Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptimizer:\n",
    "    \"\"\"Base class for optimization algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0  # Time step\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Update parameters using gradients\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement update method\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        self.t = 0\n",
    "\n",
    "class SGDOptimizer(BaseOptimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__(learning_rate)\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Standard SGD update\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                parameters[key] -= self.learning_rate * gradients[f'd{key}']\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "class MomentumOptimizer(BaseOptimizer):\n",
    "    \"\"\"SGD with Momentum optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta = beta\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Momentum update rule\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize velocity on first update\n",
    "        if not self.velocity:\n",
    "            for key in parameters:\n",
    "                if f'd{key}' in gradients:\n",
    "                    self.velocity[f'v_{key}'] = np.zeros_like(parameters[key])\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                # Update velocity\n",
    "                self.velocity[f'v_{key}'] = (self.beta * self.velocity[f'v_{key}'] + \n",
    "                                           (1 - self.beta) * gradients[f'd{key}'])\n",
    "                \n",
    "                # Update parameters\n",
    "                parameters[key] -= self.learning_rate * self.velocity[f'v_{key}']\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.velocity = {}\n",
    "\n",
    "class AdamOptimizer(BaseOptimizer):\n",
    "    \"\"\"Adam optimizer (Adaptive Moment Estimation)\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = {}  # First moment (mean)\n",
    "        self.velocity = {}  # Second moment (variance)\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Adam update rule\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize moments on first update\n",
    "        if not self.momentum:\n",
    "            for key in parameters:\n",
    "                if f'd{key}' in gradients:\n",
    "                    self.momentum[f'm_{key}'] = np.zeros_like(parameters[key])\n",
    "                    self.velocity[f'v_{key}'] = np.zeros_like(parameters[key])\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                grad = gradients[f'd{key}']\n",
    "                \n",
    "                # Update biased first moment estimate\n",
    "                self.momentum[f'm_{key}'] = (self.beta1 * self.momentum[f'm_{key}'] + \n",
    "                                           (1 - self.beta1) * grad)\n",
    "                \n",
    "                # Update biased second raw moment estimate\n",
    "                self.velocity[f'v_{key}'] = (self.beta2 * self.velocity[f'v_{key}'] + \n",
    "                                           (1 - self.beta2) * (grad ** 2))\n",
    "                \n",
    "                # Compute bias-corrected first moment estimate\n",
    "                m_corrected = self.momentum[f'm_{key}'] / (1 - self.beta1 ** self.t)\n",
    "                \n",
    "                # Compute bias-corrected second raw moment estimate\n",
    "                v_corrected = self.velocity[f'v_{key}'] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                parameters[key] -= (self.learning_rate * m_corrected / \n",
    "                                  (np.sqrt(v_corrected) + self.epsilon))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.momentum = {}\n",
    "        self.velocity = {}\n",
    "\n",
    "print(\"Optimization algorithms implemented!\")\n",
    "print(\"Available optimizers: SGD, Momentum, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTechniques:\n",
    "    \"\"\"Collection of regularization techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_regularization_cost(parameters, lambda_reg):\n",
    "        \"\"\"\n",
    "        Compute L2 regularization cost\n",
    "        \n",
    "        Parameters:\n",
    "        parameters: network parameters\n",
    "        lambda_reg: regularization strength\n",
    "        \n",
    "        Returns:\n",
    "        l2_cost: L2 regularization cost\n",
    "        \"\"\"\n",
    "        l2_cost = 0\n",
    "        L = len(parameters) // 2  # Number of layers\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            l2_cost += np.sum(np.square(parameters[f'W{l}']))\n",
    "        \n",
    "        return lambda_reg / 2 * l2_cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_regularization_gradients(parameters, gradients, lambda_reg):\n",
    "        \"\"\"\n",
    "        Add L2 regularization to gradients\n",
    "        \n",
    "        Parameters:\n",
    "        parameters: network parameters\n",
    "        gradients: computed gradients\n",
    "        lambda_reg: regularization strength\n",
    "        \n",
    "        Returns:\n",
    "        gradients: modified gradients with L2 regularization\n",
    "        \"\"\"\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            gradients[f'dW{l}'] += lambda_reg * parameters[f'W{l}']\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_forward(A, keep_prob, training=True):\n",
    "        \"\"\"\n",
    "        Apply dropout during forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        A: activations\n",
    "        keep_prob: probability of keeping each neuron\n",
    "        training: whether in training mode\n",
    "        \n",
    "        Returns:\n",
    "        A: activations after dropout\n",
    "        dropout_cache: cache for backward propagation\n",
    "        \"\"\"\n",
    "        if not training or keep_prob == 1.0:\n",
    "            return A, None\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        dropout_mask = np.random.rand(*A.shape) < keep_prob\n",
    "        \n",
    "        # Apply dropout\n",
    "        A = A * dropout_mask\n",
    "        \n",
    "        # Scale to maintain expected value\n",
    "        A = A / keep_prob\n",
    "        \n",
    "        dropout_cache = dropout_mask / keep_prob\n",
    "        \n",
    "        return A, dropout_cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_backward(dA, dropout_cache):\n",
    "        \"\"\"\n",
    "        Apply dropout during backward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        dA: upstream gradients\n",
    "        dropout_cache: cache from forward dropout\n",
    "        \n",
    "        Returns:\n",
    "        dA: gradients after dropout\n",
    "        \"\"\"\n",
    "        if dropout_cache is None:\n",
    "            return dA\n",
    "        \n",
    "        return dA * dropout_cache\n",
    "\n",
    "# Test regularization techniques\n",
    "print(\"Testing Regularization Techniques:\")\n",
    "\n",
    "# Test L2 regularization\n",
    "test_params = {\n",
    "    'W1': np.array([[0.1, 0.2], [0.3, 0.4]]),\n",
    "    'b1': np.array([[0.1], [0.2]]),\n",
    "    'W2': np.array([[0.5, 0.6]]),\n",
    "    'b2': np.array([[0.3]])\n",
    "}\n",
    "\n",
    "l2_cost = RegularizationTechniques.l2_regularization_cost(test_params, lambda_reg=0.1)\n",
    "print(f\"L2 regularization cost: {l2_cost:.6f}\")\n",
    "\n",
    "# Test dropout\n",
    "test_activations = np.random.randn(5, 10)\n",
    "dropped_A, dropout_cache = RegularizationTechniques.dropout_forward(\n",
    "    test_activations, keep_prob=0.8, training=True\n",
    ")\n",
    "\n",
    "print(f\"Original activations shape: {test_activations.shape}\")\n",
    "print(f\"After dropout shape: {dropped_A.shape}\")\n",
    "print(f\"Percentage of neurons kept: {np.mean(dropout_cache > 0) * 100:.1f}%\")\n",
    "print(\"✅ Regularization techniques implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Optimized Neural Network (15 minutes)\n",
    "\n",
    "### Enhanced Neural Network with Optimization Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedNeuralNetwork:\n",
    "    \"\"\"Neural network with optimization and regularization features\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer='adam', regularization='none', lambda_reg=0.01, \n",
    "                 keep_prob=1.0, **optimizer_kwargs):\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'sgd':\n",
    "            self.optimizer = SGDOptimizer(**optimizer_kwargs)\n",
    "        elif optimizer.lower() == 'momentum':\n",
    "            self.optimizer = MomentumOptimizer(**optimizer_kwargs)\n",
    "        elif optimizer.lower() == 'adam':\n",
    "            self.optimizer = AdamOptimizer(**optimizer_kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "        \n",
    "        self.regularization = regularization.lower()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.keep_prob = keep_prob\n",
    "        self.regularizer = RegularizationTechniques()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_cost': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_cost': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        self.parameters = {}\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims, initialization='xavier'):\n",
    "        \"\"\"Initialize network parameters with different initialization schemes\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            if initialization.lower() == 'xavier':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1.0 / layer_dims[l-1])\n",
    "            elif initialization.lower() == 'he':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            elif initialization.lower() == 'random':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initialization: {initialization}\")\n",
    "            \n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, training=True):\n",
    "        \"\"\"Forward propagation with dropout support\"\"\"\n",
    "        caches = []\n",
    "        dropout_caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            \n",
    "            # Linear transformation\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            \n",
    "            # ReLU activation\n",
    "            A = np.maximum(0, Z)\n",
    "            \n",
    "            # Apply dropout\n",
    "            A, dropout_cache = self.regularizer.dropout_forward(A, self.keep_prob, training)\n",
    "            \n",
    "            # Store caches\n",
    "            linear_cache = (A_prev, W, b)\n",
    "            caches.append((linear_cache, Z))\n",
    "            dropout_caches.append(dropout_cache)\n",
    "        \n",
    "        # Output layer (no dropout)\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{L}']\n",
    "        b = parameters[f'b{L}']\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = 1 / (1 + np.exp(-np.clip(Z, -500, 500)))  # Sigmoid\n",
    "        \n",
    "        linear_cache = (A_prev, W, b)\n",
    "        caches.append((linear_cache, Z))\n",
    "        dropout_caches.append(None)  # No dropout on output layer\n",
    "        \n",
    "        return A, caches, dropout_caches\n",
    "    \n",
    "    def compute_cost(self, AL, Y, parameters):\n",
    "        \"\"\"Compute cost with regularization\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Binary cross-entropy cost\n",
    "        AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        cross_entropy_cost = -1/m * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n",
    "        \n",
    "        # Add regularization\n",
    "        regularization_cost = 0\n",
    "        if self.regularization == 'l2':\n",
    "            regularization_cost = self.regularizer.l2_regularization_cost(parameters, self.lambda_reg)\n",
    "        \n",
    "        total_cost = cross_entropy_cost + regularization_cost\n",
    "        \n",
    "        return np.squeeze(total_cost)\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches, dropout_caches, parameters):\n",
    "        \"\"\"Backward propagation with dropout and regularization\"\"\"\n",
    "        gradients = {}\n",
    "        L = len(caches)\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        dAL = -(Y / AL) + (1 - Y) / (1 - AL)\n",
    "        \n",
    "        # Output layer\n",
    "        (A_prev, W, b), Z = caches[L-1]\n",
    "        dZ = dAL * AL * (1 - AL)  # Sigmoid derivative\n",
    "        \n",
    "        gradients[f'dW{L}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "        gradients[f'db{L}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in reversed(range(L-1)):\n",
    "            (A_prev, W, b), Z = caches[l]\n",
    "            \n",
    "            # Apply dropout to gradients\n",
    "            dA_prev = self.regularizer.dropout_backward(dA_prev, dropout_caches[l+1])\n",
    "            \n",
    "            # ReLU derivative\n",
    "            dZ = dA_prev * (Z > 0).astype(float)\n",
    "            \n",
    "            gradients[f'dW{l+1}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "            gradients[f'db{l+1}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 0:\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        # Add regularization to gradients\n",
    "        if self.regularization == 'l2':\n",
    "            gradients = self.regularizer.l2_regularization_gradients(\n",
    "                parameters, gradients, self.lambda_reg\n",
    "            )\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_val, Y_val, layer_dims, \n",
    "              num_iterations=1000, print_cost=True, print_every=100):\n",
    "        \"\"\"Train the optimized neural network\"\"\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters(layer_dims)\n",
    "        \n",
    "        # Reset optimizer\n",
    "        self.optimizer.reset()\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            # Forward propagation (training mode)\n",
    "            AL_train, caches, dropout_caches = self.forward_propagation(X_train, parameters, training=True)\n",
    "            \n",
    "            # Compute training cost\n",
    "            train_cost = self.compute_cost(AL_train, Y_train, parameters)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_propagation(AL_train, Y_train, caches, dropout_caches, parameters)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = self.optimizer.update(parameters, gradients)\n",
    "            \n",
    "            # Validation (every print_every iterations)\n",
    "            if i % print_every == 0:\n",
    "                # Training accuracy\n",
    "                train_predictions = (AL_train > 0.5).astype(int)\n",
    "                train_accuracy = np.mean(train_predictions == Y_train) * 100\n",
    "                \n",
    "                # Validation forward pass (no dropout)\n",
    "                AL_val, _, _ = self.forward_propagation(X_val, parameters, training=False)\n",
    "                val_cost = self.compute_cost(AL_val, Y_val, parameters)\n",
    "                \n",
    "                val_predictions = (AL_val > 0.5).astype(int)\n",
    "                val_accuracy = np.mean(val_predictions == Y_val) * 100\n",
    "                \n",
    "                # Store history\n",
    "                self.history['train_cost'].append(train_cost)\n",
    "                self.history['train_accuracy'].append(train_accuracy)\n",
    "                self.history['val_cost'].append(val_cost)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "                \n",
    "                if print_cost:\n",
    "                    print(f\"Iteration {i}: Train Cost = {train_cost:.6f}, Train Acc = {train_accuracy:.2f}%, \"\n",
    "                          f\"Val Cost = {val_cost:.6f}, Val Acc = {val_accuracy:.2f}%\")\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        return parameters\n",
    "    \n",
    "    def predict(self, X, parameters=None):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters\n",
    "        \n",
    "        AL, _, _ = self.forward_propagation(X, parameters, training=False)\n",
    "        predictions = (AL > 0.5).astype(int)\n",
    "        \n",
    "        return predictions, AL\n",
    "\n",
    "print(\"OptimizedNeuralNetwork class implemented successfully!\")\n",
    "print(\"Features: Multiple optimizers, L2 regularization, dropout, initialization schemes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimization Comparison Experiment (10 minutes)\n",
    "\n",
    "### Compare Different Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for optimization comparison\n",
    "print(\"Preparing Dataset for Optimization Comparison...\")\n",
    "\n",
    "# Load breast cancer dataset (binary classification)\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Classes: {np.unique(y)} (0: malignant, 1: benign)\")\n",
    "\n",
    "# Split and preprocess data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training set for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Transpose to (features, samples) format\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_val = y_val.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Network architecture\n",
    "layer_dims = [X_train.shape[0], 16, 8, 1]  # 30 features -> 16 -> 8 -> 1\n",
    "print(f\"Network architecture: {layer_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "print(\"Comparing Optimization Algorithms...\")\n",
    "\n",
    "optimizers_config = {\n",
    "    'SGD': {'optimizer': 'sgd', 'learning_rate': 0.1},\n",
    "    'Momentum': {'optimizer': 'momentum', 'learning_rate': 0.1, 'beta': 0.9},\n",
    "    'Adam': {'optimizer': 'adam', 'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.999}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "training_time = {}\n",
    "\n",
    "for optimizer_name, config in optimizers_config.items():\n",
    "    print(f\"\\nTraining with {optimizer_name} optimizer...\")\n",
    "    \n",
    "    # Create network\n",
    "    nn = OptimizedNeuralNetwork(\n",
    "        regularization='none',\n",
    "        keep_prob=1.0,  # No dropout for fair comparison\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    parameters = nn.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        layer_dims=layer_dims,\n",
    "        num_iterations=1000,\n",
    "        print_cost=False  # Silent training\n",
    "    )\n",
    "    training_time[optimizer_name] = time.time() - start_time\n",
    "    \n",
    "    # Test performance\n",
    "    test_predictions, test_probs = nn.predict(X_test)\n",
    "    test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "    \n",
    "    # Store results\n",
    "    results[optimizer_name] = {\n",
    "        'history': nn.history.copy(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'final_train_cost': nn.history['train_cost'][-1],\n",
    "        'final_val_cost': nn.history['val_cost'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"{optimizer_name} - Test Accuracy: {test_accuracy:.2f}%, Training Time: {training_time[optimizer_name]:.2f}s\")\n",
    "\n",
    "print(\"\\n=== OPTIMIZATION COMPARISON RESULTS ===\")\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    result = results[optimizer_name]\n",
    "    print(f\"{optimizer_name}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Final Training Cost: {result['final_train_cost']:.6f}\")\n",
    "    print(f\"  Final Validation Cost: {result['final_val_cost']:.6f}\")\n",
    "    print(f\"  Training Time: {training_time[optimizer_name]:.2f}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training cost comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "iterations = np.arange(0, 1000, 100)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['train_cost'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Training Cost Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation cost comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['val_cost'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Validation Cost Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['train_accuracy'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['val_accuracy'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final test accuracy bar chart\n",
    "plt.subplot(2, 3, 5)\n",
    "optimizer_names = list(optimizers_config.keys())\n",
    "test_accuracies = [results[name]['test_accuracy'] for name in optimizer_names]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = plt.bar(optimizer_names, test_accuracies, color=colors, alpha=0.7)\n",
    "plt.title('Final Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, test_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{accuracy:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "times = [training_time[name] for name in optimizer_names]\n",
    "bars = plt.bar(optimizer_names, times, color=colors, alpha=0.7)\n",
    "plt.title('Training Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Key Observations:\")\n",
    "best_accuracy = max(test_accuracies)\n",
    "best_optimizer = optimizer_names[test_accuracies.index(best_accuracy)]\n",
    "print(f\"• Best performing optimizer: {best_optimizer} ({best_accuracy:.2f}% test accuracy)\")\n",
    "\n",
    "fastest_time = min(times)\n",
    "fastest_optimizer = optimizer_names[times.index(fastest_time)]\n",
    "print(f\"• Fastest optimizer: {fastest_optimizer} ({fastest_time:.2f}s training time)\")\n",
    "print(f\"• Adam typically provides good balance of speed and performance\")\n",
    "print(f\"• Momentum helps accelerate SGD in relevant directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization Effects (5 minutes)\n",
    "\n",
    "### Compare Networks with and without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization effects\n",
    "print(\"Comparing Regularization Effects...\")\n",
    "\n",
    "regularization_configs = {\n",
    "    'No Regularization': {'regularization': 'none', 'keep_prob': 1.0},\n",
    "    'L2 Regularization': {'regularization': 'l2', 'lambda_reg': 0.01, 'keep_prob': 1.0},\n",
    "    'Dropout': {'regularization': 'none', 'keep_prob': 0.8},\n",
    "    'L2 + Dropout': {'regularization': 'l2', 'lambda_reg': 0.01, 'keep_prob': 0.8}\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for config_name, config in regularization_configs.items():\n",
    "    print(f\"\\nTraining with {config_name}...\")\n",
    "    \n",
    "    # Create network with Adam optimizer\n",
    "    nn = OptimizedNeuralNetwork(\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.01,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    parameters = nn.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        layer_dims=layer_dims,\n",
    "        num_iterations=800,\n",
    "        print_cost=False\n",
    "    )\n",
    "    \n",
    "    # Test performance\n",
    "    test_predictions, test_probs = nn.predict(X_test)\n",
    "    test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "    \n",
    "    # Calculate overfitting metric (train vs val accuracy difference)\n",
    "    final_train_acc = nn.history['train_accuracy'][-1]\n",
    "    final_val_acc = nn.history['val_accuracy'][-1]\n",
    "    overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    reg_results[config_name] = {\n",
    "        'history': nn.history.copy(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'final_train_acc': final_train_acc,\n",
    "        'final_val_acc': final_val_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"{config_name} - Test Accuracy: {test_accuracy:.2f}%, Overfitting Gap: {overfitting_gap:.2f}%\")\n",
    "\n",
    "# Visualize regularization effects\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Training vs Validation accuracy\n",
    "plt.subplot(2, 3, 1)\n",
    "iterations = np.arange(0, 800, 100)\n",
    "for config_name in regularization_configs.keys():\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['train_accuracy'], \n",
    "             label=f'{config_name} (Train)', linewidth=2, linestyle='-')\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['val_accuracy'], \n",
    "             label=f'{config_name} (Val)', linewidth=2, linestyle='--')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "config_names = list(regularization_configs.keys())\n",
    "overfitting_gaps = [reg_results[name]['overfitting_gap'] for name in config_names]\n",
    "colors = ['red', 'orange', 'lightblue', 'green']\n",
    "\n",
    "bars = plt.bar(config_names, overfitting_gaps, color=colors, alpha=0.7)\n",
    "plt.title('Overfitting Gap\\n(Train Acc - Val Acc)')\n",
    "plt.ylabel('Accuracy Gap (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, gap in zip(bars, overfitting_gaps):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{gap:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Test accuracy comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "test_accuracies_reg = [reg_results[name]['test_accuracy'] for name in config_names]\n",
    "bars = plt.bar(config_names, test_accuracies_reg, color=colors, alpha=0.7)\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(90, 100)\n",
    "\n",
    "# Add value labels\n",
    "for bar, accuracy in zip(bars, test_accuracies_reg):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             f'{accuracy:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Cost curves\n",
    "plt.subplot(2, 3, 4)\n",
    "for config_name in regularization_configs.keys():\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['train_cost'], \n",
    "             label=f'{config_name}', linewidth=2)\n",
    "plt.title('Training Cost Curves')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Regularization Analysis:\")\n",
    "best_generalization = min(overfitting_gaps)\n",
    "best_reg_method = config_names[overfitting_gaps.index(best_generalization)]\n",
    "print(f\"• Best generalization: {best_reg_method} (gap: {best_generalization:.2f}%)\")\n",
    "print(f\"• Regularization helps reduce overfitting\")\n",
    "print(f\"• L2 regularization adds penalty to large weights\")\n",
    "print(f\"• Dropout provides regularization through random neuron deactivation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Optimizer Implementation**: Created SGD, Momentum, and Adam optimizers\n",
    "- [ ] **Regularization Techniques**: Implemented L2 regularization and dropout\n",
    "- [ ] **Enhanced Network**: Built optimized neural network with all features\n",
    "- [ ] **Dataset Preparation**: Loaded and preprocessed breast cancer dataset\n",
    "- [ ] **Optimizer Comparison**: Compared SGD, Momentum, and Adam performance\n",
    "- [ ] **Regularization Analysis**: Tested different regularization approaches\n",
    "- [ ] **Visualization**: Created comprehensive performance comparison plots\n",
    "- [ ] **Performance Analysis**: Analyzed results and identified best practices\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Optimization Algorithms**: SGD, Momentum, Adam and their trade-offs\n",
    "2. **Regularization**: L2 penalty and dropout for overfitting prevention\n",
    "3. **Performance Metrics**: Training/validation curves, overfitting detection\n",
    "4. **Hyperparameter Effects**: Learning rate, regularization strength impact\n",
    "5. **Best Practices**: When to use different optimization techniques\n",
    "\n",
    "### Technical Insights:\n",
    "- **Adam Optimizer**: Often provides best balance of speed and performance\n",
    "- **Momentum**: Helps SGD overcome local minima and narrow valleys\n",
    "- **L2 Regularization**: Prevents weights from becoming too large\n",
    "- **Dropout**: Randomly deactivates neurons to prevent co-adaptation\n",
    "- **Bias Correction**: Adam uses bias correction for more stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Optimizer State Management\n",
    "def test_optimizer_state():\n",
    "    \"\"\"Test optimizer state management\"\"\"\n",
    "    try:\n",
    "        # Test Adam optimizer state\n",
    "        adam = AdamOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        # Dummy parameters and gradients\n",
    "        params = {'W1': np.random.randn(3, 2), 'b1': np.zeros((3, 1))}\n",
    "        grads = {'dW1': np.random.randn(3, 2), 'db1': np.random.randn(3, 1)}\n",
    "        \n",
    "        # First update\n",
    "        params_updated = adam.update(params, grads)\n",
    "        \n",
    "        # Check that momentum and velocity are initialized\n",
    "        assert len(adam.momentum) > 0, \"Momentum not initialized\"\n",
    "        assert len(adam.velocity) > 0, \"Velocity not initialized\"\n",
    "        assert adam.t == 1, \"Time step not updated\"\n",
    "        \n",
    "        # Reset and check state is cleared\n",
    "        adam.reset()\n",
    "        assert len(adam.momentum) == 0, \"Momentum not reset\"\n",
    "        assert len(adam.velocity) == 0, \"Velocity not reset\"\n",
    "        assert adam.t == 0, \"Time step not reset\"\n",
    "        \n",
    "        print(\"✅ Optimizer state management test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Optimizer state management test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_optimizer_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Regularization Effects\n",
    "def test_regularization_effects():\n",
    "    \"\"\"Test regularization implementation\"\"\"\n",
    "    try:\n",
    "        # Test L2 regularization cost\n",
    "        params = {\n",
    "            'W1': np.array([[1.0, 2.0], [3.0, 4.0]]),\n",
    "            'b1': np.array([[0.1], [0.2]]),\n",
    "            'W2': np.array([[0.5, 0.6]]),\n",
    "            'b2': np.array([[0.1]])\n",
    "        }\n",
    "        \n",
    "        # Calculate expected L2 cost: 0.5 * lambda * (1² + 2² + 3² + 4² + 0.5² + 0.6²)\n",
    "        expected_l2_cost = 0.5 * 0.1 * (1 + 4 + 9 + 16 + 0.25 + 0.36)\n",
    "        actual_l2_cost = RegularizationTechniques.l2_regularization_cost(params, 0.1)\n",
    "        \n",
    "        assert np.isclose(expected_l2_cost, actual_l2_cost, atol=1e-6), \"L2 cost calculation incorrect\"\n",
    "        \n",
    "        # Test dropout probability\n",
    "        test_input = np.ones((5, 100))\n",
    "        dropped_output, dropout_cache = RegularizationTechniques.dropout_forward(\n",
    "            test_input, keep_prob=0.5, training=True\n",
    "        )\n",
    "        \n",
    "        # Check that about 50% of neurons are kept\n",
    "        kept_ratio = np.mean(dropout_cache > 0)\n",
    "        assert 0.4 < kept_ratio < 0.6, f\"Dropout ratio incorrect: {kept_ratio}\"\n",
    "        \n",
    "        # Test that scaling is applied\n",
    "        non_zero_mean = np.mean(dropped_output[dropped_output > 0])\n",
    "        assert np.isclose(non_zero_mean, 2.0, atol=0.1), \"Dropout scaling incorrect\"\n",
    "        \n",
    "        print(\"✅ Regularization effects test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Regularization effects test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_regularization_effects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 3: Training Improvement\n",
    "def test_training_improvement():\n",
    "    \"\"\"Test that optimization improves performance\"\"\"\n",
    "    try:\n",
    "        # Create simple dataset\n",
    "        X_simple, y_simple = make_classification(\n",
    "            n_samples=200, n_features=10, n_classes=2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "            X_simple, y_simple, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        scaler_s = StandardScaler()\n",
    "        X_train_s = scaler_s.fit_transform(X_train_s).T\n",
    "        X_test_s = scaler_s.transform(X_test_s).T\n",
    "        y_train_s = y_train_s.reshape(1, -1)\n",
    "        y_test_s = y_test_s.reshape(1, -1)\n",
    "        \n",
    "        # Train with Adam optimizer\n",
    "        nn_test = OptimizedNeuralNetwork(optimizer='adam', learning_rate=0.01)\n",
    "        parameters = nn_test.train(\n",
    "            X_train_s, y_train_s, X_test_s, y_test_s,\n",
    "            layer_dims=[10, 8, 1],\n",
    "            num_iterations=500,\n",
    "            print_cost=False\n",
    "        )\n",
    "        \n",
    "        # Check that cost decreases\n",
    "        initial_cost = nn_test.history['train_cost'][0]\n",
    "        final_cost = nn_test.history['train_cost'][-1]\n",
    "        \n",
    "        assert final_cost < initial_cost, \"Training cost did not decrease\"\n",
    "        \n",
    "        # Check that accuracy improves\n",
    "        initial_acc = nn_test.history['train_accuracy'][0]\n",
    "        final_acc = nn_test.history['train_accuracy'][-1]\n",
    "        \n",
    "        assert final_acc > initial_acc, \"Training accuracy did not improve\"\n",
    "        assert final_acc > 80, f\"Final accuracy too low: {final_acc}%\"\n",
    "        \n",
    "        print(f\"Initial cost: {initial_cost:.4f} → Final cost: {final_cost:.4f}\")\n",
    "        print(f\"Initial accuracy: {initial_acc:.1f}% → Final accuracy: {final_acc:.1f}%\")\n",
    "        print(\"✅ Training improvement test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training improvement test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_training_improvement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Slow convergence with SGD**\n",
    "- **Cause**: Learning rate too low or poor conditioning\n",
    "- **Solution**: Try higher learning rate, use momentum or Adam optimizer\n",
    "\n",
    "**Issue 2: Training becomes unstable**\n",
    "- **Cause**: Learning rate too high, gradient explosion\n",
    "- **Solution**: Reduce learning rate, add gradient clipping, check weight initialization\n",
    "\n",
    "**Issue 3: Overfitting despite regularization**\n",
    "- **Cause**: Insufficient regularization or too complex model\n",
    "- **Solution**: Increase regularization strength, reduce network size, get more data\n",
    "\n",
    "**Issue 4: Adam optimizer not converging**\n",
    "- **Cause**: Inappropriate hyperparameters\n",
    "- **Solution**: Try different learning rate (0.001-0.01), adjust beta parameters\n",
    "\n",
    "**Issue 5: Dropout causing training instability**\n",
    "- **Cause**: Keep probability too low\n",
    "- **Solution**: Increase keep_prob (try 0.8-0.9), ensure dropout is off during testing\n",
    "\n",
    "### Performance Tips:\n",
    "- Start with Adam optimizer for most problems\n",
    "- Use learning rate scheduling for better convergence\n",
    "- Monitor both training and validation metrics\n",
    "- Apply regularization based on overfitting evidence\n",
    "- Experiment with different initialization schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell → All Output → Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"🎉 Lab 3.5: Performance Optimization Techniques Completed!\")\n",
    "print(\"\\n📋 What you accomplished:\")\n",
    "print(\"✅ Implemented advanced optimization algorithms (SGD, Momentum, Adam)\")\n",
    "print(\"✅ Added regularization techniques (L2, Dropout)\")\n",
    "print(\"✅ Built comprehensive optimized neural network framework\")\n",
    "print(\"✅ Conducted systematic optimization algorithm comparison\")\n",
    "print(\"✅ Analyzed regularization effects on overfitting\")\n",
    "print(\"✅ Created detailed performance visualizations\")\n",
    "print(\"\\n🎯 Next: Lab 3.6 - Shallow Network Application Project\")\n",
    "print(\"\\n🚀 Key Takeaways:\")\n",
    "print(\"• Adam optimizer often provides best performance\")\n",
    "print(\"• Regularization is crucial for preventing overfitting\")\n",
    "print(\"• Monitor training vs validation metrics\")\n",
    "print(\"• Hyperparameter tuning is essential for optimal results\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\n🧹 Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}