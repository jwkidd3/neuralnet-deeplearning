{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5: Performance Optimization Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement various optimization techniques for neural networks\n",
    "- Compare different optimization algorithms (SGD, Momentum, Adam)\n",
    "- Apply regularization techniques to prevent overfitting\n",
    "- Optimize training procedures and hyperparameters\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 3.1-3.4\n",
    "- Understanding of gradient descent and backpropagation\n",
    "- Knowledge of overfitting and underfitting concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Optimization Algorithms (15 minutes)\n",
    "\n",
    "### 1.1 Base Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptimizer:\n",
    "    \"\"\"Base class for optimization algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0  # Time step\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Update parameters using gradients\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement update method\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        self.t = 0\n",
    "\n",
    "class SGDOptimizer(BaseOptimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__(learning_rate)\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Standard SGD update\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                parameters[key] -= self.learning_rate * gradients[f'd{key}']\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "class MomentumOptimizer(BaseOptimizer):\n",
    "    \"\"\"SGD with Momentum optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta = beta\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Momentum update rule\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize velocity on first update\n",
    "        if not self.velocity:\n",
    "            for key in parameters:\n",
    "                if f'd{key}' in gradients:\n",
    "                    self.velocity[f'v_{key}'] = np.zeros_like(parameters[key])\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                # Update velocity\n",
    "                self.velocity[f'v_{key}'] = (self.beta * self.velocity[f'v_{key}'] + \n",
    "                                           (1 - self.beta) * gradients[f'd{key}'])\n",
    "                \n",
    "                # Update parameters\n",
    "                parameters[key] -= self.learning_rate * self.velocity[f'v_{key}']\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.velocity = {}\n",
    "\n",
    "class AdamOptimizer(BaseOptimizer):\n",
    "    \"\"\"Adam optimizer (Adaptive Moment Estimation)\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = {}  # First moment (mean)\n",
    "        self.velocity = {}  # Second moment (variance)\n",
    "    \n",
    "    def update(self, parameters, gradients):\n",
    "        \"\"\"Adam update rule\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize moments on first update\n",
    "        if not self.momentum:\n",
    "            for key in parameters:\n",
    "                if f'd{key}' in gradients:\n",
    "                    self.momentum[f'm_{key}'] = np.zeros_like(parameters[key])\n",
    "                    self.velocity[f'v_{key}'] = np.zeros_like(parameters[key])\n",
    "        \n",
    "        for key in parameters:\n",
    "            if f'd{key}' in gradients:\n",
    "                grad = gradients[f'd{key}']\n",
    "                \n",
    "                # Update biased first moment estimate\n",
    "                self.momentum[f'm_{key}'] = (self.beta1 * self.momentum[f'm_{key}'] + \n",
    "                                           (1 - self.beta1) * grad)\n",
    "                \n",
    "                # Update biased second raw moment estimate\n",
    "                self.velocity[f'v_{key}'] = (self.beta2 * self.velocity[f'v_{key}'] + \n",
    "                                           (1 - self.beta2) * (grad ** 2))\n",
    "                \n",
    "                # Compute bias-corrected first moment estimate\n",
    "                m_corrected = self.momentum[f'm_{key}'] / (1 - self.beta1 ** self.t)\n",
    "                \n",
    "                # Compute bias-corrected second raw moment estimate\n",
    "                v_corrected = self.velocity[f'v_{key}'] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                parameters[key] -= (self.learning_rate * m_corrected / \n",
    "                                  (np.sqrt(v_corrected) + self.epsilon))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.momentum = {}\n",
    "        self.velocity = {}\n",
    "\n",
    "print(\"Optimization algorithms implemented!\")\n",
    "print(\"Available optimizers: SGD, Momentum, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationTechniques:\n",
    "    \"\"\"Collection of regularization techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_regularization_cost(parameters, lambda_reg):\n",
    "        \"\"\"\n",
    "        Compute L2 regularization cost\n",
    "        \n",
    "        Parameters:\n",
    "        parameters: network parameters\n",
    "        lambda_reg: regularization strength\n",
    "        \n",
    "        Returns:\n",
    "        l2_cost: L2 regularization cost\n",
    "        \"\"\"\n",
    "        l2_cost = 0\n",
    "        L = len(parameters) // 2  # Number of layers\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            l2_cost += np.sum(np.square(parameters[f'W{l}']))\n",
    "        \n",
    "        return lambda_reg / 2 * l2_cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_regularization_gradients(parameters, gradients, lambda_reg):\n",
    "        \"\"\"\n",
    "        Add L2 regularization to gradients\n",
    "        \n",
    "        Parameters:\n",
    "        parameters: network parameters\n",
    "        gradients: computed gradients\n",
    "        lambda_reg: regularization strength\n",
    "        \n",
    "        Returns:\n",
    "        gradients: modified gradients with L2 regularization\n",
    "        \"\"\"\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            gradients[f'dW{l}'] += lambda_reg * parameters[f'W{l}']\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_forward(A, keep_prob, training=True):\n",
    "        \"\"\"\n",
    "        Apply dropout during forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        A: activations\n",
    "        keep_prob: probability of keeping each neuron\n",
    "        training: whether in training mode\n",
    "        \n",
    "        Returns:\n",
    "        A: activations after dropout\n",
    "        dropout_cache: cache for backward propagation\n",
    "        \"\"\"\n",
    "        if not training or keep_prob == 1.0:\n",
    "            return A, None\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        dropout_mask = np.random.rand(*A.shape) < keep_prob\n",
    "        \n",
    "        # Apply dropout\n",
    "        A = A * dropout_mask\n",
    "        \n",
    "        # Scale to maintain expected value\n",
    "        A = A / keep_prob\n",
    "        \n",
    "        dropout_cache = dropout_mask / keep_prob\n",
    "        \n",
    "        return A, dropout_cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_backward(dA, dropout_cache):\n",
    "        \"\"\"\n",
    "        Apply dropout during backward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        dA: upstream gradients\n",
    "        dropout_cache: cache from forward dropout\n",
    "        \n",
    "        Returns:\n",
    "        dA: gradients after dropout\n",
    "        \"\"\"\n",
    "        if dropout_cache is None:\n",
    "            return dA\n",
    "        \n",
    "        return dA * dropout_cache\n",
    "\n",
    "# Test regularization techniques\n",
    "print(\"Testing Regularization Techniques:\")\n",
    "\n",
    "# Test L2 regularization\n",
    "test_params = {\n",
    "    'W1': np.array([[0.1, 0.2], [0.3, 0.4]]),\n",
    "    'b1': np.array([[0.1], [0.2]]),\n",
    "    'W2': np.array([[0.5, 0.6]]),\n",
    "    'b2': np.array([[0.3]])\n",
    "}\n",
    "\n",
    "l2_cost = RegularizationTechniques.l2_regularization_cost(test_params, lambda_reg=0.1)\n",
    "print(f\"L2 regularization cost: {l2_cost:.6f}\")\n",
    "\n",
    "# Test dropout\n",
    "test_activations = np.random.randn(5, 10)\n",
    "dropped_A, dropout_cache = RegularizationTechniques.dropout_forward(\n",
    "    test_activations, keep_prob=0.8, training=True\n",
    ")\n",
    "\n",
    "print(f\"Original activations shape: {test_activations.shape}\")\n",
    "print(f\"After dropout shape: {dropped_A.shape}\")\n",
    "print(f\"Percentage of neurons kept: {np.mean(dropout_cache > 0) * 100:.1f}%\")\n",
    "print(\"‚úÖ Regularization techniques implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Optimized Neural Network (15 minutes)\n",
    "\n",
    "### Enhanced Neural Network with Optimization Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedNeuralNetwork:\n",
    "    \"\"\"Neural network with optimization and regularization features\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer='adam', regularization='none', lambda_reg=0.01, \n",
    "                 keep_prob=1.0, **optimizer_kwargs):\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'sgd':\n",
    "            self.optimizer = SGDOptimizer(**optimizer_kwargs)\n",
    "        elif optimizer.lower() == 'momentum':\n",
    "            self.optimizer = MomentumOptimizer(**optimizer_kwargs)\n",
    "        elif optimizer.lower() == 'adam':\n",
    "            self.optimizer = AdamOptimizer(**optimizer_kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "        \n",
    "        self.regularization = regularization.lower()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.keep_prob = keep_prob\n",
    "        self.regularizer = RegularizationTechniques()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_cost': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_cost': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        self.parameters = {}\n",
    "    \n",
    "    def initialize_parameters(self, layer_dims, initialization='xavier'):\n",
    "        \"\"\"Initialize network parameters with different initialization schemes\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            if initialization.lower() == 'xavier':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1.0 / layer_dims[l-1])\n",
    "            elif initialization.lower() == 'he':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            elif initialization.lower() == 'random':\n",
    "                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initialization: {initialization}\")\n",
    "            \n",
    "            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, X, parameters, training=True):\n",
    "        \"\"\"Forward propagation with dropout support\"\"\"\n",
    "        caches = []\n",
    "        dropout_caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            W = parameters[f'W{l}']\n",
    "            b = parameters[f'b{l}']\n",
    "            \n",
    "            # Linear transformation\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            \n",
    "            # ReLU activation\n",
    "            A = np.maximum(0, Z)\n",
    "            \n",
    "            # Apply dropout\n",
    "            A, dropout_cache = self.regularizer.dropout_forward(A, self.keep_prob, training)\n",
    "            \n",
    "            # Store caches\n",
    "            linear_cache = (A_prev, W, b)\n",
    "            caches.append((linear_cache, Z))\n",
    "            dropout_caches.append(dropout_cache)\n",
    "        \n",
    "        # Output layer (no dropout)\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{L}']\n",
    "        b = parameters[f'b{L}']\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = 1 / (1 + np.exp(-np.clip(Z, -500, 500)))  # Sigmoid\n",
    "        \n",
    "        linear_cache = (A_prev, W, b)\n",
    "        caches.append((linear_cache, Z))\n",
    "        dropout_caches.append(None)  # No dropout on output layer\n",
    "        \n",
    "        return A, caches, dropout_caches\n",
    "    \n",
    "    def compute_cost(self, AL, Y, parameters):\n",
    "        \"\"\"Compute cost with regularization\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Binary cross-entropy cost\n",
    "        AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        cross_entropy_cost = -1/m * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n",
    "        \n",
    "        # Add regularization\n",
    "        regularization_cost = 0\n",
    "        if self.regularization == 'l2':\n",
    "            regularization_cost = self.regularizer.l2_regularization_cost(parameters, self.lambda_reg)\n",
    "        \n",
    "        total_cost = cross_entropy_cost + regularization_cost\n",
    "        \n",
    "        return np.squeeze(total_cost)\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches, dropout_caches, parameters):\n",
    "        \"\"\"Backward propagation with dropout and regularization\"\"\"\n",
    "        gradients = {}\n",
    "        L = len(caches)\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        dAL = -(Y / AL) + (1 - Y) / (1 - AL)\n",
    "        \n",
    "        # Output layer\n",
    "        (A_prev, W, b), Z = caches[L-1]\n",
    "        dZ = dAL * AL * (1 - AL)  # Sigmoid derivative\n",
    "        \n",
    "        gradients[f'dW{L}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "        gradients[f'db{L}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in reversed(range(L-1)):\n",
    "            (A_prev, W, b), Z = caches[l]\n",
    "            \n",
    "            # Apply dropout to gradients\n",
    "            dA_prev = self.regularizer.dropout_backward(dA_prev, dropout_caches[l+1])\n",
    "            \n",
    "            # ReLU derivative\n",
    "            dZ = dA_prev * (Z > 0).astype(float)\n",
    "            \n",
    "            gradients[f'dW{l+1}'] = 1/m * np.dot(dZ, A_prev.T)\n",
    "            gradients[f'db{l+1}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 0:\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        # Add regularization to gradients\n",
    "        if self.regularization == 'l2':\n",
    "            gradients = self.regularizer.l2_regularization_gradients(\n",
    "                parameters, gradients, self.lambda_reg\n",
    "            )\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_val, Y_val, layer_dims, \n",
    "              num_iterations=1000, print_cost=True, print_every=100):\n",
    "        \"\"\"Train the optimized neural network\"\"\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters(layer_dims)\n",
    "        \n",
    "        # Reset optimizer\n",
    "        self.optimizer.reset()\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            # Forward propagation (training mode)\n",
    "            AL_train, caches, dropout_caches = self.forward_propagation(X_train, parameters, training=True)\n",
    "            \n",
    "            # Compute training cost\n",
    "            train_cost = self.compute_cost(AL_train, Y_train, parameters)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_propagation(AL_train, Y_train, caches, dropout_caches, parameters)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = self.optimizer.update(parameters, gradients)\n",
    "            \n",
    "            # Validation (every print_every iterations)\n",
    "            if i % print_every == 0:\n",
    "                # Training accuracy\n",
    "                train_predictions = (AL_train > 0.5).astype(int)\n",
    "                train_accuracy = np.mean(train_predictions == Y_train) * 100\n",
    "                \n",
    "                # Validation forward pass (no dropout)\n",
    "                AL_val, _, _ = self.forward_propagation(X_val, parameters, training=False)\n",
    "                val_cost = self.compute_cost(AL_val, Y_val, parameters)\n",
    "                \n",
    "                val_predictions = (AL_val > 0.5).astype(int)\n",
    "                val_accuracy = np.mean(val_predictions == Y_val) * 100\n",
    "                \n",
    "                # Store history\n",
    "                self.history['train_cost'].append(train_cost)\n",
    "                self.history['train_accuracy'].append(train_accuracy)\n",
    "                self.history['val_cost'].append(val_cost)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "                \n",
    "                if print_cost:\n",
    "                    print(f\"Iteration {i}: Train Cost = {train_cost:.6f}, Train Acc = {train_accuracy:.2f}%, \"\n",
    "                          f\"Val Cost = {val_cost:.6f}, Val Acc = {val_accuracy:.2f}%\")\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        return parameters\n",
    "    \n",
    "    def predict(self, X, parameters=None):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters\n",
    "        \n",
    "        AL, _, _ = self.forward_propagation(X, parameters, training=False)\n",
    "        predictions = (AL > 0.5).astype(int)\n",
    "        \n",
    "        return predictions, AL\n",
    "\n",
    "print(\"OptimizedNeuralNetwork class implemented successfully!\")\n",
    "print(\"Features: Multiple optimizers, L2 regularization, dropout, initialization schemes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimization Comparison Experiment (10 minutes)\n",
    "\n",
    "### Compare Different Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for optimization comparison\n",
    "print(\"Preparing Dataset for Optimization Comparison...\")\n",
    "\n",
    "# Load breast cancer dataset (binary classification)\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Classes: {np.unique(y)} (0: malignant, 1: benign)\")\n",
    "\n",
    "# Split and preprocess data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training set for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Transpose to (features, samples) format\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_val = y_val.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Network architecture\n",
    "layer_dims = [X_train.shape[0], 16, 8, 1]  # 30 features -> 16 -> 8 -> 1\n",
    "print(f\"Network architecture: {layer_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "print(\"Comparing Optimization Algorithms...\")\n",
    "\n",
    "optimizers_config = {\n",
    "    'SGD': {'optimizer': 'sgd', 'learning_rate': 0.1},\n",
    "    'Momentum': {'optimizer': 'momentum', 'learning_rate': 0.1, 'beta': 0.9},\n",
    "    'Adam': {'optimizer': 'adam', 'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.999}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "training_time = {}\n",
    "\n",
    "for optimizer_name, config in optimizers_config.items():\n",
    "    print(f\"\\nTraining with {optimizer_name} optimizer...\")\n",
    "    \n",
    "    # Create network\n",
    "    nn = OptimizedNeuralNetwork(\n",
    "        regularization='none',\n",
    "        keep_prob=1.0,  # No dropout for fair comparison\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    parameters = nn.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        layer_dims=layer_dims,\n",
    "        num_iterations=1000,\n",
    "        print_cost=False  # Silent training\n",
    "    )\n",
    "    training_time[optimizer_name] = time.time() - start_time\n",
    "    \n",
    "    # Test performance\n",
    "    test_predictions, test_probs = nn.predict(X_test)\n",
    "    test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "    \n",
    "    # Store results\n",
    "    results[optimizer_name] = {\n",
    "        'history': nn.history.copy(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'final_train_cost': nn.history['train_cost'][-1],\n",
    "        'final_val_cost': nn.history['val_cost'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"{optimizer_name} - Test Accuracy: {test_accuracy:.2f}%, Training Time: {training_time[optimizer_name]:.2f}s\")\n",
    "\n",
    "print(\"\\n=== OPTIMIZATION COMPARISON RESULTS ===\")\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    result = results[optimizer_name]\n",
    "    print(f\"{optimizer_name}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Final Training Cost: {result['final_train_cost']:.6f}\")\n",
    "    print(f\"  Final Validation Cost: {result['final_val_cost']:.6f}\")\n",
    "    print(f\"  Training Time: {training_time[optimizer_name]:.2f}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training cost comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "iterations = np.arange(0, 1000, 100)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['train_cost'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Training Cost Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation cost comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['val_cost'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Validation Cost Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['train_accuracy'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "for optimizer_name in optimizers_config.keys():\n",
    "    plt.plot(iterations, results[optimizer_name]['history']['val_accuracy'], \n",
    "             label=f'{optimizer_name}', linewidth=2)\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final test accuracy bar chart\n",
    "plt.subplot(2, 3, 5)\n",
    "optimizer_names = list(optimizers_config.keys())\n",
    "test_accuracies = [results[name]['test_accuracy'] for name in optimizer_names]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = plt.bar(optimizer_names, test_accuracies, color=colors, alpha=0.7)\n",
    "plt.title('Final Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, test_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{accuracy:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "times = [training_time[name] for name in optimizer_names]\n",
    "bars = plt.bar(optimizer_names, times, color=colors, alpha=0.7)\n",
    "plt.title('Training Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "best_accuracy = max(test_accuracies)\n",
    "best_optimizer = optimizer_names[test_accuracies.index(best_accuracy)]\n",
    "print(f\"‚Ä¢ Best performing optimizer: {best_optimizer} ({best_accuracy:.2f}% test accuracy)\")\n",
    "\n",
    "fastest_time = min(times)\n",
    "fastest_optimizer = optimizer_names[times.index(fastest_time)]\n",
    "print(f\"‚Ä¢ Fastest optimizer: {fastest_optimizer} ({fastest_time:.2f}s training time)\")\n",
    "print(f\"‚Ä¢ Adam typically provides good balance of speed and performance\")\n",
    "print(f\"‚Ä¢ Momentum helps accelerate SGD in relevant directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization Effects (5 minutes)\n",
    "\n",
    "### Compare Networks with and without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization effects\n",
    "print(\"Comparing Regularization Effects...\")\n",
    "\n",
    "regularization_configs = {\n",
    "    'No Regularization': {'regularization': 'none', 'keep_prob': 1.0},\n",
    "    'L2 Regularization': {'regularization': 'l2', 'lambda_reg': 0.01, 'keep_prob': 1.0},\n",
    "    'Dropout': {'regularization': 'none', 'keep_prob': 0.8},\n",
    "    'L2 + Dropout': {'regularization': 'l2', 'lambda_reg': 0.01, 'keep_prob': 0.8}\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for config_name, config in regularization_configs.items():\n",
    "    print(f\"\\nTraining with {config_name}...\")\n",
    "    \n",
    "    # Create network with Adam optimizer\n",
    "    nn = OptimizedNeuralNetwork(\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.01,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    parameters = nn.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        layer_dims=layer_dims,\n",
    "        num_iterations=800,\n",
    "        print_cost=False\n",
    "    )\n",
    "    \n",
    "    # Test performance\n",
    "    test_predictions, test_probs = nn.predict(X_test)\n",
    "    test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "    \n",
    "    # Calculate overfitting metric (train vs val accuracy difference)\n",
    "    final_train_acc = nn.history['train_accuracy'][-1]\n",
    "    final_val_acc = nn.history['val_accuracy'][-1]\n",
    "    overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    reg_results[config_name] = {\n",
    "        'history': nn.history.copy(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'final_train_acc': final_train_acc,\n",
    "        'final_val_acc': final_val_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"{config_name} - Test Accuracy: {test_accuracy:.2f}%, Overfitting Gap: {overfitting_gap:.2f}%\")\n",
    "\n",
    "# Visualize regularization effects\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Training vs Validation accuracy\n",
    "plt.subplot(2, 3, 1)\n",
    "iterations = np.arange(0, 800, 100)\n",
    "for config_name in regularization_configs.keys():\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['train_accuracy'], \n",
    "             label=f'{config_name} (Train)', linewidth=2, linestyle='-')\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['val_accuracy'], \n",
    "             label=f'{config_name} (Val)', linewidth=2, linestyle='--')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "config_names = list(regularization_configs.keys())\n",
    "overfitting_gaps = [reg_results[name]['overfitting_gap'] for name in config_names]\n",
    "colors = ['red', 'orange', 'lightblue', 'green']\n",
    "\n",
    "bars = plt.bar(config_names, overfitting_gaps, color=colors, alpha=0.7)\n",
    "plt.title('Overfitting Gap\\n(Train Acc - Val Acc)')\n",
    "plt.ylabel('Accuracy Gap (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, gap in zip(bars, overfitting_gaps):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{gap:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Test accuracy comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "test_accuracies_reg = [reg_results[name]['test_accuracy'] for name in config_names]\n",
    "bars = plt.bar(config_names, test_accuracies_reg, color=colors, alpha=0.7)\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(90, 100)\n",
    "\n",
    "# Add value labels\n",
    "for bar, accuracy in zip(bars, test_accuracies_reg):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             f'{accuracy:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Cost curves\n",
    "plt.subplot(2, 3, 4)\n",
    "for config_name in regularization_configs.keys():\n",
    "    plt.plot(iterations, reg_results[config_name]['history']['train_cost'], \n",
    "             label=f'{config_name}', linewidth=2)\n",
    "plt.title('Training Cost Curves')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Regularization Analysis:\")\n",
    "best_generalization = min(overfitting_gaps)\n",
    "best_reg_method = config_names[overfitting_gaps.index(best_generalization)]\n",
    "print(f\"‚Ä¢ Best generalization: {best_reg_method} (gap: {best_generalization:.2f}%)\")\n",
    "print(f\"‚Ä¢ Regularization helps reduce overfitting\")\n",
    "print(f\"‚Ä¢ L2 regularization adds penalty to large weights\")\n",
    "print(f\"‚Ä¢ Dropout provides regularization through random neuron deactivation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Check off each item as you complete it:\n",
    "\n",
    "- [ ] **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] **Optimizer Implementation**: Created SGD, Momentum, and Adam optimizers\n",
    "- [ ] **Regularization Techniques**: Implemented L2 regularization and dropout\n",
    "- [ ] **Enhanced Network**: Built optimized neural network with all features\n",
    "- [ ] **Dataset Preparation**: Loaded and preprocessed breast cancer dataset\n",
    "- [ ] **Optimizer Comparison**: Compared SGD, Momentum, and Adam performance\n",
    "- [ ] **Regularization Analysis**: Tested different regularization approaches\n",
    "- [ ] **Visualization**: Created comprehensive performance comparison plots\n",
    "- [ ] **Performance Analysis**: Analyzed results and identified best practices\n",
    "- [ ] **Lab Completion**: Successfully completed all exercises\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Optimization Algorithms**: SGD, Momentum, Adam and their trade-offs\n",
    "2. **Regularization**: L2 penalty and dropout for overfitting prevention\n",
    "3. **Performance Metrics**: Training/validation curves, overfitting detection\n",
    "4. **Hyperparameter Effects**: Learning rate, regularization strength impact\n",
    "5. **Best Practices**: When to use different optimization techniques\n",
    "\n",
    "### Technical Insights:\n",
    "- **Adam Optimizer**: Often provides best balance of speed and performance\n",
    "- **Momentum**: Helps SGD overcome local minima and narrow valleys\n",
    "- **L2 Regularization**: Prevents weights from becoming too large\n",
    "- **Dropout**: Randomly deactivates neurons to prevent co-adaptation\n",
    "- **Bias Correction**: Adam uses bias correction for more stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 1: Optimizer State Management\n",
    "def test_optimizer_state():\n",
    "    \"\"\"Test optimizer state management\"\"\"\n",
    "    try:\n",
    "        # Test Adam optimizer state\n",
    "        adam = AdamOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        # Dummy parameters and gradients\n",
    "        params = {'W1': np.random.randn(3, 2), 'b1': np.zeros((3, 1))}\n",
    "        grads = {'dW1': np.random.randn(3, 2), 'db1': np.random.randn(3, 1)}\n",
    "        \n",
    "        # First update\n",
    "        params_updated = adam.update(params, grads)\n",
    "        \n",
    "        # Check that momentum and velocity are initialized\n",
    "        assert len(adam.momentum) > 0, \"Momentum not initialized\"\n",
    "        assert len(adam.velocity) > 0, \"Velocity not initialized\"\n",
    "        assert adam.t == 1, \"Time step not updated\"\n",
    "        \n",
    "        # Reset and check state is cleared\n",
    "        adam.reset()\n",
    "        assert len(adam.momentum) == 0, \"Momentum not reset\"\n",
    "        assert len(adam.velocity) == 0, \"Velocity not reset\"\n",
    "        assert adam.t == 0, \"Time step not reset\"\n",
    "        \n",
    "        print(\"‚úÖ Optimizer state management test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimizer state management test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_optimizer_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 2: Regularization Effects\n",
    "def test_regularization_effects():\n",
    "    \"\"\"Test regularization implementation\"\"\"\n",
    "    try:\n",
    "        # Test L2 regularization cost\n",
    "        params = {\n",
    "            'W1': np.array([[1.0, 2.0], [3.0, 4.0]]),\n",
    "            'b1': np.array([[0.1], [0.2]]),\n",
    "            'W2': np.array([[0.5, 0.6]]),\n",
    "            'b2': np.array([[0.1]])\n",
    "        }\n",
    "        \n",
    "        # Calculate expected L2 cost: 0.5 * lambda * (1¬≤ + 2¬≤ + 3¬≤ + 4¬≤ + 0.5¬≤ + 0.6¬≤)\n",
    "        expected_l2_cost = 0.5 * 0.1 * (1 + 4 + 9 + 16 + 0.25 + 0.36)\n",
    "        actual_l2_cost = RegularizationTechniques.l2_regularization_cost(params, 0.1)\n",
    "        \n",
    "        assert np.isclose(expected_l2_cost, actual_l2_cost, atol=1e-6), \"L2 cost calculation incorrect\"\n",
    "        \n",
    "        # Test dropout probability\n",
    "        test_input = np.ones((5, 100))\n",
    "        dropped_output, dropout_cache = RegularizationTechniques.dropout_forward(\n",
    "            test_input, keep_prob=0.5, training=True\n",
    "        )\n",
    "        \n",
    "        # Check that about 50% of neurons are kept\n",
    "        kept_ratio = np.mean(dropout_cache > 0)\n",
    "        assert 0.4 < kept_ratio < 0.6, f\"Dropout ratio incorrect: {kept_ratio}\"\n",
    "        \n",
    "        # Test that scaling is applied\n",
    "        non_zero_mean = np.mean(dropped_output[dropped_output > 0])\n",
    "        assert np.isclose(non_zero_mean, 2.0, atol=0.1), \"Dropout scaling incorrect\"\n",
    "        \n",
    "        print(\"‚úÖ Regularization effects test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Regularization effects test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_regularization_effects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test 3: Training Improvement\n",
    "def test_training_improvement():\n",
    "    \"\"\"Test that optimization improves performance\"\"\"\n",
    "    try:\n",
    "        # Create simple dataset\n",
    "        X_simple, y_simple = make_classification(\n",
    "            n_samples=200, n_features=10, n_classes=2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "            X_simple, y_simple, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        scaler_s = StandardScaler()\n",
    "        X_train_s = scaler_s.fit_transform(X_train_s).T\n",
    "        X_test_s = scaler_s.transform(X_test_s).T\n",
    "        y_train_s = y_train_s.reshape(1, -1)\n",
    "        y_test_s = y_test_s.reshape(1, -1)\n",
    "        \n",
    "        # Train with Adam optimizer\n",
    "        nn_test = OptimizedNeuralNetwork(optimizer='adam', learning_rate=0.01)\n",
    "        parameters = nn_test.train(\n",
    "            X_train_s, y_train_s, X_test_s, y_test_s,\n",
    "            layer_dims=[10, 8, 1],\n",
    "            num_iterations=500,\n",
    "            print_cost=False\n",
    "        )\n",
    "        \n",
    "        # Check that cost decreases\n",
    "        initial_cost = nn_test.history['train_cost'][0]\n",
    "        final_cost = nn_test.history['train_cost'][-1]\n",
    "        \n",
    "        assert final_cost < initial_cost, \"Training cost did not decrease\"\n",
    "        \n",
    "        # Check that accuracy improves\n",
    "        initial_acc = nn_test.history['train_accuracy'][0]\n",
    "        final_acc = nn_test.history['train_accuracy'][-1]\n",
    "        \n",
    "        assert final_acc > initial_acc, \"Training accuracy did not improve\"\n",
    "        assert final_acc > 80, f\"Final accuracy too low: {final_acc}%\"\n",
    "        \n",
    "        print(f\"Initial cost: {initial_cost:.4f} ‚Üí Final cost: {final_cost:.4f}\")\n",
    "        print(f\"Initial accuracy: {initial_acc:.1f}% ‚Üí Final accuracy: {final_acc:.1f}%\")\n",
    "        print(\"‚úÖ Training improvement test passed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training improvement test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_training_improvement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Slow convergence with SGD**\n",
    "- **Cause**: Learning rate too low or poor conditioning\n",
    "- **Solution**: Try higher learning rate, use momentum or Adam optimizer\n",
    "\n",
    "**Issue 2: Training becomes unstable**\n",
    "- **Cause**: Learning rate too high, gradient explosion\n",
    "- **Solution**: Reduce learning rate, add gradient clipping, check weight initialization\n",
    "\n",
    "**Issue 3: Overfitting despite regularization**\n",
    "- **Cause**: Insufficient regularization or too complex model\n",
    "- **Solution**: Increase regularization strength, reduce network size, get more data\n",
    "\n",
    "**Issue 4: Adam optimizer not converging**\n",
    "- **Cause**: Inappropriate hyperparameters\n",
    "- **Solution**: Try different learning rate (0.001-0.01), adjust beta parameters\n",
    "\n",
    "**Issue 5: Dropout causing training instability**\n",
    "- **Cause**: Keep probability too low\n",
    "- **Solution**: Increase keep_prob (try 0.8-0.9), ensure dropout is off during testing\n",
    "\n",
    "### Performance Tips:\n",
    "- Start with Adam optimizer for most problems\n",
    "- Use learning rate scheduling for better convergence\n",
    "- Monitor both training and validation metrics\n",
    "- Apply regularization based on overfitting evidence\n",
    "- Experiment with different initialization schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Instructions\n",
    "\n",
    "1. **Save your work**: Save this notebook with your implementations\n",
    "2. **Clear output**: Cell ‚Üí All Output ‚Üí Clear (optional, saves space)\n",
    "3. **Close plots**: Close any open matplotlib windows\n",
    "4. **Memory cleanup**: Variables will be cleared when kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"üéâ Lab 3.5: Performance Optimization Techniques Completed!\")\n",
    "print(\"\\nüìã What you accomplished:\")\n",
    "print(\"‚úÖ Implemented advanced optimization algorithms (SGD, Momentum, Adam)\")\n",
    "print(\"‚úÖ Added regularization techniques (L2, Dropout)\")\n",
    "print(\"‚úÖ Built comprehensive optimized neural network framework\")\n",
    "print(\"‚úÖ Conducted systematic optimization algorithm comparison\")\n",
    "print(\"‚úÖ Analyzed regularization effects on overfitting\")\n",
    "print(\"‚úÖ Created detailed performance visualizations\")\n",
    "print(\"\\nüéØ Next: Lab 3.6 - Shallow Network Application Project\")\n",
    "print(\"\\nüöÄ Key Takeaways:\")\n",
    "print(\"‚Ä¢ Adam optimizer often provides best performance\")\n",
    "print(\"‚Ä¢ Regularization is crucial for preventing overfitting\")\n",
    "print(\"‚Ä¢ Monitor training vs validation metrics\")\n",
    "print(\"‚Ä¢ Hyperparameter tuning is essential for optimal results\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nüßπ Memory cleaned up successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}