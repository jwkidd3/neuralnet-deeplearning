{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2: Mathematical Foundations Review\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Review essential linear algebra concepts for neural networks\n",
    "- Understand and implement vector and matrix operations\n",
    "- Apply calculus concepts (derivatives) in the context of neural networks\n",
    "- Practice mathematical operations that form the backbone of deep learning\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1.1 (Environment Setup)\n",
    "- Basic understanding of high school algebra and calculus\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Environment ready for mathematical foundations review!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vectors and Vector Operations\n",
    "\n",
    "In neural networks, we frequently work with vectors representing data points, weights, and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 1: VECTORS AND VECTOR OPERATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create sample vectors\n",
    "vector_a = np.array([1, 2, 3])\n",
    "vector_b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"Vector a: {vector_a}\")\n",
    "print(f\"Vector b: {vector_b}\")\n",
    "print(f\"Shape of vector a: {vector_a.shape}\")\n",
    "print(f\"Length of vector a: {len(vector_a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Addition and Subtraction\n",
    "print(\"\\nVector Arithmetic:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "vector_sum = vector_a + vector_b\n",
    "vector_diff = vector_a - vector_b\n",
    "\n",
    "print(f\"a + b = {vector_sum}\")\n",
    "print(f\"a - b = {vector_diff}\")\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar = 2.5\n",
    "scaled_vector = scalar * vector_a\n",
    "print(f\"\\n{scalar} * a = {scaled_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product (very important for neural networks!)\n",
    "print(\"\\nDot Product:\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "dot_product = np.dot(vector_a, vector_b)\n",
    "print(f\"a Â· b = {dot_product}\")\n",
    "\n",
    "# Alternative way to compute dot product\n",
    "dot_product_alt = np.sum(vector_a * vector_b)\n",
    "print(f\"Alternative calculation: {dot_product_alt}\")\n",
    "\n",
    "# Manual calculation for understanding\n",
    "manual_dot = vector_a[0]*vector_b[0] + vector_a[1]*vector_b[1] + vector_a[2]*vector_b[2]\n",
    "print(f\"Manual calculation: {vector_a[0]}*{vector_b[0]} + {vector_a[1]}*{vector_b[1]} + {vector_a[2]}*{vector_b[2]} = {manual_dot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Magnitude (Length)\n",
    "print(\"\\nVector Magnitude:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "magnitude_a = np.linalg.norm(vector_a)\n",
    "magnitude_b = np.sqrt(np.sum(vector_b**2))  # Alternative calculation\n",
    "\n",
    "print(f\"Magnitude of a: {magnitude_a:.4f}\")\n",
    "print(f\"Magnitude of b: {magnitude_b:.4f}\")\n",
    "\n",
    "# Unit vector (normalized)\n",
    "unit_vector_a = vector_a / magnitude_a\n",
    "print(f\"\\nUnit vector of a: {unit_vector_a}\")\n",
    "print(f\"Magnitude of unit vector: {np.linalg.norm(unit_vector_a):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Matrices and Matrix Operations\n",
    "\n",
    "Matrices are fundamental in neural networks for representing weights, data batches, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 2: MATRICES AND MATRIX OPERATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create sample matrices\n",
    "matrix_A = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6]])\n",
    "\n",
    "matrix_B = np.array([[7, 8],\n",
    "                     [9, 10],\n",
    "                     [11, 12]])\n",
    "\n",
    "print(f\"Matrix A (2x3):\\n{matrix_A}\")\n",
    "print(f\"\\nMatrix B (3x2):\\n{matrix_B}\")\n",
    "print(f\"\\nShape of A: {matrix_A.shape}\")\n",
    "print(f\"Shape of B: {matrix_B.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication (most important operation in neural networks!)\n",
    "print(\"\\nMatrix Multiplication:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# A (2x3) Ã— B (3x2) = C (2x2)\n",
    "matrix_C = np.dot(matrix_A, matrix_B)\n",
    "# Alternative: matrix_C = matrix_A @ matrix_B\n",
    "\n",
    "print(f\"A Ã— B = \\n{matrix_C}\")\n",
    "print(f\"Shape of result: {matrix_C.shape}\")\n",
    "\n",
    "# Manual calculation for first element to understand\n",
    "first_element = matrix_A[0,0]*matrix_B[0,0] + matrix_A[0,1]*matrix_B[1,0] + matrix_A[0,2]*matrix_B[2,0]\n",
    "print(f\"\\nFirst element calculation: {matrix_A[0,0]}*{matrix_B[0,0]} + {matrix_A[0,1]}*{matrix_B[1,0]} + {matrix_A[0,2]}*{matrix_B[2,0]} = {first_element}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-Vector Multiplication (common in neural network forward pass)\n",
    "print(\"\\nMatrix-Vector Multiplication:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create a vector\n",
    "vector_x = np.array([1, 2, 3])\n",
    "print(f\"Vector x: {vector_x}\")\n",
    "\n",
    "# Multiply matrix A with vector x\n",
    "result_vector = np.dot(matrix_A, vector_x)\n",
    "print(f\"A Ã— x = {result_vector}\")\n",
    "print(f\"Shape: ({matrix_A.shape[0]},) - this is a {matrix_A.shape[0]}-dimensional vector\")\n",
    "\n",
    "# This is essentially what happens in a neural network layer!\n",
    "print(\"\\nðŸ’¡ This is essentially what happens in a neural network layer:\")\n",
    "print(\"   Weights (matrix) Ã— Input (vector) = Output (vector)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Transpose (very important for backpropagation)\n",
    "print(\"\\nMatrix Transpose:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "matrix_A_T = matrix_A.T\n",
    "print(f\"A transpose (3x2):\\n{matrix_A_T}\")\n",
    "print(f\"Shape: {matrix_A_T.shape}\")\n",
    "\n",
    "# Properties of transpose\n",
    "print(f\"\\nOriginal A shape: {matrix_A.shape}\")\n",
    "print(f\"Transposed A shape: {matrix_A_T.shape}\")\n",
    "print(\"Notice how rows become columns and vice versa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "print(\"\\nElement-wise Operations:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create two matrices of the same shape\n",
    "matrix_X = np.array([[1, 2], [3, 4]])\n",
    "matrix_Y = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(f\"Matrix X:\\n{matrix_X}\")\n",
    "print(f\"\\nMatrix Y:\\n{matrix_Y}\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "element_wise_mult = matrix_X * matrix_Y\n",
    "print(f\"\\nElement-wise multiplication X * Y:\\n{element_wise_mult}\")\n",
    "\n",
    "# Element-wise addition\n",
    "element_wise_add = matrix_X + matrix_Y\n",
    "print(f\"\\nElement-wise addition X + Y:\\n{element_wise_add}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Broadcasting in NumPy\n",
    "\n",
    "Broadcasting allows operations between arrays of different shapes, which is very useful in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 30)\n",
    "print(\"PART 3: BROADCASTING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Matrix + scalar (broadcasting)\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "scalar = 10\n",
    "\n",
    "result = matrix + scalar\n",
    "print(f\"Original matrix:\\n{matrix}\")\n",
    "print(f\"\\nMatrix + {scalar}:\\n{result}\")\n",
    "print(\"The scalar is 'broadcasted' to match the matrix shape!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix + vector (broadcasting)\n",
    "print(\"\\nMatrix + Vector Broadcasting:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "# Add a vector to each row\n",
    "vector = np.array([10, 20, 30])\n",
    "result = matrix + vector\n",
    "\n",
    "print(f\"Matrix (3x3):\\n{matrix}\")\n",
    "print(f\"\\nVector (3,): {vector}\")\n",
    "print(f\"\\nMatrix + Vector:\\n{result}\")\n",
    "print(\"\\nThe vector is added to each row of the matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column vector broadcasting (adding bias in neural networks)\n",
    "print(\"\\nColumn Vector Broadcasting (like adding bias):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Add a column vector to each column\n",
    "column_vector = np.array([[100], [200], [300]])\n",
    "result = matrix + column_vector\n",
    "\n",
    "print(f\"Matrix (3x3):\\n{matrix}\")\n",
    "print(f\"\\nColumn vector (3x1):\\n{column_vector}\")\n",
    "print(f\"\\nMatrix + Column Vector:\\n{result}\")\n",
    "print(\"\\nThe column vector is added to each column of the matrix!\")\n",
    "print(\"ðŸ’¡ This is how bias terms are added in neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Calculus Review - Derivatives\n",
    "\n",
    "Derivatives are essential for understanding how neural networks learn through backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 4: DERIVATIVES AND GRADIENTS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Visualize a simple function and its derivative\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x**2  # f(x) = xÂ²\n",
    "dy_dx = 2*x  # f'(x) = 2x\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = xÂ²')\n",
    "plt.plot(x, dy_dx, 'r--', linewidth=2, label=\"f'(x) = 2x\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function and its Derivative')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize slope at specific points\n",
    "plt.subplot(1, 2, 2)\n",
    "test_points = [-2, -1, 0, 1, 2]\n",
    "for point in test_points:\n",
    "    y_point = point**2\n",
    "    slope = 2*point\n",
    "    \n",
    "    # Plot tangent line\n",
    "    x_tangent = np.linspace(point-0.5, point+0.5, 10)\n",
    "    y_tangent = slope * (x_tangent - point) + y_point\n",
    "    \n",
    "    plt.plot(point, y_point, 'ro', markersize=8)\n",
    "    plt.plot(x_tangent, y_tangent, 'g-', alpha=0.7)\n",
    "    plt.text(point, y_point+0.5, f'slope={slope}', ha='center', fontsize=8)\n",
    "\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = xÂ²')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Tangent Lines (Slopes)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The derivative tells us the slope of the function at each point!\")\n",
    "print(\"In neural networks, we use derivatives to find the direction to adjust weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical derivative approximation\n",
    "print(\"\\nNumerical Derivative Approximation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Example function: f(x) = xÂ²\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def numerical_derivative(func, x, h=1e-7):\n",
    "    \"\"\"Compute numerical derivative using finite differences\"\"\"\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "# Test at various points\n",
    "test_points = [-2, -1, 0, 1, 2]\n",
    "print(\"Point\\tAnalytical\\tNumerical\\tError\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for x_test in test_points:\n",
    "    analytical = 2 * x_test  # True derivative of xÂ²\n",
    "    numerical = numerical_derivative(f, x_test)\n",
    "    error = abs(analytical - numerical)\n",
    "    \n",
    "    print(f\"{x_test}\\t{analytical:.6f}\\t{numerical:.6f}\\t{error:.2e}\")\n",
    "\n",
    "print(\"\\nNumerical derivatives are how computers compute gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule demonstration (crucial for backpropagation)\n",
    "print(\"\\nChain Rule Demonstration:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Composite function: h(x) = (xÂ² + 1)Â³\n",
    "# Let u = xÂ² + 1, so h(x) = uÂ³\n",
    "# dh/dx = dh/du Ã— du/dx = 3uÂ² Ã— 2x = 3(xÂ² + 1)Â² Ã— 2x\n",
    "\n",
    "def h(x):\n",
    "    return (x**2 + 1)**3\n",
    "\n",
    "def h_derivative(x):\n",
    "    return 3 * (x**2 + 1)**2 * 2 * x\n",
    "\n",
    "x_test = 2.0\n",
    "analytical = h_derivative(x_test)\n",
    "numerical = numerical_derivative(h, x_test)\n",
    "\n",
    "print(f\"Function: h(x) = (xÂ² + 1)Â³\")\n",
    "print(f\"At x = {x_test}:\")\n",
    "print(f\"  Analytical derivative: {analytical:.6f}\")\n",
    "print(f\"  Numerical derivative:  {numerical:.6f}\")\n",
    "print(f\"  Error: {abs(analytical - numerical):.2e}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ The chain rule is the foundation of backpropagation in neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Gradients and Multivariable Functions\n",
    "\n",
    "Neural networks work with functions of many variables, so we need to understand gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 5: GRADIENTS (MULTIVARIABLE)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple 2D function: f(x, y) = xÂ² + yÂ²\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return 2*x\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 2*y\n",
    "\n",
    "# Visualize the function\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = xÂ² + yÂ²')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axis('equal')\n",
    "\n",
    "# Gradient vectors\n",
    "ax3 = fig.add_subplot(133)\n",
    "x_arrows = np.arange(-2, 3, 1)\n",
    "y_arrows = np.arange(-2, 3, 1)\n",
    "X_arrows, Y_arrows = np.meshgrid(x_arrows, y_arrows)\n",
    "U = df_dx(X_arrows, Y_arrows)  # x-component of gradient\n",
    "V = df_dy(X_arrows, Y_arrows)  # y-component of gradient\n",
    "\n",
    "ax3.quiver(X_arrows, Y_arrows, U, V, scale=20, alpha=0.8, color='red')\n",
    "ax3.contour(X, Y, Z, levels=10, alpha=0.5)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('Gradient Vectors')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient vector points in the direction of steepest increase!\")\n",
    "print(\"In neural networks, we go in the opposite direction (gradient descent).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient computation for specific points\n",
    "print(\"\\nGradient Computation at Specific Points:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "test_points = [(1, 1), (2, -1), (-1, 2), (0, 0)]\n",
    "\n",
    "print(\"Point\\t\\tGradient\\t\\tMagnitude\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for x, y in test_points:\n",
    "    grad_x = df_dx(x, y)\n",
    "    grad_y = df_dy(x, y)\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    print(f\"({x:2}, {y:2})\\t\\t({grad_x:2}, {grad_y:2})\\t\\t{magnitude:.3f}\")\n",
    "\n",
    "print(\"\\nNote: At (0,0), the gradient is (0,0) - this is a minimum point!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Applications to Neural Networks\n",
    "\n",
    "Let's connect these mathematical concepts to neural networks with simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 6: NEURAL NETWORK CONNECTIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Simple neural network computation example\n",
    "print(\"Simple Neural Network Layer Example:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Input data (3 features, 4 data points)\n",
    "X = np.array([[1, 2, 3],      # Data point 1\n",
    "              [4, 5, 6],      # Data point 2  \n",
    "              [7, 8, 9],      # Data point 3\n",
    "              [2, 1, 4]])     # Data point 4\n",
    "\n",
    "print(f\"Input data X (4 samples, 3 features):\\n{X}\")\n",
    "\n",
    "# Weights (3 inputs, 2 neurons)\n",
    "W = np.array([[0.1, 0.4],    # Weights for feature 1\n",
    "              [0.2, 0.5],    # Weights for feature 2\n",
    "              [0.3, 0.6]])   # Weights for feature 3\n",
    "\n",
    "print(f\"\\nWeights W (3 features, 2 neurons):\\n{W}\")\n",
    "\n",
    "# Bias\n",
    "b = np.array([0.1, 0.2])\n",
    "print(f\"\\nBias b: {b}\")\n",
    "\n",
    "# Forward pass: z = XW + b\n",
    "z = np.dot(X, W) + b  # Broadcasting adds bias to each row\n",
    "print(f\"\\nLinear output z = XW + b (4 samples, 2 neurons):\\n{z}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ This is exactly what happens in a neural network layer!\")\n",
    "print(\"   - Matrix multiplication: XW\")\n",
    "print(\"   - Broadcasting: + b\")\n",
    "print(\"   - Next step would be applying activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function example (Mean Squared Error)\n",
    "print(\"\\nLoss Function Example (MSE):\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Predictions and true labels\n",
    "y_pred = np.array([0.8, 0.3, 0.9, 0.1])\n",
    "y_true = np.array([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"True labels: {y_true}\")\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = np.mean((y_pred - y_true)**2)\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Gradient of MSE with respect to predictions\n",
    "gradient = 2 * (y_pred - y_true) / len(y_pred)\n",
    "print(f\"\\nGradient âˆ‚MSE/âˆ‚y_pred: {gradient}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ This gradient tells us how to adjust our predictions!\")\n",
    "print(\"   Positive gradient â†’ decrease prediction\")\n",
    "print(\"   Negative gradient â†’ increase prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of gradient descent concept\n",
    "print(\"\\nGradient Descent Visualization:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Simple 1D function to minimize: f(w) = (w-2)Â² + 1\n",
    "def loss_function(w):\n",
    "    return (w - 2)**2 + 1\n",
    "\n",
    "def loss_gradient(w):\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "# Gradient descent simulation\n",
    "w = -1.0  # Starting point\n",
    "learning_rate = 0.3\n",
    "history = [w]\n",
    "loss_history = [loss_function(w)]\n",
    "\n",
    "for i in range(10):\n",
    "    grad = loss_gradient(w)\n",
    "    w = w - learning_rate * grad  # Gradient descent step\n",
    "    history.append(w)\n",
    "    loss_history.append(loss_function(w))\n",
    "\n",
    "# Plot the optimization process\n",
    "w_range = np.linspace(-2, 4, 100)\n",
    "loss_range = loss_function(w_range)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(w_range, loss_range, 'b-', linewidth=2, label='Loss function')\n",
    "plt.plot(history, [loss_function(w) for w in history], 'ro-', \n",
    "         markersize=6, label='Gradient descent path')\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gradient Descent on Loss Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history, 'g.-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Reduction Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final weight: {w:.4f} (optimal is 2.0)\")\n",
    "print(f\"Final loss: {loss_function(w):.6f} (minimum is 1.0)\")\n",
    "print(\"\\nðŸ’¡ This is how neural networks learn - by following gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checklist\n",
    "\n",
    "Mark each concept as understood:\n",
    "\n",
    "- [ ] Vector operations (addition, dot product, magnitude)\n",
    "- [ ] Matrix operations (multiplication, transpose)\n",
    "- [ ] Matrix-vector multiplication\n",
    "- [ ] Broadcasting in NumPy\n",
    "- [ ] Derivatives and their geometric meaning\n",
    "- [ ] Chain rule\n",
    "- [ ] Gradients for multivariable functions\n",
    "- [ ] Connection to neural network computations\n",
    "- [ ] Loss functions and their gradients\n",
    "- [ ] Gradient descent concept\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "1. **Vectors**: Represent data points, weights, and activations\n",
    "2. **Matrices**: Store weights and transform data in layers\n",
    "3. **Matrix Multiplication**: Core operation in neural network forward pass\n",
    "4. **Broadcasting**: Allows operations between different shaped arrays\n",
    "5. **Derivatives**: Measure rate of change, essential for learning\n",
    "6. **Gradients**: Show direction of steepest increase in multivariable functions\n",
    "7. **Chain Rule**: Enables backpropagation through network layers\n",
    "8. **Gradient Descent**: Algorithm for minimizing loss functions\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Matrix dimension mismatch:**\n",
    "- Always check shapes before multiplication\n",
    "- Remember: (m,n) Ã— (n,p) = (m,p)\n",
    "\n",
    "**2. Broadcasting errors:**\n",
    "- Understand which dimensions can be broadcasted\n",
    "- Use `.reshape()` to fix dimension issues\n",
    "\n",
    "**3. Numerical instability:**\n",
    "- Be careful with very large or small numbers\n",
    "- Use appropriate data types (float32 vs float64)\n",
    "\n",
    "**4. Plotting issues:**\n",
    "- Ensure matplotlib backend is properly configured\n",
    "- Try `%matplotlib inline` if plots don't appear\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll implement activation functions that transform the linear combinations we compute using these mathematical operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've reviewed the essential mathematical foundations for neural networks!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}