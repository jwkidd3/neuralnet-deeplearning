{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3: Activation Function Implementation\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the purpose and importance of activation functions in neural networks\n",
    "- Implement common activation functions from scratch\n",
    "- Analyze the mathematical properties of different activation functions\n",
    "- Visualize activation functions and their derivatives\n",
    "- Apply activation functions to transform neural network outputs\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1.1 (Environment Setup)\n",
    "- Completed Lab 1.2 (Mathematical Foundations)\n",
    "- Understanding of derivatives and function behavior\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Environment ready for activation function implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Need for Activation Functions\n",
    "\n",
    "Let's first understand why we need activation functions in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PART 1: WHY DO WE NEED ACTIVATION FUNCTIONS?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example: Linear transformations without activation functions\n",
    "print(\"Linear Network Example (WITHOUT activation functions):\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Input\n",
    "x = np.array([1, 2])\n",
    "print(f\"Input: {x}\")\n",
    "\n",
    "# First layer weights and computation\n",
    "W1 = np.array([[0.5, 0.3], [0.2, 0.8]])\n",
    "z1 = np.dot(x, W1)\n",
    "print(f\"\\nFirst layer: z1 = x @ W1 = {z1}\")\n",
    "\n",
    "# Second layer weights and computation\n",
    "W2 = np.array([[1.0, 0.5], [0.3, 1.2]])\n",
    "z2 = np.dot(z1, W2)\n",
    "print(f\"Second layer: z2 = z1 @ W2 = {z2}\")\n",
    "\n",
    "# This is equivalent to a single linear transformation!\n",
    "W_combined = np.dot(W1, W2)\n",
    "z_direct = np.dot(x, W_combined)\n",
    "print(f\"\\nDirect computation: x @ (W1 @ W2) = {z_direct}\")\n",
    "print(f\"Are they equal? {np.allclose(z2, z_direct)}\")\n",
    "\n",
    "print(\"\\n💡 Key Insight: Without activation functions, multiple layers collapse\")\n",
    "print(\"   into a single linear transformation! We need non-linearity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Basic Activation Functions\n",
    "\n",
    "Let's implement the most common activation functions from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 2: ACTIVATION FUNCTION IMPLEMENTATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Sigmoid (Logistic) Function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function: σ(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Input value(s) - can be scalar, vector, or matrix\n",
    "    \n",
    "    Returns:\n",
    "        Output in range (0, 1)\n",
    "    \"\"\"\n",
    "    # Clip z to prevent overflow\n",
    "    z = np.clip(z, -500, 500)  # Prevent numerical overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function: σ'(z) = σ(z) * (1 - σ(z))\n",
    "    \"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "print(\"✅ Sigmoid function implemented\")\n",
    "\n",
    "# 2. Hyperbolic Tangent (tanh) Function\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Input value(s)\n",
    "    \n",
    "    Returns:\n",
    "        Output in range (-1, 1)\n",
    "    \"\"\"\n",
    "    return np.tanh(z)  # NumPy has an optimized version\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of tanh function: tanh'(z) = 1 - tanh²(z)\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "print(\"✅ Tanh function implemented\")\n",
    "\n",
    "# 3. Rectified Linear Unit (ReLU) Function\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation function: ReLU(z) = max(0, z)\n",
    "    \n",
    "    Args:\n",
    "        z: Input value(s)\n",
    "    \n",
    "    Returns:\n",
    "        Output in range [0, +∞)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function: ReLU'(z) = 1 if z > 0, else 0\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"✅ ReLU function implemented\")\n",
    "\n",
    "# 4. Leaky ReLU Function\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation function: LeakyReLU(z) = max(αz, z)\n",
    "    \n",
    "    Args:\n",
    "        z: Input value(s)\n",
    "        alpha: Slope for negative values (default: 0.01)\n",
    "    \n",
    "    Returns:\n",
    "        Output allowing small negative values\n",
    "    \"\"\"\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def leaky_relu_derivative(z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Derivative of Leaky ReLU function\n",
    "    \"\"\"\n",
    "    return np.where(z > 0, 1, alpha)\n",
    "\n",
    "print(\"✅ Leaky ReLU function implemented\")\n",
    "\n",
    "# 5. Linear (Identity) Function\n",
    "def linear(z):\n",
    "    \"\"\"\n",
    "    Linear activation function: f(z) = z\n",
    "    Often used in output layers for regression\n",
    "    \"\"\"\n",
    "    return z\n",
    "\n",
    "def linear_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of linear function: f'(z) = 1\n",
    "    \"\"\"\n",
    "    return np.ones_like(z)\n",
    "\n",
    "print(\"✅ Linear function implemented\")\n",
    "print(\"\\nAll activation functions ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Activation Functions\n",
    "\n",
    "Let's test our implementations with various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 3: TESTING ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test inputs\n",
    "test_inputs = np.array([-5, -2, -1, 0, 1, 2, 5])\n",
    "print(f\"Test inputs: {test_inputs}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test each activation function\n",
    "functions = {\n",
    "    'Sigmoid': sigmoid,\n",
    "    'Tanh': tanh,\n",
    "    'ReLU': relu,\n",
    "    'Leaky ReLU': leaky_relu,\n",
    "    'Linear': linear\n",
    "}\n",
    "\n",
    "print(f\"{'Function':<12} | {'Input':<20} | {'Output':<30}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, func in functions.items():\n",
    "    outputs = func(test_inputs)\n",
    "    print(f\"{name:<12} | {str(test_inputs):<20} | {str(np.round(outputs, 4)):<30}\")\n",
    "\n",
    "print(\"\\n💡 Observations:\")\n",
    "print(\"   - Sigmoid: Outputs between 0 and 1\")\n",
    "print(\"   - Tanh: Outputs between -1 and 1, zero-centered\")\n",
    "print(\"   - ReLU: Zero for negative inputs, linear for positive\")\n",
    "print(\"   - Leaky ReLU: Small slope for negative inputs\")\n",
    "print(\"   - Linear: Outputs equal to inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Activation Functions\n",
    "\n",
    "Visual understanding is crucial for choosing the right activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 4: VISUALIZING ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create input range for plotting\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Activation Functions and Their Derivatives', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Function definitions for plotting\n",
    "activation_functions = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative, 'blue'),\n",
    "    ('Tanh', tanh, tanh_derivative, 'red'),\n",
    "    ('ReLU', relu, relu_derivative, 'green'),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_derivative, 'orange'),\n",
    "    ('Linear', linear, linear_derivative, 'purple')\n",
    "]\n",
    "\n",
    "# Plot each function\n",
    "for i, (name, func, deriv_func, color) in enumerate(activation_functions):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    if i < 5:  # We have 5 functions but 6 subplot positions\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Compute function values\n",
    "        y = func(x)\n",
    "        dy = deriv_func(x)\n",
    "        \n",
    "        # Plot function and derivative\n",
    "        ax.plot(x, y, color=color, linewidth=2, label=f'{name}')\n",
    "        ax.plot(x, dy, color=color, linewidth=2, linestyle='--', alpha=0.7, label=f\"{name}'\")\n",
    "        \n",
    "        ax.set_title(f'{name} Function')\n",
    "        ax.set_xlabel('Input (z)')\n",
    "        ax.set_ylabel('Output')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Remove the empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Key Visual Observations:\")\n",
    "print(\"   - Sigmoid: S-shaped curve, saturates at extremes\")\n",
    "print(\"   - Tanh: Similar to sigmoid but zero-centered\")\n",
    "print(\"   - ReLU: Sharp corner at zero, zero gradient for negative inputs\")\n",
    "print(\"   - Leaky ReLU: Small but non-zero gradient for negative inputs\")\n",
    "print(\"   - Linear: Constant slope, constant gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analyzing Activation Function Properties\n",
    "\n",
    "Let's analyze important properties like range, continuity, and gradient behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 5: ACTIVATION FUNCTION PROPERTIES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Analyze properties\n",
    "print(f\"{'Function':<12} | {'Range':<15} | {'Zero-Centered':<15} | {'Monotonic':<12} | {'Differentiable':<15}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "properties = [\n",
    "    ('Sigmoid', '(0, 1)', 'No', 'Yes', 'Yes'),\n",
    "    ('Tanh', '(-1, 1)', 'Yes', 'Yes', 'Yes'),\n",
    "    ('ReLU', '[0, +∞)', 'No', 'Yes', 'No (at 0)'),\n",
    "    ('Leaky ReLU', '(-∞, +∞)', 'No', 'Yes', 'No (at 0)'),\n",
    "    ('Linear', '(-∞, +∞)', 'Yes', 'Yes', 'Yes')\n",
    "]\n",
    "\n",
    "for name, range_val, zero_cent, monotonic, diff in properties:\n",
    "    print(f\"{name:<12} | {range_val:<15} | {zero_cent:<15} | {monotonic:<12} | {diff:<15}\")\n",
    "\n",
    "print(\"\\n🔍 Detailed Analysis:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient analysis - important for training\n",
    "print(\"\\nGradient Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Test gradient magnitudes at different points\n",
    "test_points = [-3, -1, 0, 1, 3]\n",
    "\n",
    "print(f\"{'Function':<12} | Input: {test_points}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "grad_functions = {\n",
    "    'Sigmoid': sigmoid_derivative,\n",
    "    'Tanh': tanh_derivative,\n",
    "    'ReLU': relu_derivative,\n",
    "    'Leaky ReLU': leaky_relu_derivative,\n",
    "    'Linear': linear_derivative\n",
    "}\n",
    "\n",
    "for name, grad_func in grad_functions.items():\n",
    "    gradients = grad_func(np.array(test_points))\n",
    "    grad_str = [f\"{g:.3f}\" for g in gradients]\n",
    "    print(f\"{name:<12} | Gradients: {grad_str}\")\n",
    "\n",
    "print(\"\\n⚠️  Gradient Problems:\")\n",
    "print(\"   - Sigmoid/Tanh: Vanishing gradients for large |z|\")\n",
    "print(\"   - ReLU: Dead neurons (zero gradient for z ≤ 0)\")\n",
    "print(\"   - Leaky ReLU: Helps with dead neuron problem\")\n",
    "print(\"   - Linear: No gradient issues, but no non-linearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturation analysis\n",
    "print(\"\\nSaturation Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Check how quickly functions saturate\n",
    "large_inputs = np.array([-10, -5, -2, 2, 5, 10])\n",
    "\n",
    "print(f\"{'Function':<12} | Input: {large_inputs}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, func in functions.items():\n",
    "    if name != 'Leaky ReLU':  # Skip to avoid parameter issues in loop\n",
    "        outputs = func(large_inputs)\n",
    "        out_str = [f\"{o:.3f}\" for o in outputs]\n",
    "        print(f\"{name:<12} | Outputs: {out_str}\")\n",
    "\n",
    "# Special case for Leaky ReLU\n",
    "outputs = leaky_relu(large_inputs)\n",
    "out_str = [f\"{o:.3f}\" for o in outputs]\n",
    "print(f\"{'Leaky ReLU':<12} | Outputs: {out_str}\")\n",
    "\n",
    "print(\"\\n📈 Saturation Observations:\")\n",
    "print(\"   - Sigmoid: Saturates quickly (≈0 for z<-5, ≈1 for z>5)\")\n",
    "print(\"   - Tanh: Saturates at ±1\")\n",
    "print(\"   - ReLU: No saturation for positive inputs\")\n",
    "print(\"   - Leaky ReLU: No saturation\")\n",
    "print(\"   - Linear: No saturation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Applications and Neural Network Integration\n",
    "\n",
    "Let's see how activation functions work in a simple neural network context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PART 6: ACTIVATION FUNCTIONS IN NEURAL NETWORKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate a simple neural network layer\n",
    "print(\"Simple Neural Network Layer Simulation:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Sample input data (batch of 3 samples, 4 features each)\n",
    "X = np.array([[1.0, 2.0, -1.0, 0.5],\n",
    "              [-0.5, 1.5, 2.0, -1.0],\n",
    "              [2.0, -1.0, 0.0, 1.5]])\n",
    "\n",
    "print(f\"Input data X (3 samples, 4 features):\\n{X}\")\n",
    "\n",
    "# Weights and bias for a layer with 3 neurons\n",
    "W = np.random.randn(4, 3) * 0.5  # Small random weights\n",
    "b = np.array([0.1, -0.2, 0.0])   # Small bias values\n",
    "\n",
    "print(f\"\\nWeights W (4 inputs, 3 neurons):\\n{W}\")\n",
    "print(f\"\\nBias b: {b}\")\n",
    "\n",
    "# Compute linear combination (before activation)\n",
    "Z = np.dot(X, W) + b\n",
    "print(f\"\\nLinear output Z = XW + b:\\n{Z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different activation functions\n",
    "print(\"\\nApplying Different Activation Functions:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "activations = {\n",
    "    'No activation (linear)': Z,\n",
    "    'Sigmoid': sigmoid(Z),\n",
    "    'Tanh': tanh(Z),\n",
    "    'ReLU': relu(Z),\n",
    "    'Leaky ReLU': leaky_relu(Z)\n",
    "}\n",
    "\n",
    "for name, activation in activations.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(activation)\n",
    "    print(f\"Range: [{np.min(activation):.3f}, {np.max(activation):.3f}]\")\n",
    "\n",
    "print(\"\\n💡 Notice how each activation function transforms the outputs differently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of different activation functions on network depth\n",
    "print(\"\\nDeep Network Simulation (5 layers):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Start with a simple input\n",
    "x = np.array([1.0, -0.5])\n",
    "print(f\"Initial input: {x}\")\n",
    "\n",
    "# Define weights for each layer (2->2->2->2->2->1)\n",
    "weights = [\n",
    "    np.array([[0.5, -0.3], [0.2, 0.8]]),  # Layer 1\n",
    "    np.array([[0.4, 0.6], [-0.1, 0.5]]),  # Layer 2  \n",
    "    np.array([[0.3, -0.4], [0.7, 0.2]]),  # Layer 3\n",
    "    np.array([[0.1, 0.9], [-0.6, 0.3]]),  # Layer 4\n",
    "    np.array([[0.8], [0.4]])               # Layer 5 (output)\n",
    "]\n",
    "\n",
    "# Test with different activation functions\n",
    "activation_funcs = {'ReLU': relu, 'Sigmoid': sigmoid, 'Tanh': tanh}\n",
    "\n",
    "for act_name, act_func in activation_funcs.items():\n",
    "    print(f\"\\n--- Using {act_name} activation ---\")\n",
    "    current_input = x.copy()\n",
    "    \n",
    "    for layer, W in enumerate(weights):\n",
    "        # Linear transformation\n",
    "        z = np.dot(current_input, W)\n",
    "        \n",
    "        # Apply activation (except for output layer)\n",
    "        if layer < len(weights) - 1:  # Hidden layers\n",
    "            current_input = act_func(z)\n",
    "            print(f\"  Layer {layer+1}: {current_input}\")\n",
    "        else:  # Output layer (no activation for this example)\n",
    "            output = z\n",
    "            print(f\"  Output: {output}\")\n",
    "\n",
    "print(\"\\n🧠 Observations:\")\n",
    "print(\"   - Different activation functions lead to different outputs\")\n",
    "print(\"   - ReLU can lead to some neurons 'dying' (outputting 0)\")\n",
    "print(\"   - Sigmoid/Tanh keep values in bounded ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Activation Functions (Bonus)\n",
    "\n",
    "Let's implement some modern activation functions used in current research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 7: ADVANCED ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Swish (SiLU) activation function\n",
    "def swish(z):\n",
    "    \"\"\"\n",
    "    Swish activation function: f(z) = z * sigmoid(z)\n",
    "    Also known as SiLU (Sigmoid Linear Unit)\n",
    "    \"\"\"\n",
    "    return z * sigmoid(z)\n",
    "\n",
    "def swish_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of Swish function\n",
    "    \"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s + z * s * (1 - s)\n",
    "\n",
    "# GELU activation function (approximation)\n",
    "def gelu(z):\n",
    "    \"\"\"\n",
    "    GELU activation function (Gaussian Error Linear Unit)\n",
    "    Approximation: GELU(z) ≈ 0.5 * z * (1 + tanh(√(2/π) * (z + 0.044715 * z³)))\n",
    "    \"\"\"\n",
    "    return 0.5 * z * (1 + np.tanh(np.sqrt(2 / np.pi) * (z + 0.044715 * z**3)))\n",
    "\n",
    "# ELU activation function\n",
    "def elu(z, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ELU activation function: f(z) = z if z > 0, else α(e^z - 1)\n",
    "    \"\"\"\n",
    "    return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "\n",
    "print(\"✅ Advanced activation functions implemented\")\n",
    "\n",
    "# Test advanced functions\n",
    "x_test = np.linspace(-3, 3, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot advanced activation functions\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_test, swish(x_test), 'b-', linewidth=2, label='Swish')\n",
    "plt.plot(x_test, swish_derivative(x_test), 'b--', linewidth=2, alpha=0.7, label=\"Swish'\")\n",
    "plt.title('Swish Activation')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_test, gelu(x_test), 'r-', linewidth=2, label='GELU')\n",
    "plt.title('GELU Activation')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x_test, elu(x_test), 'g-', linewidth=2, label='ELU')\n",
    "plt.title('ELU Activation')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔬 Advanced Function Properties:\")\n",
    "print(\"   - Swish: Smooth, non-monotonic, performs well in deep networks\")\n",
    "print(\"   - GELU: Smooth approximation of ReLU, used in transformers\")\n",
    "print(\"   - ELU: Smooth, has negative values, reduces bias shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Choosing the Right Activation Function\n",
    "\n",
    "Guidelines for selecting activation functions in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PART 8: ACTIVATION FUNCTION SELECTION GUIDE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "selection_guide = {\n",
    "    'Hidden Layers': {\n",
    "        'Default choice': 'ReLU - Fast, simple, works well in most cases',\n",
    "        'Deep networks': 'ReLU, Leaky ReLU, or ELU - Help with gradient flow',\n",
    "        'When ReLU fails': 'Leaky ReLU, ELU, or Swish - Avoid dead neurons',\n",
    "        'Research/Cutting-edge': 'GELU, Swish - Better performance in some cases'\n",
    "    },\n",
    "    'Output Layers': {\n",
    "        'Binary classification': 'Sigmoid - Outputs probability [0,1]',\n",
    "        'Multi-class classification': 'Softmax - Outputs probability distribution',\n",
    "        'Regression': 'Linear - No constraints on output range',\n",
    "        'Bounded regression': 'Sigmoid or Tanh - For bounded target values'\n",
    "    }\n",
    "}\n",
    "\n",
    "for layer_type, recommendations in selection_guide.items():\n",
    "    print(f\"\\n{layer_type}:\")\n",
    "    print(\"-\" * (len(layer_type) + 1))\n",
    "    for scenario, recommendation in recommendations.items():\n",
    "        print(f\"  {scenario}: {recommendation}\")\n",
    "\n",
    "print(\"\\n⚡ Performance Comparison (General Guidelines):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "performance_data = {\n",
    "    'Function': ['ReLU', 'Leaky ReLU', 'ELU', 'Sigmoid', 'Tanh', 'Swish', 'GELU'],\n",
    "    'Speed': ['Fast', 'Fast', 'Medium', 'Slow', 'Medium', 'Slow', 'Slow'],\n",
    "    'Gradient Flow': ['Good*', 'Good', 'Good', 'Poor', 'Poor', 'Good', 'Good'],\n",
    "    'Deep Networks': ['Good*', 'Good', 'Good', 'Poor', 'Poor', 'Excellent', 'Excellent']\n",
    "}\n",
    "\n",
    "print(f\"{'Function':<12} | {'Speed':<8} | {'Gradient Flow':<13} | {'Deep Networks':<13}\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(len(performance_data['Function'])):\n",
    "    func = performance_data['Function'][i]\n",
    "    speed = performance_data['Speed'][i]\n",
    "    grad = performance_data['Gradient Flow'][i]\n",
    "    deep = performance_data['Deep Networks'][i]\n",
    "    print(f\"{func:<12} | {speed:<8} | {grad:<13} | {deep:<13}\")\n",
    "\n",
    "print(\"\\n* Can suffer from dead neuron problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checklist\n",
    "\n",
    "Mark each concept as understood:\n",
    "\n",
    "- [ ] Purpose of activation functions in neural networks\n",
    "- [ ] Sigmoid function implementation and properties\n",
    "- [ ] Tanh function implementation and properties\n",
    "- [ ] ReLU function implementation and properties\n",
    "- [ ] Leaky ReLU function implementation and properties\n",
    "- [ ] Visualization of activation functions and derivatives\n",
    "- [ ] Understanding of gradient behavior\n",
    "- [ ] Saturation effects in different functions\n",
    "- [ ] Integration of activation functions in neural networks\n",
    "- [ ] Guidelines for choosing activation functions\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Numerical overflow in sigmoid:**\n",
    "- Solution: Clip input values to prevent exp() overflow\n",
    "- Use: `z = np.clip(z, -500, 500)`\n",
    "\n",
    "**2. NaN values in calculations:**\n",
    "- Check for division by zero\n",
    "- Ensure proper handling of edge cases (e.g., z=0 in derivatives)\n",
    "\n",
    "**3. Plotting issues:**\n",
    "- Ensure input ranges are appropriate for each function\n",
    "- Use sufficient resolution for smooth curves\n",
    "\n",
    "**4. Shape mismatches:**\n",
    "- Activation functions should preserve input shape\n",
    "- Check that derivative functions return same shape as input\n",
    "\n",
    "**5. Performance issues:**\n",
    "- For large arrays, consider using NumPy's built-in functions when available\n",
    "- Profile your code to identify bottlenecks\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "1. **Non-linearity**: Activation functions introduce non-linearity to neural networks\n",
    "2. **Gradient Flow**: Different functions affect how gradients flow during backpropagation\n",
    "3. **Saturation**: Some functions saturate (flat gradients) at extreme values\n",
    "4. **Dead Neurons**: ReLU can cause neurons to permanently output zero\n",
    "5. **Function Choice**: Selection depends on layer type, network depth, and problem domain\n",
    "6. **Computational Efficiency**: Simpler functions (ReLU) are faster than complex ones (Sigmoid)\n",
    "7. **Range and Centering**: Output ranges and zero-centering affect network behavior\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll implement a basic neuron that combines the mathematical operations from Lab 1.2 with the activation functions from this lab to create a complete computational unit.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've successfully implemented and analyzed activation functions for neural networks!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
