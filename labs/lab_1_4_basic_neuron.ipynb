{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4: Basic Neuron Implementation\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the structure and components of an artificial neuron\n",
    "- Implement a complete neuron from scratch using NumPy\n",
    "- Combine linear transformations with activation functions\n",
    "- Test neurons with different inputs and configurations\n",
    "- Build the foundation for neural network layers\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1.1 (Environment Setup)\n",
    "- Completed Lab 1.2 (Mathematical Foundations)\n",
    "- Completed Lab 1.3 (Activation Functions)\n",
    "- Understanding of matrix operations and activation functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Environment ready for neuron implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Artificial Neuron\n",
    "\n",
    "Let's start by understanding the mathematical model of an artificial neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 1: ANATOMY OF AN ARTIFICIAL NEURON\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\"\"\n",
    "🧠 Artificial Neuron Components:\n",
    "\n",
    "1. INPUTS (x₁, x₂, ..., xₙ):\n",
    "   - Features from previous layer or raw data\n",
    "   - Can be any real numbers\n",
    "\n",
    "2. WEIGHTS (w₁, w₂, ..., wₙ):\n",
    "   - Learnable parameters\n",
    "   - Determine the importance of each input\n",
    "   - Adjusted during training\n",
    "\n",
    "3. BIAS (b):\n",
    "   - Additional learnable parameter\n",
    "   - Shifts the activation function\n",
    "   - Allows neuron to fire even with zero inputs\n",
    "\n",
    "4. LINEAR COMBINATION (z):\n",
    "   - z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "   - z = Σ(wᵢxᵢ) + b = w·x + b\n",
    "\n",
    "5. ACTIVATION FUNCTION (f):\n",
    "   - Applies non-linearity: a = f(z)\n",
    "   - Common choices: sigmoid, tanh, ReLU\n",
    "\n",
    "6. OUTPUT (a):\n",
    "   - Final neuron output: a = f(w·x + b)\n",
    "   - Becomes input to next layer\n",
    "\"\"\")\n",
    "\n",
    "# Visual representation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Draw neuron diagram\n",
    "# Input nodes\n",
    "input_positions = [(1, 4), (1, 3), (1, 2), (1, 1)]\n",
    "input_labels = ['x₁', 'x₂', 'x₃', 'x₄']\n",
    "\n",
    "# Draw inputs\n",
    "for i, ((x, y), label) in enumerate(zip(input_positions, input_labels)):\n",
    "    circle = plt.Circle((x, y), 0.2, color='lightblue', ec='black')\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw neuron\n",
    "neuron_pos = (4, 2.5)\n",
    "neuron_circle = plt.Circle(neuron_pos, 0.5, color='lightgreen', ec='black', linewidth=2)\n",
    "ax.add_patch(neuron_circle)\n",
    "ax.text(neuron_pos[0], neuron_pos[1], 'Σ', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw connections with weights\n",
    "weights = ['w₁', 'w₂', 'w₃', 'w₄']\n",
    "for i, ((x1, y1), weight) in enumerate(zip(input_positions, weights)):\n",
    "    x2, y2 = neuron_pos\n",
    "    # Draw line\n",
    "    ax.plot([x1 + 0.2, x2 - 0.5], [y1, y2], 'k-', linewidth=2)\n",
    "    # Add weight label\n",
    "    mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    ax.text(mid_x, mid_y + 0.1, weight, ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Draw bias\n",
    "ax.text(neuron_pos[0], neuron_pos[1] - 0.8, '+b', ha='center', va='center', \n",
    "        fontsize=12, bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Draw activation function\n",
    "act_pos = (6.5, 2.5)\n",
    "ax.text(act_pos[0], act_pos[1], 'f(z)', ha='center', va='center', \n",
    "        fontsize=14, bbox=dict(boxstyle='round,pad=0.4', facecolor='orange', alpha=0.8))\n",
    "\n",
    "# Draw connection to activation\n",
    "ax.plot([neuron_pos[0] + 0.5, act_pos[0] - 0.5], [neuron_pos[1], act_pos[1]], 'k-', linewidth=2)\n",
    "ax.text((neuron_pos[0] + act_pos[0]) / 2, neuron_pos[1] + 0.2, 'z', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Draw output\n",
    "output_pos = (8.5, 2.5)\n",
    "output_circle = plt.Circle(output_pos, 0.2, color='lightcoral', ec='black')\n",
    "ax.add_patch(output_circle)\n",
    "ax.text(output_pos[0], output_pos[1], 'a', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw final connection\n",
    "ax.plot([act_pos[0] + 0.5, output_pos[0] - 0.2], [act_pos[1], output_pos[1]], 'k-', linewidth=2)\n",
    "\n",
    "# Add equation\n",
    "ax.text(4.5, 0.5, 'z = w₁x₁ + w₂x₂ + w₃x₃ + w₄x₄ + b', ha='center', fontsize=12, \n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "ax.text(4.5, 0, 'a = f(z)', ha='center', fontsize=12,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "ax.set_xlim(0, 9)\n",
    "ax.set_ylim(-0.5, 4.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Artificial Neuron Structure', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 This is the fundamental building block of neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a Basic Neuron Class\n",
    "\n",
    "Let's implement our first neuron from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 2: BASIC NEURON IMPLEMENTATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# First, let's import activation functions from our previous lab\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def linear(z):\n",
    "    \"\"\"Linear activation function\"\"\"\n",
    "    return z\n",
    "\n",
    "# Define activation function derivatives\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def linear_derivative(z):\n",
    "    return np.ones_like(z)\n",
    "\n",
    "print(\"Activation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A single artificial neuron implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, activation='sigmoid', learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize neuron with random weights and zero bias\n",
    "        \n",
    "        Args:\n",
    "            n_inputs (int): Number of input features\n",
    "            activation (str): Activation function ('sigmoid', 'tanh', 'relu', 'linear')\n",
    "            learning_rate (float): Learning rate for weight updates\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.weights = np.random.randn(n_inputs) * 0.1\n",
    "        \n",
    "        # Initialize bias to zero\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activation_name = activation\n",
    "        self._set_activation_function(activation)\n",
    "        \n",
    "        # Store computation history for analysis\n",
    "        self.last_input = None\n",
    "        self.last_z = None\n",
    "        self.last_output = None\n",
    "    \n",
    "    def _set_activation_function(self, activation):\n",
    "        \"\"\"Set the activation function and its derivative\"\"\"\n",
    "        activation_functions = {\n",
    "            'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "            'tanh': (tanh, tanh_derivative),\n",
    "            'relu': (relu, relu_derivative),\n",
    "            'linear': (linear, linear_derivative)\n",
    "        }\n",
    "        \n",
    "        if activation not in activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.activation_func, self.activation_derivative = activation_functions[activation]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: compute neuron output\n",
    "        \n",
    "        Args:\n",
    "            inputs (np.array): Input features\n",
    "        \n",
    "        Returns:\n",
    "            float: Neuron output\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy array if needed\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        # Check input dimensions\n",
    "        if len(inputs) != self.n_inputs:\n",
    "            raise ValueError(f\"Expected {self.n_inputs} inputs, got {len(inputs)}\")\n",
    "        \n",
    "        # Store input for later use\n",
    "        self.last_input = inputs.copy()\n",
    "        \n",
    "        # Compute linear combination: z = w·x + b\n",
    "        self.last_z = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # Apply activation function: a = f(z)\n",
    "        self.last_output = self.activation_func(self.last_z)\n",
    "        \n",
    "        return self.last_output\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return current weights and bias\"\"\"\n",
    "        return {\n",
    "            'weights': self.weights.copy(),\n",
    "            'bias': self.bias,\n",
    "            'activation': self.activation_name\n",
    "        }\n",
    "    \n",
    "    def set_parameters(self, weights, bias):\n",
    "        \"\"\"Set weights and bias manually\"\"\"\n",
    "        if len(weights) != self.n_inputs:\n",
    "            raise ValueError(f\"Expected {self.n_inputs} weights, got {len(weights)}\")\n",
    "        \n",
    "        self.weights = np.array(weights).copy()\n",
    "        self.bias = bias\n",
    "    \n",
    "    def get_computation_details(self):\n",
    "        \"\"\"Return details of last computation\"\"\"\n",
    "        if self.last_input is None:\n",
    "            return \"No computation performed yet\"\n",
    "        \n",
    "        return {\n",
    "            'input': self.last_input,\n",
    "            'weights': self.weights,\n",
    "            'bias': self.bias,\n",
    "            'linear_output': self.last_z,\n",
    "            'activation_output': self.last_output,\n",
    "            'activation_function': self.activation_name\n",
    "        }\n",
    "\n",
    "print(\"✅ Neuron class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Our Neuron Implementation\n",
    "\n",
    "Let's create and test neurons with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 3: TESTING NEURON IMPLEMENTATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test 1: Simple 2-input neuron with sigmoid activation\n",
    "print(\"\\nTest 1: 2-input Sigmoid Neuron\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create neuron\n",
    "neuron1 = Neuron(n_inputs=2, activation='sigmoid')\n",
    "\n",
    "# Set specific weights and bias for predictable results\n",
    "neuron1.set_parameters(weights=[0.5, 0.3], bias=0.1)\n",
    "\n",
    "print(f\"Neuron parameters: {neuron1.get_parameters()}\")\n",
    "\n",
    "# Test with different inputs\n",
    "test_inputs = [\n",
    "    [1.0, 2.0],\n",
    "    [0.0, 0.0],\n",
    "    [-1.0, 1.0],\n",
    "    [2.0, -1.0]\n",
    "]\n",
    "\n",
    "print(\"\\nInput\\t\\t-> Output\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for inputs in test_inputs:\n",
    "    output = neuron1.forward(inputs)\n",
    "    print(f\"{inputs}\\t-> {output:.4f}\")\n",
    "    \n",
    "    # Show computation details for first input\n",
    "    if inputs == test_inputs[0]:\n",
    "        details = neuron1.get_computation_details()\n",
    "        print(f\"  Details: z = {details['linear_output']:.4f}, σ(z) = {details['activation_output']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Compare different activation functions\n",
    "print(\"\\nTest 2: Different Activation Functions\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create neurons with different activations\n",
    "activations = ['sigmoid', 'tanh', 'relu', 'linear']\n",
    "neurons = {}\n",
    "\n",
    "# Same input for all neurons\n",
    "test_input = [1.5, -0.8, 0.3]\n",
    "weights = [0.4, -0.2, 0.6]\n",
    "bias = 0.1\n",
    "\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(\"\\nActivation\\tLinear Output (z)\\tFinal Output (a)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for activation in activations:\n",
    "    # Create neuron\n",
    "    neuron = Neuron(n_inputs=3, activation=activation)\n",
    "    neuron.set_parameters(weights=weights, bias=bias)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = neuron.forward(test_input)\n",
    "    \n",
    "    # Get details\n",
    "    details = neuron.get_computation_details()\n",
    "    \n",
    "    print(f\"{activation:<10}\\t{details['linear_output']:.4f}\\t\\t{details['activation_output']:.4f}\")\n",
    "    \n",
    "    neurons[activation] = neuron\n",
    "\n",
    "print(\"\\n💡 Notice how the same linear output produces different final outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Neuron behavior with varying inputs\n",
    "print(\"\\nTest 3: Neuron Response to Input Variations\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create a neuron for testing\n",
    "test_neuron = Neuron(n_inputs=1, activation='sigmoid')\n",
    "test_neuron.set_parameters(weights=[1.0], bias=0.0)\n",
    "\n",
    "# Generate range of inputs\n",
    "input_range = np.linspace(-5, 5, 21)\n",
    "outputs = []\n",
    "\n",
    "print(\"Input\\tOutput\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "for x in input_range:\n",
    "    output = test_neuron.forward([x])\n",
    "    outputs.append(output)\n",
    "    if x in [-5, -2, 0, 2, 5]:  # Print only some values\n",
    "        print(f\"{x:.1f}\\t{output:.4f}\")\n",
    "\n",
    "# Plot the neuron's response\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(input_range, outputs, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Input Value')\n",
    "plt.ylabel('Neuron Output')\n",
    "plt.title('Neuron Response Curve (Sigmoid Activation)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 This S-shaped curve is characteristic of sigmoid neurons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Neuron Decision Boundaries\n",
    "\n",
    "Let's visualize how neurons make decisions with 2D inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 4: NEURON DECISION BOUNDARIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a 2-input neuron for classification\n",
    "classifier_neuron = Neuron(n_inputs=2, activation='sigmoid')\n",
    "\n",
    "# Set weights that create an interesting decision boundary\n",
    "classifier_neuron.set_parameters(weights=[1.0, -0.5], bias=-0.2)\n",
    "\n",
    "print(f\"Classifier neuron parameters: {classifier_neuron.get_parameters()}\")\n",
    "print(\"Decision boundary: points where output = 0.5 (z = 0)\")\n",
    "print(\"Equation: 1.0*x₁ + (-0.5)*x₂ + (-0.2) = 0\")\n",
    "print(\"Simplified: x₂ = 2*x₁ - 0.4\")\n",
    "\n",
    "# Create a grid of points\n",
    "x1_range = np.linspace(-2, 3, 100)\n",
    "x2_range = np.linspace(-2, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Compute neuron output for each point\n",
    "Z = np.zeros_like(X1)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[1]):\n",
    "        Z[i, j] = classifier_neuron.forward([X1[i, j], X2[i, j]])\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Contour plot showing decision regions\n",
    "contour = ax1.contourf(X1, X2, Z, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "decision_boundary = ax1.contour(X1, X2, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "ax1.clabel(decision_boundary, inline=True, fontsize=12, fmt='Decision\\nBoundary')\n",
    "\n",
    "# Add some test points\n",
    "test_points = np.array([[0, 0], [1, 1], [2, 0], [0, 2], [-1, 1]])\n",
    "for point in test_points:\n",
    "    output = classifier_neuron.forward(point)\n",
    "    color = 'red' if output > 0.5 else 'blue'\n",
    "    ax1.plot(point[0], point[1], 'o', color=color, markersize=10, \n",
    "            markeredgecolor='black', markeredgewidth=2)\n",
    "    ax1.text(point[0]+0.1, point[1]+0.1, f'{output:.2f}', fontsize=10, \n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax1.set_xlabel('Input x₁')\n",
    "ax1.set_ylabel('Input x₂')\n",
    "ax1.set_title('Neuron Decision Boundary')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(contour, ax=ax1, label='Neuron Output')\n",
    "\n",
    "# Plot 2: 3D surface showing neuron output\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "surface = ax2.plot_surface(X1, X2, Z, cmap='RdYlBu', alpha=0.8)\n",
    "ax2.set_xlabel('Input x₁')\n",
    "ax2.set_ylabel('Input x₂')\n",
    "ax2.set_zlabel('Neuron Output')\n",
    "ax2.set_title('Neuron Output Surface')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Key Observations:\")\n",
    "print(\"   - Red points (output > 0.5): Classified as 'positive'\")\n",
    "print(\"   - Blue points (output < 0.5): Classified as 'negative'\")\n",
    "print(\"   - Black line: Decision boundary where output = 0.5\")\n",
    "print(\"   - Linear decision boundary is a fundamental limitation of single neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multiple Neurons Working Together\n",
    "\n",
    "Let's see what happens when we combine multiple neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 5: MULTIPLE NEURONS (SIMPLE LAYER)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a simple layer with 3 neurons\n",
    "class SimpleLayer:\n",
    "    \"\"\"A simple layer containing multiple neurons\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, activation='sigmoid'):\n",
    "        self.neurons = []\n",
    "        for _ in range(n_neurons):\n",
    "            neuron = Neuron(n_inputs, activation)\n",
    "            self.neurons.append(neuron)\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass through all neurons\"\"\"\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            output = neuron.forward(inputs)\n",
    "            outputs.append(output)\n",
    "        return np.array(outputs)\n",
    "    \n",
    "    def get_all_parameters(self):\n",
    "        \"\"\"Get parameters of all neurons\"\"\"\n",
    "        params = []\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            params.append(f\"Neuron {i+1}: {neuron.get_parameters()}\")\n",
    "        return params\n",
    "\n",
    "# Create a layer with 3 neurons\n",
    "layer = SimpleLayer(n_inputs=2, n_neurons=3, activation='sigmoid')\n",
    "\n",
    "# Set different parameters for each neuron\n",
    "layer.neurons[0].set_parameters(weights=[1.0, 0.5], bias=0.0)   # Neuron 1\n",
    "layer.neurons[1].set_parameters(weights=[0.3, -0.8], bias=0.2)  # Neuron 2\n",
    "layer.neurons[2].set_parameters(weights=[-0.6, 0.4], bias=-0.1) # Neuron 3\n",
    "\n",
    "print(\"Layer Configuration:\")\n",
    "for param in layer.get_all_parameters():\n",
    "    print(f\"  {param}\")\n",
    "\n",
    "# Test the layer with different inputs\n",
    "test_inputs = [\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0], \n",
    "    [1.0, 1.0],\n",
    "    [-1.0, 0.5]\n",
    "]\n",
    "\n",
    "print(\"\\nLayer Testing:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Input':<15} | {'Neuron 1':<10} | {'Neuron 2':<10} | {'Neuron 3':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for inputs in test_inputs:\n",
    "    outputs = layer.forward(inputs)\n",
    "    print(f\"{str(inputs):<15} | {outputs[0]:.4f}     | {outputs[1]:.4f}     | {outputs[2]:.4f}\")\n",
    "\n",
    "print(\"\\n💡 Each neuron responds differently to the same input!\")\n",
    "print(\"   This creates a richer representation than a single neuron.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how multiple neurons create different decision boundaries\n",
    "print(\"\\nVisualizing Multiple Decision Boundaries:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create grid for visualization\n",
    "x1_range = np.linspace(-2, 2, 100)\n",
    "x2_range = np.linspace(-2, 2, 100)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Compute outputs for each neuron\n",
    "Z1 = np.zeros_like(X1)\n",
    "Z2 = np.zeros_like(X1)\n",
    "Z3 = np.zeros_like(X1)\n",
    "\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[1]):\n",
    "        point = [X1[i, j], X2[i, j]]\n",
    "        Z1[i, j] = layer.neurons[0].forward(point)\n",
    "        Z2[i, j] = layer.neurons[1].forward(point)\n",
    "        Z3[i, j] = layer.neurons[2].forward(point)\n",
    "\n",
    "# Plot all three decision boundaries\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Neuron 1\n",
    "im1 = axes[0].contourf(X1, X2, Z1, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "axes[0].contour(X1, X2, Z1, levels=[0.5], colors='black', linewidths=3)\n",
    "axes[0].set_title('Neuron 1 Decision Boundary')\n",
    "axes[0].set_xlabel('Input x₁')\n",
    "axes[0].set_ylabel('Input x₂')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Neuron 2\n",
    "im2 = axes[1].contourf(X1, X2, Z2, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "axes[1].contour(X1, X2, Z2, levels=[0.5], colors='black', linewidths=3)\n",
    "axes[1].set_title('Neuron 2 Decision Boundary')\n",
    "axes[1].set_xlabel('Input x₁')\n",
    "axes[1].set_ylabel('Input x₂')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Neuron 3\n",
    "im3 = axes[2].contourf(X1, X2, Z3, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "axes[2].contour(X1, X2, Z3, levels=[0.5], colors='black', linewidths=3)\n",
    "axes[2].set_title('Neuron 3 Decision Boundary')\n",
    "axes[2].set_xlabel('Input x₁')\n",
    "axes[2].set_ylabel('Input x₂')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🧠 Notice how each neuron learns a different linear decision boundary!\")\n",
    "print(\"   When combined, these can approximate more complex decision regions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Example - Simple Pattern Recognition\n",
    "\n",
    "Let's use our neurons to solve a simple pattern recognition problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PART 6: PATTERN RECOGNITION WITH NEURONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Problem: Classify points based on their position relative to a circle\n",
    "# Inside circle = class 0, Outside circle = class 1\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate random points\n",
    "X = np.random.uniform(-2, 2, (n_samples, 2))\n",
    "\n",
    "# Create labels based on distance from origin (circle with radius 1.2)\n",
    "distances = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "y = (distances > 1.2).astype(int)  # 1 if outside circle, 0 if inside\n",
    "\n",
    "print(f\"Generated {n_samples} data points\")\n",
    "print(f\"Class 0 (inside circle): {np.sum(y == 0)} points\")\n",
    "print(f\"Class 1 (outside circle): {np.sum(y == 1)} points\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Data visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['blue', 'red']\n",
    "for class_idx in [0, 1]:\n",
    "    class_points = X[y == class_idx]\n",
    "    plt.scatter(class_points[:, 0], class_points[:, 1], \n",
    "               c=colors[class_idx], label=f'Class {class_idx}', alpha=0.7)\n",
    "\n",
    "# Draw the true decision boundary (circle)\n",
    "circle = plt.Circle((0, 0), 1.2, fill=False, color='black', linewidth=2, linestyle='--')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.xlabel('x₁')\n",
    "plt.ylabel('x₂')\n",
    "plt.title('Data Distribution\\n(True boundary: circle with radius 1.2)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Now let's try to solve this with a single neuron\n",
    "print(\"\\nTesting Single Neuron Performance:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a neuron for classification\n",
    "pattern_neuron = Neuron(n_inputs=2, activation='sigmoid')\n",
    "\n",
    "# Try different weight configurations manually\n",
    "configurations = [\n",
    "    {'weights': [1.0, 0.0], 'bias': 0.0, 'name': 'Vertical boundary'},\n",
    "    {'weights': [0.0, 1.0], 'bias': 0.0, 'name': 'Horizontal boundary'},\n",
    "    {'weights': [1.0, 1.0], 'bias': -1.0, 'name': 'Diagonal boundary'},\n",
    "    {'weights': [0.8, 0.6], 'bias': -1.2, 'name': 'Custom boundary'}\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_config = None\n",
    "\n",
    "for config in configurations:\n",
    "    pattern_neuron.set_parameters(config['weights'], config['bias'])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "    for point in X:\n",
    "        output = pattern_neuron.forward(point)\n",
    "        prediction = 1 if output > 0.5 else 0\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    print(f\"{config['name']:<20}: Accuracy = {accuracy:.3f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_config = config\n",
    "\n",
    "print(f\"\\nBest configuration: {best_config['name']} (Accuracy: {best_accuracy:.3f})\")\n",
    "\n",
    "# Visualize the best neuron's decision boundary\n",
    "pattern_neuron.set_parameters(best_config['weights'], best_config['bias'])\n",
    "\n",
    "# Create prediction grid\n",
    "x1_range = np.linspace(-2.5, 2.5, 100)\n",
    "x2_range = np.linspace(-2.5, 2.5, 100)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "Z_grid = np.zeros_like(X1_grid)\n",
    "\n",
    "for i in range(X1_grid.shape[0]):\n",
    "    for j in range(X1_grid.shape[1]):\n",
    "        Z_grid[i, j] = pattern_neuron.forward([X1_grid[i, j], X2_grid[i, j]])\n",
    "\n",
    "# Plot 2: Neuron decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(X1_grid, X2_grid, Z_grid, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "plt.contour(X1_grid, X2_grid, Z_grid, levels=[0.5], colors='black', linewidths=3)\n",
    "\n",
    "# Plot data points\n",
    "for class_idx in [0, 1]:\n",
    "    class_points = X[y == class_idx]\n",
    "    plt.scatter(class_points[:, 0], class_points[:, 1], \n",
    "               c=colors[class_idx], label=f'Class {class_idx}', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Draw true boundary\n",
    "circle = plt.Circle((0, 0), 1.2, fill=False, color='white', linewidth=2, linestyle='--')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.xlabel('x₁')\n",
    "plt.ylabel('x₂')\n",
    "plt.title(f'Single Neuron Classification\\n{best_config[\"name\"]} (Acc: {best_accuracy:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🤔 Limitation Observed:\")\n",
    "print(\"   Single neurons can only create LINEAR decision boundaries!\")\n",
    "print(\"   The true boundary is circular (non-linear), so accuracy is limited.\")\n",
    "print(\"   This is why we need multiple layers and neurons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance Analysis and Optimization\n",
    "\n",
    "Let's analyze the computational performance of our neuron implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 7: PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import time\n",
    "\n",
    "# Test computational performance\n",
    "def benchmark_neuron(n_inputs, n_tests=10000):\n",
    "    \"\"\"Benchmark neuron forward pass performance\"\"\"\n",
    "    \n",
    "    neuron = Neuron(n_inputs=n_inputs, activation='sigmoid')\n",
    "    test_input = np.random.randn(n_inputs)\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(100):\n",
    "        neuron.forward(test_input)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_tests):\n",
    "        neuron.forward(test_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    time_per_forward = total_time / n_tests\n",
    "    \n",
    "    return total_time, time_per_forward\n",
    "\n",
    "# Test different input sizes\n",
    "input_sizes = [2, 10, 50, 100, 500]\n",
    "n_tests = 10000\n",
    "\n",
    "print(f\"Benchmarking {n_tests} forward passes:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Inputs':<8} | {'Total Time':<12} | {'Time/Forward':<15} | {'Forwards/sec':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in input_sizes:\n",
    "    total_time, time_per_forward = benchmark_neuron(size, n_tests)\n",
    "    forwards_per_sec = 1 / time_per_forward\n",
    "    \n",
    "    print(f\"{size:<8} | {total_time:<12.4f} | {time_per_forward:<15.6f} | {forwards_per_sec:<12.0f}\")\n",
    "\n",
    "print(\"\\n⚡ Performance Insights:\")\n",
    "print(\"   - Forward pass time scales linearly with number of inputs\")\n",
    "print(\"   - Modern GPUs can handle millions of neurons in parallel\")\n",
    "print(\"   - Vectorization (next labs) will improve performance significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage analysis\n",
    "print(\"\\nMemory Usage Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def analyze_neuron_memory(n_inputs):\n",
    "    \"\"\"Analyze memory usage of a neuron\"\"\"\n",
    "    \n",
    "    neuron = Neuron(n_inputs=n_inputs)\n",
    "    \n",
    "    # Calculate parameter memory\n",
    "    weights_memory = n_inputs * 8  # 8 bytes per float64\n",
    "    bias_memory = 8  # 8 bytes for bias\n",
    "    total_params_memory = weights_memory + bias_memory\n",
    "    \n",
    "    # Calculate activation memory (for storing intermediate results)\n",
    "    activation_memory = 8 * 3  # input, z, output\n",
    "    \n",
    "    total_memory = total_params_memory + activation_memory\n",
    "    \n",
    "    return {\n",
    "        'weights_memory': weights_memory,\n",
    "        'bias_memory': bias_memory,\n",
    "        'activation_memory': activation_memory,\n",
    "        'total_memory': total_memory,\n",
    "        'parameters': n_inputs + 1  # weights + bias\n",
    "    }\n",
    "\n",
    "print(f\"{'Inputs':<8} | {'Parameters':<12} | {'Memory (bytes)':<15} | {'Memory (KB)':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for size in input_sizes:\n",
    "    memory_info = analyze_neuron_memory(size)\n",
    "    memory_kb = memory_info['total_memory'] / 1024\n",
    "    \n",
    "    print(f\"{size:<8} | {memory_info['parameters']:<12} | {memory_info['total_memory']:<15} | {memory_kb:<12.3f}\")\n",
    "\n",
    "print(\"\\n💾 Memory Observations:\")\n",
    "print(\"   - Memory usage dominated by weight storage\")\n",
    "print(\"   - Each parameter requires 8 bytes (float64)\")\n",
    "print(\"   - Large networks require careful memory management\")\n",
    "\n",
    "# Estimate memory for a realistic network\n",
    "print(\"\\n🧠 Realistic Network Example:\")\n",
    "layers = [784, 256, 128, 64, 10]  # MNIST-like architecture\n",
    "total_params = 0\n",
    "total_memory = 0\n",
    "\n",
    "print(\"Layer structure: Input(784) -> Hidden(256) -> Hidden(128) -> Hidden(64) -> Output(10)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(layers)-1):\n",
    "    layer_params = layers[i] * layers[i+1] + layers[i+1]  # weights + biases\n",
    "    layer_memory = layer_params * 8  # bytes\n",
    "    total_params += layer_params\n",
    "    total_memory += layer_memory\n",
    "    \n",
    "    print(f\"Layer {i+1}: {layers[i]:>3} -> {layers[i+1]:>3} | {layer_params:>8} params | {layer_memory/1024:>8.1f} KB\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Total memory: {total_memory/1024:.1f} KB = {total_memory/(1024*1024):.2f} MB\")\n",
    "print(\"This is just for storing parameters - actual training requires much more memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checklist\n",
    "\n",
    "Mark each concept as understood:\n",
    "\n",
    "- [ ] Structure of an artificial neuron (inputs, weights, bias, activation)\n",
    "- [ ] Implementation of Neuron class with forward pass\n",
    "- [ ] Testing neurons with different activation functions\n",
    "- [ ] Understanding decision boundaries created by single neurons\n",
    "- [ ] Combining multiple neurons in a simple layer\n",
    "- [ ] Limitations of linear decision boundaries\n",
    "- [ ] Pattern recognition example with real data\n",
    "- [ ] Performance analysis and computational considerations\n",
    "- [ ] Memory usage and scalability insights\n",
    "- [ ] Foundation for building neural network layers\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Dimension mismatch errors:**\n",
    "- Check that input size matches neuron's expected input dimensions\n",
    "- Verify weight vector length equals number of inputs\n",
    "\n",
    "**2. Numerical overflow in activation functions:**\n",
    "- Ensure activation functions handle extreme values properly\n",
    "- Consider gradient clipping for very large weights\n",
    "\n",
    "**3. Poor classification performance:**\n",
    "- Remember single neurons can only create linear boundaries\n",
    "- Try different weight initializations\n",
    "- Consider if problem requires multiple neurons/layers\n",
    "\n",
    "**4. Slow performance:**\n",
    "- Profile code to identify bottlenecks\n",
    "- Consider vectorized operations (covered in next labs)\n",
    "\n",
    "**5. Visualization issues:**\n",
    "- Ensure matplotlib backend supports interactive plots\n",
    "- Check that grid resolution is appropriate\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "1. **Artificial Neuron**: Mathematical model combining linear transformation and non-linear activation\n",
    "2. **Forward Pass**: Computing output through weights → linear combination → activation\n",
    "3. **Decision Boundaries**: Single neurons create linear decision boundaries\n",
    "4. **Multiple Neurons**: Combining neurons creates richer representations\n",
    "5. **Linear Limitations**: Single neurons cannot solve non-linearly separable problems\n",
    "6. **Computational Complexity**: Performance scales with input size and network depth\n",
    "7. **Memory Requirements**: Dominated by parameter storage, grows with network size\n",
    "8. **Foundation Building**: Neurons are building blocks for larger networks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll visualize neural networks and understand how multiple neurons and layers work together to create complex decision boundaries and solve non-linear problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've successfully implemented and analyzed artificial neurons from scratch!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}