{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.5: Neural Network Visualization\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Visualize neural network architectures and data flow\n",
    "- Create interactive visualizations of network behavior\n",
    "- Understand how multiple layers transform data representations\n",
    "- Analyze feature learning through visualization\n",
    "- Build intuition about deep learning through visual exploration\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1.1 (Environment Setup)\n",
    "- Completed Lab 1.2 (Mathematical Foundations)\n",
    "- Completed Lab 1.3 (Activation Functions)\n",
    "- Completed Lab 1.4 (Basic Neuron Implementation)\n",
    "- Understanding of neural network components\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "print(\"Environment ready for neural network visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Network Architecture Visualization\n",
    "\n",
    "Let's start by creating clear visualizations of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 1: NEURAL NETWORK ARCHITECTURE DIAGRAMS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def draw_neural_network(layer_sizes, layer_names=None, title=\"Neural Network Architecture\"):\n",
    "    \"\"\"\n",
    "    Draw a neural network architecture diagram\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: List of integers representing number of neurons in each layer\n",
    "        layer_names: List of names for each layer\n",
    "        title: Title for the diagram\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Network parameters\n",
    "    layer_spacing = 3.0  # Horizontal distance between layers\n",
    "    max_neurons = max(layer_sizes)\n",
    "    neuron_radius = 0.3\n",
    "    \n",
    "    # Colors for different layer types\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']\n",
    "    \n",
    "    # Store neuron positions for drawing connections\n",
    "    neuron_positions = []\n",
    "    \n",
    "    # Draw each layer\n",
    "    for layer_idx, n_neurons in enumerate(layer_sizes):\n",
    "        x = layer_idx * layer_spacing\n",
    "        color = colors[layer_idx % len(colors)]\n",
    "        \n",
    "        # Calculate vertical spacing to center the layer\n",
    "        if n_neurons == 1:\n",
    "            y_positions = [0]\n",
    "        else:\n",
    "            y_start = -(n_neurons - 1) * 0.8 / 2\n",
    "            y_positions = [y_start + i * 0.8 for i in range(n_neurons)]\n",
    "        \n",
    "        layer_positions = []\n",
    "        \n",
    "        # Draw neurons in this layer\n",
    "        for neuron_idx, y in enumerate(y_positions):\n",
    "            circle = patches.Circle((x, y), neuron_radius, \n",
    "                                  facecolor=color, edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            layer_positions.append((x, y))\n",
    "            \n",
    "            # Add neuron index for small layers\n",
    "            if n_neurons <= 5:\n",
    "                ax.text(x, y, str(neuron_idx + 1), ha='center', va='center',\n",
    "                       fontweight='bold', fontsize=10)\n",
    "        \n",
    "        neuron_positions.append(layer_positions)\n",
    "        \n",
    "        # Add layer label\n",
    "        label_y = max(y_positions) + 1.2 if y_positions else 1.2\n",
    "        if layer_names:\n",
    "            layer_name = layer_names[layer_idx]\n",
    "        else:\n",
    "            if layer_idx == 0:\n",
    "                layer_name = \"Input Layer\"\n",
    "            elif layer_idx == len(layer_sizes) - 1:\n",
    "                layer_name = \"Output Layer\"\n",
    "            else:\n",
    "                layer_name = f\"Hidden Layer {layer_idx}\"\n",
    "        \n",
    "        ax.text(x, label_y, f\"{layer_name}\\n({n_neurons} neurons)\", \n",
    "               ha='center', va='center', fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Draw connections between layers\n",
    "    for layer_idx in range(len(neuron_positions) - 1):\n",
    "        current_layer = neuron_positions[layer_idx]\n",
    "        next_layer = neuron_positions[layer_idx + 1]\n",
    "        \n",
    "        # Draw connections between all neurons in adjacent layers\n",
    "        for current_pos in current_layer:\n",
    "            for next_pos in next_layer:\n",
    "                # Calculate connection endpoints (edge of circles)\n",
    "                x1, y1 = current_pos[0] + neuron_radius, current_pos[1]\n",
    "                x2, y2 = next_pos[0] - neuron_radius, next_pos[1]\n",
    "                \n",
    "                # Draw connection with varying thickness for visual appeal\n",
    "                ax.plot([x1, x2], [y1, y2], 'gray', alpha=0.6, linewidth=0.8)\n",
    "    \n",
    "    # Set plot properties\n",
    "    ax.set_xlim(-1, (len(layer_sizes) - 1) * layer_spacing + 1)\n",
    "    y_range = max_neurons * 0.4 + 2\n",
    "    ax.set_ylim(-y_range, y_range)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add information box\n",
    "    total_neurons = sum(layer_sizes)\n",
    "    total_connections = sum(layer_sizes[i] * layer_sizes[i+1] for i in range(len(layer_sizes)-1))\n",
    "    \n",
    "    info_text = f\"Total Neurons: {total_neurons}\\nTotal Connections: {total_connections}\"\n",
    "    ax.text(0.02, 0.98, info_text, transform=ax.transAxes, fontsize=10,\n",
    "           verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', \n",
    "                                            facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 1: Simple feedforward network\n",
    "print(\"\\nExample 1: Simple Classification Network\")\n",
    "draw_neural_network([3, 4, 4, 2], \n",
    "                   [\"Input\\n(3 features)\", \"Hidden 1\\n(4 neurons)\", \n",
    "                    \"Hidden 2\\n(4 neurons)\", \"Output\\n(2 classes)\"],\n",
    "                   \"Simple Classification Network\")\n",
    "\n",
    "# Example 2: Deep network\n",
    "print(\"\\nExample 2: Deep Network for Image Recognition\")\n",
    "draw_neural_network([784, 256, 128, 64, 10],\n",
    "                   [\"Input\\n(28×28 pixels)\", \"Hidden 1\\n(256 neurons)\",\n",
    "                    \"Hidden 2\\n(128 neurons)\", \"Hidden 3\\n(64 neurons)\", \n",
    "                    \"Output\\n(10 digits)\"],\n",
    "                   \"Deep Network for MNIST Digit Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Flow Visualization\n",
    "\n",
    "Let's visualize how data flows through a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 2: DATA FLOW VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Import activation functions from previous labs\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Simple neural network implementation for visualization\n",
    "class VisualizationNetwork:\n",
    "    def __init__(self, layer_sizes, activation='sigmoid'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Xavier initialization\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) / np.sqrt(layer_sizes[i])\n",
    "            b = np.zeros(layer_sizes[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "        \n",
    "        # Store intermediate values for visualization\n",
    "        self.activations = []\n",
    "        self.linear_outputs = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with intermediate value storage\"\"\"\n",
    "        self.activations = [x]  # Store input as first activation\n",
    "        self.linear_outputs = []\n",
    "        \n",
    "        current_activation = x\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Linear combination\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            self.linear_outputs.append(z)\n",
    "            \n",
    "            # Apply activation function (except for output layer in some cases)\n",
    "            if i < self.n_layers - 2:  # Hidden layers\n",
    "                current_activation = self.activation(z)\n",
    "            else:  # Output layer\n",
    "                current_activation = self.activation(z)  # Or could be linear\n",
    "            \n",
    "            self.activations.append(current_activation)\n",
    "        \n",
    "        return current_activation\n",
    "\n",
    "# Create a small network for detailed visualization\n",
    "network = VisualizationNetwork([2, 3, 2], activation='sigmoid')\n",
    "\n",
    "# Test input\n",
    "test_input = np.array([0.5, -0.3])\n",
    "output = network.forward(test_input)\n",
    "\n",
    "print(f\"Network architecture: {network.layer_sizes}\")\n",
    "print(f\"Test input: {test_input}\")\n",
    "print(f\"Network output: {output}\")\n",
    "\n",
    "# Visualize the data flow\n",
    "def visualize_data_flow(network, input_data, title=\"Neural Network Data Flow\"):\n",
    "    \"\"\"Visualize how data flows through the network\"\"\"\n",
    "    fig, axes = plt.subplots(2, len(network.layer_sizes), figsize=(18, 10))\n",
    "    \n",
    "    # Top row: Activation values\n",
    "    # Bottom row: Weights and connections\n",
    "    \n",
    "    for layer_idx in range(len(network.layer_sizes)):\n",
    "        # Plot activation values\n",
    "        ax_top = axes[0, layer_idx]\n",
    "        ax_bottom = axes[1, layer_idx]\n",
    "        \n",
    "        activation = network.activations[layer_idx]\n",
    "        n_neurons = len(activation)\n",
    "        \n",
    "        # Top plot: Bar chart of activation values\n",
    "        bars = ax_top.bar(range(n_neurons), activation, \n",
    "                         color='lightblue', edgecolor='black')\n",
    "        \n",
    "        # Color bars based on activation strength\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if height > 0.7:\n",
    "                bar.set_color('red')  # High activation\n",
    "            elif height > 0.3:\n",
    "                bar.set_color('orange')  # Medium activation\n",
    "            else:\n",
    "                bar.set_color('lightblue')  # Low activation\n",
    "        \n",
    "        ax_top.set_ylim(0, 1)\n",
    "        ax_top.set_title(f'Layer {layer_idx} Activations')\n",
    "        ax_top.set_xlabel('Neuron')\n",
    "        ax_top.set_ylabel('Activation')\n",
    "        ax_top.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(activation):\n",
    "            ax_top.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Bottom plot: Weight visualization (if not last layer)\n",
    "        if layer_idx < len(network.layer_sizes) - 1:\n",
    "            weights = network.weights[layer_idx]\n",
    "            im = ax_bottom.imshow(weights.T, cmap='RdBu', aspect='auto', \n",
    "                                vmin=-np.max(np.abs(weights)), vmax=np.max(np.abs(weights)))\n",
    "            ax_bottom.set_title(f'Weights: Layer {layer_idx} → {layer_idx+1}')\n",
    "            ax_bottom.set_xlabel(f'Input from Layer {layer_idx}')\n",
    "            ax_bottom.set_ylabel(f'Output to Layer {layer_idx+1}')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax_bottom, fraction=0.046, pad=0.04)\n",
    "        else:\n",
    "            ax_bottom.axis('off')\n",
    "            ax_bottom.text(0.5, 0.5, 'Output Layer\\n(No outgoing weights)', \n",
    "                         ha='center', va='center', transform=ax_bottom.transAxes,\n",
    "                         fontsize=12, bbox=dict(boxstyle='round,pad=0.5', \n",
    "                                               facecolor='lightyellow'))\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "print(\"\\nDetailed Data Flow Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "visualize_data_flow(network, test_input)\n",
    "\n",
    "# Show intermediate computations\n",
    "print(\"\\nLayer-by-layer Computation Details:\")\n",
    "print(\"=\" * 40)\n",
    "for i in range(len(network.layer_sizes)):\n",
    "    print(f\"\\nLayer {i} ({network.layer_sizes[i]} neurons):\")\n",
    "    print(f\"  Activations: {network.activations[i]}\")\n",
    "    \n",
    "    if i < len(network.linear_outputs):\n",
    "        print(f\"  Linear output (z): {network.linear_outputs[i]}\")\n",
    "        print(f\"  After activation: {network.activations[i+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Decision Boundary Evolution\n",
    "\n",
    "Let's visualize how decision boundaries change with network depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 3: DECISION BOUNDARY EVOLUTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Generate sample 2D classification data\n",
    "def generate_classification_data(n_samples=200):\n",
    "    \"\"\"Generate 2D classification data with interesting patterns\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create XOR-like pattern\n",
    "    X = np.random.uniform(-2, 2, (n_samples, 2))\n",
    "    \n",
    "    # XOR pattern: class 1 if x1*x2 > 0, else class 0\n",
    "    y = ((X[:, 0] * X[:, 1]) > 0).astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_data, y_data = generate_classification_data()\n",
    "\n",
    "print(f\"Generated {len(X_data)} data points\")\n",
    "print(f\"Class distribution: Class 0: {np.sum(y_data == 0)}, Class 1: {np.sum(y_data == 1)}\")\n",
    "\n",
    "# Create networks with different architectures\n",
    "networks = {\n",
    "    'Single Layer': VisualizationNetwork([2, 1], 'sigmoid'),\n",
    "    'One Hidden': VisualizationNetwork([2, 3, 1], 'sigmoid'),\n",
    "    'Two Hidden': VisualizationNetwork([2, 4, 3, 1], 'sigmoid'),\n",
    "    'Deep Network': VisualizationNetwork([2, 6, 4, 3, 1], 'sigmoid')\n",
    "}\n",
    "\n",
    "def plot_decision_boundary(network, X, y, title, ax):\n",
    "    \"\"\"Plot decision boundary for a network\"\"\"\n",
    "    \n",
    "    # Create a mesh\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = []\n",
    "    for point in mesh_points:\n",
    "        output = network.forward(point)\n",
    "        Z.append(output[0] if hasattr(output, '__len__') else output)\n",
    "    \n",
    "    Z = np.array(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z, levels=20, alpha=0.6, cmap='RdYlBu')\n",
    "    boundary = ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['blue', 'red']\n",
    "    for class_idx in [0, 1]:\n",
    "        class_points = X[y == class_idx]\n",
    "        ax.scatter(class_points[:, 0], class_points[:, 1], \n",
    "                  c=colors[class_idx], label=f'Class {class_idx}', \n",
    "                  alpha=0.8, edgecolors='black', s=50)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundaries for different network architectures\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, network) in enumerate(networks.items()):\n",
    "    plot_decision_boundary(network, X_data, y_data, \n",
    "                          f'{name}\\n{network.layer_sizes}', axes[idx])\n",
    "\n",
    "plt.suptitle('Decision Boundaries with Different Network Architectures\\n(XOR-like Problem)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🧠 Key Observations:\")\n",
    "print(\"   - Single layer: Only linear decision boundaries\")\n",
    "print(\"   - One hidden layer: Can create non-linear boundaries\")\n",
    "print(\"   - Deeper networks: More complex, flexible boundaries\")\n",
    "print(\"   - XOR problem requires at least one hidden layer to solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Learning Visualization\n",
    "\n",
    "Let's visualize how neural networks learn to represent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 4: FEATURE LEARNING VISUALIZATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a network that we can analyze layer by layer\n",
    "def visualize_feature_transformation(network, X_sample, title=\"Feature Transformation\"):\n",
    "    \"\"\"Visualize how features are transformed through network layers\"\"\"\n",
    "    \n",
    "    n_layers = len(network.layer_sizes)\n",
    "    n_samples = min(len(X_sample), 100)  # Limit samples for clarity\n",
    "    \n",
    "    # Process all samples through the network\n",
    "    layer_outputs = [[] for _ in range(n_layers)]\n",
    "    \n",
    "    for sample in X_sample[:n_samples]:\n",
    "        network.forward(sample)\n",
    "        for i, activation in enumerate(network.activations):\n",
    "            layer_outputs[i].append(activation.copy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for i in range(n_layers):\n",
    "        layer_outputs[i] = np.array(layer_outputs[i])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, n_layers, figsize=(4*n_layers, 8))\n",
    "    if n_layers == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        layer_data = layer_outputs[layer_idx]\n",
    "        \n",
    "        # Top plot: Feature space visualization (for 2D/3D)\n",
    "        ax_top = axes[0, layer_idx]\n",
    "        \n",
    "        if layer_data.shape[1] == 1:\n",
    "            # 1D output - histogram\n",
    "            ax_top.hist(layer_data[:, 0], bins=20, alpha=0.7, \n",
    "                       color='lightblue', edgecolor='black')\n",
    "            ax_top.set_title(f'Layer {layer_idx} Distribution')\n",
    "            ax_top.set_xlabel('Activation Value')\n",
    "            ax_top.set_ylabel('Frequency')\n",
    "            \n",
    "        elif layer_data.shape[1] == 2:\n",
    "            # 2D output - scatter plot\n",
    "            colors = ['blue' if y_data[i] == 0 else 'red' for i in range(n_samples)]\n",
    "            ax_top.scatter(layer_data[:, 0], layer_data[:, 1], c=colors, alpha=0.7)\n",
    "            ax_top.set_title(f'Layer {layer_idx} Feature Space')\n",
    "            ax_top.set_xlabel('Feature 1')\n",
    "            ax_top.set_ylabel('Feature 2')\n",
    "            \n",
    "        else:\n",
    "            # Higher dimensional - show heatmap of activations\n",
    "            im = ax_top.imshow(layer_data[:50].T, aspect='auto', cmap='viridis')\n",
    "            ax_top.set_title(f'Layer {layer_idx} Activations')\n",
    "            ax_top.set_xlabel('Sample')\n",
    "            ax_top.set_ylabel('Neuron')\n",
    "            plt.colorbar(im, ax=ax_top, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        ax_top.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom plot: Activation statistics\n",
    "        ax_bottom = axes[1, layer_idx]\n",
    "        \n",
    "        # Show mean activation per neuron\n",
    "        mean_activations = np.mean(layer_data, axis=0)\n",
    "        std_activations = np.std(layer_data, axis=0)\n",
    "        \n",
    "        bars = ax_bottom.bar(range(len(mean_activations)), mean_activations, \n",
    "                           yerr=std_activations, capsize=5,\n",
    "                           color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        ax_bottom.set_title(f'Layer {layer_idx} Statistics')\n",
    "        ax_bottom.set_xlabel('Neuron')\n",
    "        ax_bottom.set_ylabel('Mean Activation ± Std')\n",
    "        ax_bottom.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight neurons with high/low activity\n",
    "        for i, bar in enumerate(bars):\n",
    "            if mean_activations[i] > 0.7:\n",
    "                bar.set_color('red')  # High activity\n",
    "            elif mean_activations[i] < 0.3:\n",
    "                bar.set_color('blue')  # Low activity\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze a 3-layer network in detail\n",
    "feature_network = VisualizationNetwork([2, 4, 3, 1], 'sigmoid')\n",
    "print(f\"\\nAnalyzing network: {feature_network.layer_sizes}\")\n",
    "print(\"Processing sample data through all layers...\")\n",
    "\n",
    "visualize_feature_transformation(feature_network, X_data, \n",
    "                               \"Feature Learning: Layer-by-Layer Transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Weight and Bias Visualization\n",
    "\n",
    "Let's examine how weights and biases determine network behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 5: WEIGHTS AND BIAS ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def visualize_weights_and_biases(network, title=\"Network Parameters\"):\n",
    "    \"\"\"Comprehensive visualization of network weights and biases\"\"\"\n",
    "    \n",
    "    n_layers = len(network.weights)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Weight matrices\n",
    "    for i, weights in enumerate(network.weights):\n",
    "        # Weight heatmap\n",
    "        ax1 = plt.subplot(3, n_layers, i + 1)\n",
    "        im1 = ax1.imshow(weights, cmap='RdBu', aspect='auto',\n",
    "                        vmin=-np.max(np.abs(weights)), vmax=np.max(np.abs(weights)))\n",
    "        ax1.set_title(f'Weight Matrix {i+1}\\n{weights.shape}')\n",
    "        ax1.set_xlabel('Output Neurons')\n",
    "        ax1.set_ylabel('Input Neurons')\n",
    "        plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Weight distribution\n",
    "        ax2 = plt.subplot(3, n_layers, n_layers + i + 1)\n",
    "        ax2.hist(weights.flatten(), bins=30, alpha=0.7, \n",
    "                color='skyblue', edgecolor='black')\n",
    "        ax2.set_title(f'Weight Distribution {i+1}')\n",
    "        ax2.set_xlabel('Weight Value')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Statistics text\n",
    "        mean_w = np.mean(weights)\n",
    "        std_w = np.std(weights)\n",
    "        min_w = np.min(weights)\n",
    "        max_w = np.max(weights)\n",
    "        \n",
    "        stats_text = f'Mean: {mean_w:.3f}\\nStd: {std_w:.3f}\\nMin: {min_w:.3f}\\nMax: {max_w:.3f}'\n",
    "        ax2.text(0.7, 0.7, stats_text, transform=ax2.transAxes,\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Bias visualization\n",
    "        ax3 = plt.subplot(3, n_layers, 2*n_layers + i + 1)\n",
    "        biases = network.biases[i]\n",
    "        bars = ax3.bar(range(len(biases)), biases, \n",
    "                      color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "        ax3.set_title(f'Bias Values {i+1}')\n",
    "        ax3.set_xlabel('Neuron')\n",
    "        ax3.set_ylabel('Bias Value')\n",
    "        ax3.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Color bars based on bias magnitude\n",
    "        for j, bar in enumerate(bars):\n",
    "            if abs(biases[j]) > 0.5:\n",
    "                bar.set_color('red' if biases[j] > 0 else 'blue')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze our network's parameters\n",
    "print(\"\\nNetwork Parameter Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "visualize_weights_and_biases(feature_network, \n",
    "                           \"Weight and Bias Analysis: Feature Learning Network\")\n",
    "\n",
    "# Show detailed parameter information\n",
    "print(\"\\nDetailed Parameter Summary:\")\n",
    "print(\"=\" * 40)\n",
    "total_params = 0\n",
    "\n",
    "for i, (weights, biases) in enumerate(zip(feature_network.weights, feature_network.biases)):\n",
    "    layer_params = weights.size + biases.size\n",
    "    total_params += layer_params\n",
    "    \n",
    "    print(f\"\\nLayer {i+1} -> Layer {i+2}:\")\n",
    "    print(f\"  Weight matrix: {weights.shape}\")\n",
    "    print(f\"  Bias vector: {biases.shape}\")\n",
    "    print(f\"  Parameters: {layer_params}\")\n",
    "    print(f\"  Weight range: [{np.min(weights):.3f}, {np.max(weights):.3f}]\")\n",
    "    print(f\"  Bias range: [{np.min(biases):.3f}, {np.max(biases):.3f}]\")\n",
    "\n",
    "print(f\"\\nTotal network parameters: {total_params}\")\n",
    "print(f\"Memory usage (approx): {total_params * 8 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Interactive Network Explorer\n",
    "\n",
    "Let's create an interactive tool to explore how network parameters affect behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 6: INTERACTIVE NETWORK EXPLORATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple interactive network exploration\n",
    "def explore_network_behavior(network_configs, test_points):\n",
    "    \"\"\"Explore different network configurations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(network_configs), figsize=(5*len(network_configs), 10))\n",
    "    if len(network_configs) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for idx, (name, network) in enumerate(network_configs.items()):\n",
    "        ax_top = axes[0, idx]\n",
    "        ax_bottom = axes[1, idx]\n",
    "        \n",
    "        # Test the network with different inputs\n",
    "        outputs = []\n",
    "        for point in test_points:\n",
    "            output = network.forward(point)\n",
    "            outputs.append(output[0] if hasattr(output, '__len__') else output)\n",
    "        \n",
    "        # Top plot: Input-output mapping\n",
    "        if test_points.shape[1] == 2:\n",
    "            # 2D input case\n",
    "            scatter = ax_top.scatter(test_points[:, 0], test_points[:, 1], \n",
    "                                   c=outputs, cmap='viridis', s=50, alpha=0.7)\n",
    "            ax_top.set_xlabel('Input 1')\n",
    "            ax_top.set_ylabel('Input 2')\n",
    "            plt.colorbar(scatter, ax=ax_top, fraction=0.046, pad=0.04, label='Output')\n",
    "        else:\n",
    "            # 1D input case\n",
    "            ax_top.plot(test_points[:, 0], outputs, 'bo-', linewidth=2, markersize=4)\n",
    "            ax_top.set_xlabel('Input')\n",
    "            ax_top.set_ylabel('Output')\n",
    "        \n",
    "        ax_top.set_title(f'{name}\\nInput-Output Mapping')\n",
    "        ax_top.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom plot: Network response surface (for 2D input)\n",
    "        if test_points.shape[1] == 2:\n",
    "            # Create a finer grid\n",
    "            x_range = np.linspace(test_points[:, 0].min(), test_points[:, 0].max(), 50)\n",
    "            y_range = np.linspace(test_points[:, 1].min(), test_points[:, 1].max(), 50)\n",
    "            X, Y = np.meshgrid(x_range, y_range)\n",
    "            Z = np.zeros_like(X)\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                    point = np.array([X[i, j], Y[i, j]])\n",
    "                    output = network.forward(point)\n",
    "                    Z[i, j] = output[0] if hasattr(output, '__len__') else output\n",
    "            \n",
    "            contour = ax_bottom.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "            ax_bottom.contour(X, Y, Z, levels=10, colors='black', alpha=0.3, linewidths=0.5)\n",
    "            ax_bottom.set_xlabel('Input 1')\n",
    "            ax_bottom.set_ylabel('Input 2')\n",
    "            plt.colorbar(contour, ax=ax_bottom, fraction=0.046, pad=0.04, label='Output')\n",
    "        else:\n",
    "            # For 1D, show derivative approximation\n",
    "            x_fine = np.linspace(test_points[:, 0].min(), test_points[:, 0].max(), 100)\n",
    "            y_fine = []\n",
    "            for x in x_fine:\n",
    "                output = network.forward(np.array([x]))\n",
    "                y_fine.append(output[0] if hasattr(output, '__len__') else output)\n",
    "            \n",
    "            ax_bottom.plot(x_fine, y_fine, 'r-', linewidth=2, label='Network Response')\n",
    "            ax_bottom.set_xlabel('Input')\n",
    "            ax_bottom.set_ylabel('Output')\n",
    "            ax_bottom.legend()\n",
    "        \n",
    "        ax_bottom.set_title(f'{name}\\nResponse Surface')\n",
    "        ax_bottom.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Network Behavior Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create test configurations\n",
    "test_networks = {\n",
    "    'Linear (No Hidden)': VisualizationNetwork([2, 1], 'sigmoid'),\n",
    "    'Small Hidden': VisualizationNetwork([2, 3, 1], 'sigmoid'),\n",
    "    'Large Hidden': VisualizationNetwork([2, 8, 1], 'sigmoid'),\n",
    "}\n",
    "\n",
    "# Generate test points\n",
    "test_grid = np.array([[x, y] for x in np.linspace(-2, 2, 10) \n",
    "                                for y in np.linspace(-2, 2, 10)])\n",
    "\n",
    "print(\"\\nComparing Network Architectures:\")\n",
    "print(\"-\" * 40)\n",
    "explore_network_behavior(test_networks, test_grid)\n",
    "\n",
    "# Show complexity comparison\n",
    "print(\"\\nComplexity Comparison:\")\n",
    "print(\"=\" * 30)\n",
    "for name, network in test_networks.items():\n",
    "    total_params = sum(w.size + b.size for w, b in zip(network.weights, network.biases))\n",
    "    print(f\"{name:<20}: {network.layer_sizes} -> {total_params} parameters\")\n",
    "\n",
    "print(\"\\n🔍 Observations:\")\n",
    "print(\"   - More parameters allow more complex decision boundaries\")\n",
    "print(\"   - But also increase risk of overfitting\")\n",
    "print(\"   - Architecture choice is crucial for performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Learning Process Visualization\n",
    "\n",
    "Let's simulate and visualize how a network might learn over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 7: SIMULATED LEARNING VISUALIZATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def simulate_learning_process(initial_network, target_function, n_steps=10):\n",
    "    \"\"\"Simulate how network parameters might change during learning\"\"\"\n",
    "    \n",
    "    # Generate training data\n",
    "    X_train = np.random.uniform(-2, 2, (100, 2))\n",
    "    y_train = target_function(X_train)\n",
    "    \n",
    "    # Create snapshots of the network at different \"training stages\"\n",
    "    networks = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Create a copy of the network\n",
    "        network_copy = VisualizationNetwork(initial_network.layer_sizes, 'sigmoid')\n",
    "        \n",
    "        # Simulate weight evolution (simplified)\n",
    "        progress = step / (n_steps - 1)  # 0 to 1\n",
    "        \n",
    "        for i in range(len(network_copy.weights)):\n",
    "            # Start with random weights, gradually \"improve\" them\n",
    "            # This is a simplified simulation, not actual training\n",
    "            noise_scale = 1.0 - 0.8 * progress  # Reduce randomness over time\n",
    "            network_copy.weights[i] = initial_network.weights[i] * (1 + noise_scale * np.random.randn(*initial_network.weights[i].shape) * 0.1)\n",
    "            network_copy.biases[i] = initial_network.biases[i] * (1 + noise_scale * np.random.randn(*initial_network.biases[i].shape) * 0.1)\n",
    "        \n",
    "        networks.append((f\"Step {step+1}\", network_copy))\n",
    "    \n",
    "    return networks, X_train, y_train\n",
    "\n",
    "# Define a target function (XOR-like)\n",
    "def xor_target(X):\n",
    "    return ((X[:, 0] * X[:, 1]) > 0).astype(float)\n",
    "\n",
    "# Create initial network\n",
    "learning_network = VisualizationNetwork([2, 4, 1], 'sigmoid')\n",
    "\n",
    "# Simulate learning process\n",
    "print(\"\\nSimulating learning process...\")\n",
    "learning_snapshots, X_train, y_train = simulate_learning_process(\n",
    "    learning_network, xor_target, n_steps=6)\n",
    "\n",
    "# Visualize learning progression\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (step_name, network) in enumerate(learning_snapshots):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = -2.5, 2.5\n",
    "    y_min, y_max = -2.5, 2.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = []\n",
    "    for point in mesh_points:\n",
    "        output = network.forward(point)\n",
    "        Z.append(output[0] if hasattr(output, '__len__') else output)\n",
    "    \n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z, levels=20, alpha=0.6, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot training data\n",
    "    colors = ['blue', 'red']\n",
    "    for class_val in [0, 1]:\n",
    "        class_points = X_train[y_train == class_val]\n",
    "        ax.scatter(class_points[:, 0], class_points[:, 1], \n",
    "                  c=colors[int(class_val)], label=f'Class {int(class_val)}',\n",
    "                  alpha=0.8, edgecolors='black', s=30)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = []\n",
    "    for point in X_train:\n",
    "        output = network.forward(point)\n",
    "        pred = 1 if (output[0] if hasattr(output, '__len__') else output) > 0.5 else 0\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    accuracy = np.mean(np.array(predictions) == y_train)\n",
    "    \n",
    "    ax.set_title(f'{step_name}\\nAccuracy: {accuracy:.3f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle('Simulated Learning Process: Decision Boundary Evolution', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📚 Learning Process Insights:\")\n",
    "print(\"   - Initially: Random decision boundaries\")\n",
    "print(\"   - Gradually: Boundaries adapt to data patterns\")\n",
    "print(\"   - Eventually: Complex patterns can be learned\")\n",
    "print(\"   - This simulation shows the power of iterative learning!\")\n",
    "\n",
    "# Show weight evolution\n",
    "print(\"\\nWeight Evolution Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "first_network = learning_snapshots[0][1]\n",
    "last_network = learning_snapshots[-1][1]\n",
    "\n",
    "for layer_idx in range(len(first_network.weights)):\n",
    "    initial_weights = first_network.weights[layer_idx]\n",
    "    final_weights = last_network.weights[layer_idx]\n",
    "    \n",
    "    weight_change = np.mean(np.abs(final_weights - initial_weights))\n",
    "    print(f\"Layer {layer_idx+1}: Average weight change = {weight_change:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checklist\n",
    "\n",
    "Mark each concept as understood:\n",
    "\n",
    "- [ ] Neural network architecture diagrams\n",
    "- [ ] Data flow visualization through network layers\n",
    "- [ ] Decision boundary evolution with network depth\n",
    "- [ ] Feature transformation visualization\n",
    "- [ ] Weight and bias analysis techniques\n",
    "- [ ] Interactive network exploration methods\n",
    "- [ ] Learning process simulation and visualization\n",
    "- [ ] Understanding of network complexity tradeoffs\n",
    "- [ ] Visual intuition for deep learning concepts\n",
    "- [ ] Tools for analyzing network behavior\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Visualization rendering problems:**\n",
    "- Ensure matplotlib backend supports interactive plots\n",
    "- Try `%matplotlib inline` or `%matplotlib notebook`\n",
    "- Restart Jupyter kernel if plots don't appear\n",
    "\n",
    "**2. Memory issues with large visualizations:**\n",
    "- Reduce grid resolution for decision boundary plots\n",
    "- Limit number of test points for analysis\n",
    "- Use smaller networks for detailed analysis\n",
    "\n",
    "**3. Slow visualization performance:**\n",
    "- Optimize network forward pass implementation\n",
    "- Use vectorized operations where possible\n",
    "- Consider sampling data for large datasets\n",
    "\n",
    "**4. Colormap interpretation:**\n",
    "- Check colorbar scales and ranges\n",
    "- Use consistent color schemes across plots\n",
    "- Consider colorblind-friendly palettes\n",
    "\n",
    "**5. Plot layout issues:**\n",
    "- Adjust figure sizes for better visibility\n",
    "- Use `plt.tight_layout()` to prevent overlap\n",
    "- Consider subplot arrangements for complex visualizations\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "1. **Architecture Visualization**: Clear diagrams help understand network structure\n",
    "2. **Data Flow**: Tracking how information transforms through layers\n",
    "3. **Decision Boundaries**: Visual representation of network decision-making\n",
    "4. **Feature Learning**: How networks create useful representations\n",
    "5. **Parameter Analysis**: Understanding weights and biases through visualization\n",
    "6. **Network Comparison**: Visual tools for comparing different architectures\n",
    "7. **Learning Dynamics**: Visualization of how networks change during training\n",
    "8. **Interactive Exploration**: Tools for hands-on understanding\n",
    "\n",
    "## Applications in Practice\n",
    "\n",
    "These visualization techniques are used in real-world deep learning for:\n",
    "- **Model debugging**: Identifying problems in network behavior\n",
    "- **Architecture design**: Choosing appropriate network structures\n",
    "- **Training monitoring**: Tracking learning progress\n",
    "- **Feature analysis**: Understanding what networks learn\n",
    "- **Hyperparameter tuning**: Visual feedback for optimization\n",
    "- **Model interpretation**: Making black-box models more transparent\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll apply these visualization techniques to analyze real-world problems and understand how different network architectures perform on practical tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've mastered neural network visualization techniques that will help you understand and debug deep learning models!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}