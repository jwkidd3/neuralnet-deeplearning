{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.6: Real-world Problem Analysis\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Analyze real-world problems to determine if neural networks are appropriate\n",
    "- Identify different types of machine learning problems (classification, regression, etc.)\n",
    "- Understand data requirements and preprocessing needs\n",
    "- Select appropriate network architectures for different problem types\n",
    "- Recognize common challenges in real-world applications\n",
    "- Apply problem analysis framework to practical scenarios\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1.1 (Environment Setup)\n",
    "- Completed Lab 1.2 (Mathematical Foundations)\n",
    "- Completed Lab 1.3 (Activation Functions)\n",
    "- Completed Lab 1.4 (Basic Neuron Implementation)\n",
    "- Completed Lab 1.5 (Neural Network Visualization)\n",
    "- Understanding of neural network fundamentals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression, make_circles\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Environment ready for real-world problem analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem Classification Framework\n",
    "\n",
    "Let's establish a framework for analyzing real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PART 1: REAL-WORLD PROBLEM CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class ProblemAnalyzer:\n",
    "    \"\"\"Framework for analyzing real-world machine learning problems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.problem_types = {\n",
    "            'classification': {\n",
    "                'description': 'Predict discrete categories or classes',\n",
    "                'examples': ['Email spam detection', 'Image recognition', 'Medical diagnosis'],\n",
    "                'output_type': 'Categorical',\n",
    "                'metrics': ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "                'activation': 'Sigmoid (binary) / Softmax (multi-class)'\n",
    "            },\n",
    "            'regression': {\n",
    "                'description': 'Predict continuous numerical values',\n",
    "                'examples': ['Stock price prediction', 'House price estimation', 'Temperature forecasting'],\n",
    "                'output_type': 'Continuous',\n",
    "                'metrics': ['MSE', 'MAE', 'R¬≤'],\n",
    "                'activation': 'Linear (or ReLU for non-negative)'\n",
    "            },\n",
    "            'clustering': {\n",
    "                'description': 'Group similar data points together',\n",
    "                'examples': ['Customer segmentation', 'Gene sequencing', 'Market research'],\n",
    "                'output_type': 'Group assignments',\n",
    "                'metrics': ['Silhouette score', 'Within-cluster sum of squares'],\n",
    "                'activation': 'Various (unsupervised learning)'\n",
    "            },\n",
    "            'sequence_modeling': {\n",
    "                'description': 'Handle sequential or time-series data',\n",
    "                'examples': ['Natural language processing', 'Speech recognition', 'Time series forecasting'],\n",
    "                'output_type': 'Sequences or single values',\n",
    "                'metrics': ['Perplexity', 'BLEU score', 'Time-series accuracy'],\n",
    "                'activation': 'Varies by task'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_problem(self, problem_description, data_info=None):\n",
    "        \"\"\"Analyze a problem and suggest approach\"\"\"\n",
    "        print(f\"\\nüîç PROBLEM ANALYSIS: {problem_description}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # This is a simplified heuristic analysis\n",
    "        # In practice, you'd have more sophisticated logic\n",
    "        \n",
    "        analysis = {\n",
    "            'problem_type': None,\n",
    "            'complexity': 'Unknown',\n",
    "            'data_requirements': [],\n",
    "            'suggested_architecture': None,\n",
    "            'challenges': [],\n",
    "            'preprocessing_steps': []\n",
    "        }\n",
    "        \n",
    "        # Simple keyword-based classification (in practice, this would be more sophisticated)\n",
    "        description_lower = problem_description.lower()\n",
    "        \n",
    "        if any(word in description_lower for word in ['classify', 'classify', 'detection', 'recognition', 'diagnosis']):\n",
    "            analysis['problem_type'] = 'classification'\n",
    "        elif any(word in description_lower for word in ['predict', 'forecast', 'estimate', 'price', 'value']):\n",
    "            analysis['problem_type'] = 'regression'\n",
    "        elif any(word in description_lower for word in ['cluster', 'segment', 'group']):\n",
    "            analysis['problem_type'] = 'clustering'\n",
    "        elif any(word in description_lower for word in ['sequence', 'text', 'language', 'speech', 'time series']):\n",
    "            analysis['problem_type'] = 'sequence_modeling'\n",
    "        else:\n",
    "            analysis['problem_type'] = 'unclear - needs more information'\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def print_problem_types(self):\n",
    "        \"\"\"Display problem type reference\"\"\"\n",
    "        print(\"\\nüìä MACHINE LEARNING PROBLEM TYPES REFERENCE\")\n",
    "        print(\"=\" * 55)\n",
    "        \n",
    "        for prob_type, info in self.problem_types.items():\n",
    "            print(f\"\\n{prob_type.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Description: {info['description']}\")\n",
    "            print(f\"  Output Type: {info['output_type']}\")\n",
    "            print(f\"  Common Metrics: {', '.join(info['metrics'])}\")\n",
    "            print(f\"  Typical Activation: {info['activation']}\")\n",
    "            print(f\"  Examples:\")\n",
    "            for example in info['examples']:\n",
    "                print(f\"    ‚Ä¢ {example}\")\n",
    "\n",
    "# Create analyzer and display reference\n",
    "analyzer = ProblemAnalyzer()\n",
    "analyzer.print_problem_types()\n",
    "\n",
    "print(\"\\nüí° Key Questions for Problem Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "questions = [\n",
    "    \"What type of output do we need? (Categories vs. Numbers vs. Groups)\",\n",
    "    \"What data is available? (Structured vs. Unstructured)\",\n",
    "    \"How much data do we have? (Sample size considerations)\",\n",
    "    \"What is the business objective? (Accuracy vs. Speed vs. Interpretability)\",\n",
    "    \"Are there any constraints? (Real-time, memory, computational resources)\",\n",
    "    \"How will success be measured? (What metrics matter?)\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Case Study Analysis\n",
    "\n",
    "Let's analyze several real-world case studies to understand problem characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 2: REAL-WORLD CASE STUDIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define case studies\n",
    "case_studies = [\n",
    "    {\n",
    "        'title': 'Email Spam Detection',\n",
    "        'description': 'Classify incoming emails as spam or not spam based on content and metadata',\n",
    "        'data_type': 'Text and numerical features',\n",
    "        'sample_size': '10,000 - 1,000,000+ emails',\n",
    "        'business_goal': 'Protect users from spam while minimizing false positives',\n",
    "        'constraints': 'Real-time processing, low false positive rate critical'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Medical Image Analysis',\n",
    "        'description': 'Detect tumors in medical X-rays or MRI scans',\n",
    "        'data_type': 'High-resolution images',\n",
    "        'sample_size': '1,000 - 100,000 labeled images',\n",
    "        'business_goal': 'Assist doctors in early disease detection',\n",
    "        'constraints': 'Extremely high accuracy required, interpretability important'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Stock Price Prediction',\n",
    "        'description': 'Predict future stock prices based on historical data and market indicators',\n",
    "        'data_type': 'Time series numerical data',\n",
    "        'sample_size': 'Years of historical data',\n",
    "        'business_goal': 'Inform investment decisions',\n",
    "        'constraints': 'Real-time processing, highly volatile and noisy data'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Autonomous Vehicle Object Detection',\n",
    "        'description': 'Identify and classify objects (cars, pedestrians, signs) in real-time video',\n",
    "        'data_type': 'Video streams from multiple cameras',\n",
    "        'sample_size': 'Millions of labeled frames',\n",
    "        'business_goal': 'Enable safe autonomous driving',\n",
    "        'constraints': 'Real-time processing, safety-critical, multiple object types'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Customer Recommendation System',\n",
    "        'description': 'Recommend products to customers based on purchase history and preferences',\n",
    "        'data_type': 'User behavior data, product features',\n",
    "        'sample_size': 'Millions of users and products',\n",
    "        'business_goal': 'Increase sales and customer satisfaction',\n",
    "        'constraints': 'Scalability, cold start problem, privacy considerations'\n",
    "    }\n",
    "]\n",
    "\n",
    "def analyze_case_study(case):\n",
    "    \"\"\"Detailed analysis of a case study\"\"\"\n",
    "    print(f\"\\nüéØ CASE STUDY: {case['title']}\")\n",
    "    print(\"=\" * (len(case['title']) + 15))\n",
    "    \n",
    "    print(f\"Problem: {case['description']}\")\n",
    "    print(f\"Data Type: {case['data_type']}\")\n",
    "    print(f\"Sample Size: {case['sample_size']}\")\n",
    "    print(f\"Business Goal: {case['business_goal']}\")\n",
    "    print(f\"Constraints: {case['constraints']}\")\n",
    "    \n",
    "    # Analyze problem characteristics\n",
    "    analysis = analyzer.analyze_problem(case['description'])\n",
    "    \n",
    "    print(f\"\\nANALYSIS RESULTS:\")\n",
    "    print(f\"Problem Type: {analysis['problem_type'].upper() if analysis['problem_type'] else 'UNCLEAR'}\")\n",
    "    \n",
    "    # Add specific recommendations based on case study\n",
    "    if case['title'] == 'Email Spam Detection':\n",
    "        print(\"Recommended Architecture: Feedforward network or SVM\")\n",
    "        print(\"Key Features: Word frequencies, sender reputation, email metadata\")\n",
    "        print(\"Challenges: Evolving spam tactics, language variations\")\n",
    "        print(\"Preprocessing: Text tokenization, feature vectorization, normalization\")\n",
    "        \n",
    "    elif case['title'] == 'Medical Image Analysis':\n",
    "        print(\"Recommended Architecture: Convolutional Neural Network (CNN)\")\n",
    "        print(\"Key Features: Image pixels, texture patterns, shape features\")\n",
    "        print(\"Challenges: Limited labeled data, class imbalance, regulatory approval\")\n",
    "        print(\"Preprocessing: Image normalization, augmentation, region of interest extraction\")\n",
    "        \n",
    "    elif case['title'] == 'Stock Price Prediction':\n",
    "        print(\"Recommended Architecture: Recurrent Neural Network (RNN/LSTM)\")\n",
    "        print(\"Key Features: Historical prices, volume, technical indicators\")\n",
    "        print(\"Challenges: Market volatility, non-stationarity, external factors\")\n",
    "        print(\"Preprocessing: Time series normalization, feature engineering, windowing\")\n",
    "        \n",
    "    elif case['title'] == 'Autonomous Vehicle Object Detection':\n",
    "        print(\"Recommended Architecture: Convolutional Neural Network with object detection\")\n",
    "        print(\"Key Features: Image pixels, depth information, motion vectors\")\n",
    "        print(\"Challenges: Real-time processing, varying lighting, safety requirements\")\n",
    "        print(\"Preprocessing: Image preprocessing, data augmentation, multi-scale detection\")\n",
    "        \n",
    "    elif case['title'] == 'Customer Recommendation System':\n",
    "        print(\"Recommended Architecture: Collaborative filtering or deep embeddings\")\n",
    "        print(\"Key Features: User history, product features, ratings\")\n",
    "        print(\"Challenges: Sparsity, scalability, cold start problem\")\n",
    "        print(\"Preprocessing: User-item matrix, feature encoding, dimensionality reduction\")\n",
    "\n",
    "# Analyze each case study\n",
    "for case in case_studies:\n",
    "    analyze_case_study(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Analysis and Preprocessing\n",
    "\n",
    "Let's explore different types of data and their preprocessing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 3: DATA ANALYSIS AND PREPROCESSING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generate sample datasets for different problem types\n",
    "def generate_sample_datasets():\n",
    "    \"\"\"Generate sample datasets for analysis\"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Classification dataset\n",
    "    X_class, y_class = make_classification(n_samples=1000, n_features=20, \n",
    "                                          n_informative=10, n_redundant=10,\n",
    "                                          n_classes=3, random_state=42)\n",
    "    datasets['classification'] = (X_class, y_class)\n",
    "    \n",
    "    # 2. Regression dataset\n",
    "    X_reg, y_reg = make_regression(n_samples=1000, n_features=10, \n",
    "                                  noise=0.1, random_state=42)\n",
    "    datasets['regression'] = (X_reg, y_reg)\n",
    "    \n",
    "    # 3. Non-linear classification (circles)\n",
    "    X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, \n",
    "                                       factor=0.6, random_state=42)\n",
    "    datasets['nonlinear'] = (X_circles, y_circles)\n",
    "    \n",
    "    # 4. Simulated time series data\n",
    "    t = np.linspace(0, 10, 1000)\n",
    "    ts_data = np.sin(2 * np.pi * t) + 0.5 * np.sin(4 * np.pi * t) + 0.1 * np.random.randn(1000)\n",
    "    datasets['timeseries'] = (t.reshape(-1, 1), ts_data)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "datasets = generate_sample_datasets()\n",
    "\n",
    "def analyze_dataset(name, X, y, dataset_info):\n",
    "    \"\"\"Analyze dataset characteristics\"\"\"\n",
    "    print(f\"\\nüìä DATASET: {name.upper()}\")\n",
    "    print(\"=\" * (len(name) + 12))\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Data type: {X.dtype}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nData Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(X, axis=0)[:3]}...\" if X.shape[1] > 3 else f\"  Mean: {np.mean(X, axis=0)}\")\n",
    "    print(f\"  Std:  {np.std(X, axis=0)[:3]}...\" if X.shape[1] > 3 else f\"  Std: {np.std(X, axis=0)}\")\n",
    "    print(f\"  Min:  {np.min(X, axis=0)[:3]}...\" if X.shape[1] > 3 else f\"  Min: {np.min(X, axis=0)}\")\n",
    "    print(f\"  Max:  {np.max(X, axis=0)[:3]}...\" if X.shape[1] > 3 else f\"  Max: {np.max(X, axis=0)}\")\n",
    "    \n",
    "    # Target statistics\n",
    "    if len(np.unique(y)) < 10:  # Likely classification\n",
    "        unique_values, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"\\nTarget Distribution:\")\n",
    "        for val, count in zip(unique_values, counts):\n",
    "            print(f\"  Class {val}: {count} samples ({count/len(y)*100:.1f}%)\")\n",
    "    else:  # Likely regression\n",
    "        print(f\"\\nTarget Statistics:\")\n",
    "        print(f\"  Mean: {np.mean(y):.3f}\")\n",
    "        print(f\"  Std:  {np.std(y):.3f}\")\n",
    "        print(f\"  Min:  {np.min(y):.3f}\")\n",
    "        print(f\"  Max:  {np.max(y):.3f}\")\n",
    "    \n",
    "    # Preprocessing recommendations\n",
    "    print(f\"\\nPreprocessing Recommendations:\")\n",
    "    \n",
    "    # Check for different scales\n",
    "    feature_ranges = np.max(X, axis=0) - np.min(X, axis=0)\n",
    "    max_range = np.max(feature_ranges)\n",
    "    min_range = np.min(feature_ranges)\n",
    "    \n",
    "    if max_range / min_range > 10:\n",
    "        print(f\"  ‚ö†Ô∏è  Features have different scales - consider normalization\")\n",
    "        print(f\"     Range ratio: {max_range/min_range:.1f}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Feature scales are reasonably similar\")\n",
    "    \n",
    "    # Check for missing values (simulated)\n",
    "    if np.random.random() < 0.3:  # Randomly simulate missing data\n",
    "        print(f\"  ‚ö†Ô∏è  Dataset may have missing values - check and handle\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No missing values detected\")\n",
    "    \n",
    "    # Problem-specific recommendations\n",
    "    if name == 'classification':\n",
    "        print(f\"  ‚Ä¢ Standard scaling or normalization recommended\")\n",
    "        print(f\"  ‚Ä¢ Consider feature selection for high-dimensional data\")\n",
    "        print(f\"  ‚Ä¢ Check class balance for imbalanced datasets\")\n",
    "    elif name == 'regression':\n",
    "        print(f\"  ‚Ä¢ Standard scaling recommended\")\n",
    "        print(f\"  ‚Ä¢ Check for outliers in target variable\")\n",
    "        print(f\"  ‚Ä¢ Consider target transformation if skewed\")\n",
    "    elif name == 'nonlinear':\n",
    "        print(f\"  ‚Ä¢ Standard scaling recommended\")\n",
    "        print(f\"  ‚Ä¢ Visualization important for understanding decision boundary\")\n",
    "        print(f\"  ‚Ä¢ May need deeper networks for complex boundaries\")\n",
    "    elif name == 'timeseries':\n",
    "        print(f\"  ‚Ä¢ Check for stationarity\")\n",
    "        print(f\"  ‚Ä¢ Consider windowing for sequence prediction\")\n",
    "        print(f\"  ‚Ä¢ Normalization within windows may be needed\")\n",
    "\n",
    "# Analyze each dataset\n",
    "dataset_info = {\n",
    "    'classification': 'Multi-class classification with mixed informative/redundant features',\n",
    "    'regression': 'Regression with continuous target and multiple features',\n",
    "    'nonlinear': 'Binary classification with circular decision boundary',\n",
    "    'timeseries': 'Time series data with periodic components and noise'\n",
    "}\n",
    "\n",
    "for name, (X, y) in datasets.items():\n",
    "    analyze_dataset(name, X, y, dataset_info[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visual Data Exploration\n",
    "\n",
    "Let's create visualizations to better understand our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"PART 4: VISUAL DATA EXPLORATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Classification data - first two features\n",
    "X_class, y_class = datasets['classification']\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['red', 'blue', 'green']\n",
    "for class_idx in range(3):\n",
    "    mask = y_class == class_idx\n",
    "    ax1.scatter(X_class[mask, 0], X_class[mask, 1], \n",
    "               c=colors[class_idx], label=f'Class {class_idx}', alpha=0.6)\n",
    "ax1.set_title('Classification Data\\n(First 2 Features)')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Regression data - feature vs target\n",
    "X_reg, y_reg = datasets['regression']\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(X_reg[:, 0], y_reg, alpha=0.6, color='purple')\n",
    "ax2.set_title('Regression Data\\n(Feature 1 vs Target)')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Target Value')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Non-linear data\n",
    "X_circles, y_circles = datasets['nonlinear']\n",
    "ax3 = axes[0, 2]\n",
    "colors = ['red', 'blue']\n",
    "for class_idx in range(2):\n",
    "    mask = y_circles == class_idx\n",
    "    ax3.scatter(X_circles[mask, 0], X_circles[mask, 1], \n",
    "               c=colors[class_idx], label=f'Class {class_idx}', alpha=0.6)\n",
    "ax3.set_title('Non-linear Classification\\n(Circular Boundary)')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "# 4. Time series data\n",
    "t, ts_data = datasets['timeseries']\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(t[:200], ts_data[:200], 'b-', linewidth=1.5)  # Show first 200 points\n",
    "ax4.set_title('Time Series Data\\n(First 200 Points)')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Value')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Feature distribution for classification data\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(X_class[:, 0], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax5.set_title('Feature Distribution\\n(Classification Feature 1)')\n",
    "ax5.set_xlabel('Feature Value')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Correlation analysis\n",
    "ax6 = axes[1, 2]\n",
    "# Calculate correlation matrix for first 5 features of classification data\n",
    "corr_matrix = np.corrcoef(X_class[:, :5].T)\n",
    "im = ax6.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax6.set_title('Feature Correlation Matrix\\n(First 5 Features)')\n",
    "ax6.set_xlabel('Feature')\n",
    "ax6.set_ylabel('Feature')\n",
    "\n",
    "# Add correlation values to heatmap\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        text = ax6.text(j, i, f'{corr_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax6, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Visual Analysis Insights:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"1. Classification data shows separable classes in 2D projection\")\n",
    "print(\"2. Regression data shows relationship between features and target\")\n",
    "print(\"3. Non-linear data requires non-linear decision boundary\")\n",
    "print(\"4. Time series shows periodic patterns and noise\")\n",
    "print(\"5. Feature distributions help identify outliers and skewness\")\n",
    "print(\"6. Correlation matrix reveals feature relationships\")\n",
    "\n",
    "print(\"\\nüí° Visualization Benefits:\")\n",
    "print(\"-\" * 25)\n",
    "benefits = [\n",
    "    \"Identify data quality issues (outliers, missing values)\",\n",
    "    \"Understand feature relationships and correlations\",\n",
    "    \"Assess problem complexity and separability\",\n",
    "    \"Guide preprocessing decisions\",\n",
    "    \"Inform architecture choices\",\n",
    "    \"Detect class imbalance or bias\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(f\"‚Ä¢ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Architecture Selection Guidelines\n",
    "\n",
    "Let's establish guidelines for selecting appropriate network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"PART 5: NETWORK ARCHITECTURE SELECTION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class ArchitectureSelector:\n",
    "    \"\"\"Helper class for selecting network architectures\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.architecture_guide = {\n",
    "            'feedforward': {\n",
    "                'description': 'Basic fully connected layers',\n",
    "                'best_for': ['Tabular data', 'Simple classification', 'Simple regression'],\n",
    "                'pros': ['Simple', 'Fast training', 'Good baseline'],\n",
    "                'cons': ['Limited for complex patterns', 'No spatial awareness'],\n",
    "                'typical_size': '1-3 hidden layers, 10-1000 neurons per layer'\n",
    "            },\n",
    "            'convolutional': {\n",
    "                'description': 'Convolutional Neural Networks (CNNs)',\n",
    "                'best_for': ['Image data', 'Spatial patterns', '2D/3D data'],\n",
    "                'pros': ['Spatial awareness', 'Translation invariant', 'Parameter sharing'],\n",
    "                'cons': ['More complex', 'Requires more data', 'Computationally intensive'],\n",
    "                'typical_size': 'Multiple conv layers + pooling + dense layers'\n",
    "            },\n",
    "            'recurrent': {\n",
    "                'description': 'Recurrent Neural Networks (RNNs/LSTMs)',\n",
    "                'best_for': ['Sequential data', 'Time series', 'Natural language'],\n",
    "                'pros': ['Memory of past inputs', 'Variable length sequences'],\n",
    "                'cons': ['Vanishing gradients', 'Sequential processing', 'Complex training'],\n",
    "                'typical_size': '1-3 LSTM layers, 50-500 units per layer'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def recommend_architecture(self, problem_type, data_characteristics):\n",
    "        \"\"\"Recommend architecture based on problem and data\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Simple heuristic rules\n",
    "        if 'tabular' in data_characteristics or 'structured' in data_characteristics:\n",
    "            recommendations.append(('feedforward', 0.9))\n",
    "        \n",
    "        if 'image' in data_characteristics or 'spatial' in data_characteristics:\n",
    "            recommendations.append(('convolutional', 0.95))\n",
    "        \n",
    "        if 'sequential' in data_characteristics or 'time_series' in data_characteristics:\n",
    "            recommendations.append(('recurrent', 0.9))\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(('feedforward', 0.8))  # Default\n",
    "        \n",
    "        return sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def size_network(self, n_samples, n_features, problem_complexity='medium'):\n",
    "        \"\"\"Suggest network size based on data characteristics\"\"\"\n",
    "        \n",
    "        complexity_multipliers = {\n",
    "            'simple': 0.5,\n",
    "            'medium': 1.0,\n",
    "            'complex': 2.0\n",
    "        }\n",
    "        \n",
    "        multiplier = complexity_multipliers.get(problem_complexity, 1.0)\n",
    "        \n",
    "        # Rule of thumb: start with network size proportional to input size\n",
    "        suggested_sizes = {\n",
    "            'small': int(n_features * multiplier),\n",
    "            'medium': int(n_features * 2 * multiplier),\n",
    "            'large': int(n_features * 4 * multiplier)\n",
    "        }\n",
    "        \n",
    "        # Adjust based on sample size\n",
    "        if n_samples < 1000:\n",
    "            for key in suggested_sizes:\n",
    "                suggested_sizes[key] = max(10, suggested_sizes[key] // 2)\n",
    "        elif n_samples > 10000:\n",
    "            for key in suggested_sizes:\n",
    "                suggested_sizes[key] = min(1000, suggested_sizes[key] * 2)\n",
    "        \n",
    "        return suggested_sizes\n",
    "\n",
    "selector = ArchitectureSelector()\n",
    "\n",
    "# Display architecture guide\n",
    "print(\"\\nüèóÔ∏è ARCHITECTURE SELECTION GUIDE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for arch_type, info in selector.architecture_guide.items():\n",
    "    print(f\"\\n{arch_type.upper()}:\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(f\"  Best for: {', '.join(info['best_for'])}\")\n",
    "    print(f\"  Pros: {', '.join(info['pros'])}\")\n",
    "    print(f\"  Cons: {', '.join(info['cons'])}\")\n",
    "    print(f\"  Typical size: {info['typical_size']}\")\n",
    "\n",
    "# Apply to our datasets\n",
    "print(\"\\n\\nüìã ARCHITECTURE RECOMMENDATIONS FOR OUR DATASETS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "dataset_characteristics = {\n",
    "    'classification': ['tabular', 'structured', 'high_dimensional'],\n",
    "    'regression': ['tabular', 'structured', 'continuous_target'],\n",
    "    'nonlinear': ['tabular', 'structured', 'non_linear_boundary'],\n",
    "    'timeseries': ['sequential', 'time_series', 'temporal_patterns']\n",
    "}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\n{dataset_name.upper()} DATASET:\")\n",
    "    print(\"-\" * (len(dataset_name) + 9))\n",
    "    \n",
    "    characteristics = dataset_characteristics[dataset_name]\n",
    "    print(f\"Characteristics: {', '.join(characteristics)}\")\n",
    "    \n",
    "    # Get architecture recommendations\n",
    "    recommendations = selector.recommend_architecture('classification', characteristics)\n",
    "    print(f\"Recommended architectures:\")\n",
    "    for arch, confidence in recommendations:\n",
    "        print(f\"  {arch.title()}: {confidence:.1%} confidence\")\n",
    "    \n",
    "    # Get size recommendations\n",
    "    n_samples, n_features = X.shape\n",
    "    sizes = selector.size_network(n_samples, n_features)\n",
    "    print(f\"Suggested layer sizes:\")\n",
    "    for size_type, size in sizes.items():\n",
    "        print(f\"  {size_type.title()}: {size} neurons\")\n",
    "    \n",
    "    print(f\"\\nSpecific suggestions:\")\n",
    "    if dataset_name == 'classification':\n",
    "        print(f\"  ‚Ä¢ Start with 2 hidden layers: {sizes['medium']}, {sizes['small']}\")\n",
    "        print(f\"  ‚Ä¢ Use ReLU activation for hidden layers\")\n",
    "        print(f\"  ‚Ä¢ Use softmax activation for output (3 classes)\")\n",
    "    elif dataset_name == 'regression':\n",
    "        print(f\"  ‚Ä¢ Start with 2 hidden layers: {sizes['medium']}, {sizes['small']}\")\n",
    "        print(f\"  ‚Ä¢ Use ReLU activation for hidden layers\")\n",
    "        print(f\"  ‚Ä¢ Use linear activation for output\")\n",
    "    elif dataset_name == 'nonlinear':\n",
    "        print(f\"  ‚Ä¢ May need deeper network: 3 layers of {sizes['medium']}, {sizes['medium']}, {sizes['small']}\")\n",
    "        print(f\"  ‚Ä¢ Use ReLU or tanh activation\")\n",
    "        print(f\"  ‚Ä¢ Use sigmoid activation for binary output\")\n",
    "    elif dataset_name == 'timeseries':\n",
    "        print(f\"  ‚Ä¢ Consider LSTM with {sizes['medium']} units\")\n",
    "        print(f\"  ‚Ä¢ Use sequence-to-one prediction\")\n",
    "        print(f\"  ‚Ä¢ Consider windowing approach\")\n",
    "\n",
    "print(\"\\n\\n‚öñÔ∏è ARCHITECTURE SELECTION PRINCIPLES\")\n",
    "print(\"=\" * 40)\n",
    "principles = [\n",
    "    \"Start simple and increase complexity as needed\",\n",
    "    \"Match architecture to data structure (spatial, sequential, tabular)\",\n",
    "    \"Consider computational constraints and real-time requirements\",\n",
    "    \"Use domain knowledge to guide architecture choices\",\n",
    "    \"Experiment with different sizes and depths\",\n",
    "    \"Monitor for overfitting as complexity increases\"\n",
    "]\n",
    "\n",
    "for i, principle in enumerate(principles, 1):\n",
    "    print(f\"{i}. {principle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Common Challenges and Solutions\n",
    "\n",
    "Let's identify common real-world challenges and their solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"PART 6: REAL-WORLD CHALLENGES & SOLUTIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "challenges_and_solutions = {\n",
    "    'Limited Data': {\n",
    "        'description': 'Not enough labeled data for training',\n",
    "        'indicators': ['Small dataset (< 1000 samples)', 'High-dimensional features', 'Poor model performance'],\n",
    "        'solutions': [\n",
    "            'Data augmentation techniques',\n",
    "            'Transfer learning from pre-trained models',\n",
    "            'Regularization (dropout, L1/L2)',\n",
    "            'Simpler model architectures',\n",
    "            'Cross-validation for better evaluation'\n",
    "        ],\n",
    "        'prevention': 'Collect more data, use synthetic data generation'\n",
    "    },\n",
    "    'Class Imbalance': {\n",
    "        'description': 'Unequal distribution of classes in dataset',\n",
    "        'indicators': ['Skewed class distribution', 'High accuracy but poor recall', 'Model bias toward majority class'],\n",
    "        'solutions': [\n",
    "            'Resampling techniques (SMOTE, undersampling)',\n",
    "            'Class weights in loss function',\n",
    "            'Ensemble methods',\n",
    "            'Different evaluation metrics (F1, precision, recall)',\n",
    "            'Anomaly detection approaches for extreme imbalance'\n",
    "        ],\n",
    "        'prevention': 'Stratified sampling, balanced data collection'\n",
    "    },\n",
    "    'Overfitting': {\n",
    "        'description': 'Model performs well on training but poorly on test data',\n",
    "        'indicators': ['Large gap between train/validation accuracy', 'High variance in performance', 'Complex model with limited data'],\n",
    "        'solutions': [\n",
    "            'Regularization techniques (dropout, L1/L2)',\n",
    "            'Early stopping based on validation loss',\n",
    "            'Cross-validation',\n",
    "            'Reduce model complexity',\n",
    "            'More training data'\n",
    "        ],\n",
    "        'prevention': 'Proper validation strategy, simpler models initially'\n",
    "    },\n",
    "    'Poor Feature Quality': {\n",
    "        'description': 'Features are not informative or contain noise',\n",
    "        'indicators': ['Random performance', 'No learning progress', 'High feature correlation'],\n",
    "        'solutions': [\n",
    "            'Feature selection and engineering',\n",
    "            'Dimensionality reduction (PCA, t-SNE)',\n",
    "            'Domain expertise for feature creation',\n",
    "            'Exploratory data analysis',\n",
    "            'Automated feature selection'\n",
    "        ],\n",
    "        'prevention': 'Domain knowledge, thorough EDA, feature importance analysis'\n",
    "    },\n",
    "    'Scalability Issues': {\n",
    "        'description': 'Model cannot handle production data volumes or speed',\n",
    "        'indicators': ['Slow inference time', 'Memory errors', 'Cannot handle real-time requests'],\n",
    "        'solutions': [\n",
    "            'Model compression and pruning',\n",
    "            'Quantization to lower precision',\n",
    "            'Distributed computing',\n",
    "            'Model serving optimization',\n",
    "            'Simpler architectures for production'\n",
    "        ],\n",
    "        'prevention': 'Consider scalability early, benchmark on realistic data'\n",
    "    },\n",
    "    'Data Drift': {\n",
    "        'description': 'Data distribution changes over time in production',\n",
    "        'indicators': ['Declining model performance', 'Different feature distributions', 'New data patterns'],\n",
    "        'solutions': [\n",
    "            'Continuous monitoring and retraining',\n",
    "            'Adaptive learning systems',\n",
    "            'A/B testing for model updates',\n",
    "            'Data validation pipelines',\n",
    "            'Ensemble methods for robustness'\n",
    "        ],\n",
    "        'prevention': 'Build monitoring systems, plan for model updates'\n",
    "    }\n",
    "}\n",
    "\n",
    "def display_challenge(challenge_name, challenge_info):\n",
    "    \"\"\"Display detailed information about a challenge\"\"\"\n",
    "    print(f\"\\nüö® CHALLENGE: {challenge_name.upper()}\")\n",
    "    print(\"=\" * (len(challenge_name) + 13))\n",
    "    \n",
    "    print(f\"Description: {challenge_info['description']}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Warning Indicators:\")\n",
    "    for indicator in challenge_info['indicators']:\n",
    "        print(f\"  ‚Ä¢ {indicator}\")\n",
    "    \n",
    "    print(f\"\\nüí° Solutions:\")\n",
    "    for solution in challenge_info['solutions']:\n",
    "        print(f\"  ‚Ä¢ {solution}\")\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è  Prevention: {challenge_info['prevention']}\")\n",
    "\n",
    "# Display all challenges\n",
    "for challenge_name, challenge_info in challenges_and_solutions.items():\n",
    "    display_challenge(challenge_name, challenge_info)\n",
    "\n",
    "print(\"\\n\\nüîß GENERAL PROBLEM-SOLVING WORKFLOW\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "workflow_steps = [\n",
    "    \"1. Define the Problem\",\n",
    "    \"   ‚Ä¢ What exactly are we trying to predict/classify?\",\n",
    "    \"   ‚Ä¢ What does success look like?\",\n",
    "    \"   ‚Ä¢ What are the business constraints?\",\n",
    "    \"\",\n",
    "    \"2. Analyze the Data\",\n",
    "    \"   ‚Ä¢ Explore data distributions and patterns\",\n",
    "    \"   ‚Ä¢ Check for quality issues (missing values, outliers)\",\n",
    "    \"   ‚Ä¢ Understand feature relationships\",\n",
    "    \"\",\n",
    "    \"3. Choose Architecture\",\n",
    "    \"   ‚Ä¢ Match architecture to data type\",\n",
    "    \"   ‚Ä¢ Start simple, increase complexity as needed\",\n",
    "    \"   ‚Ä¢ Consider computational constraints\",\n",
    "    \"\",\n",
    "    \"4. Preprocess Data\",\n",
    "    \"   ‚Ä¢ Handle missing values and outliers\",\n",
    "    \"   ‚Ä¢ Normalize/standardize features\",\n",
    "    \"   ‚Ä¢ Create train/validation/test splits\",\n",
    "    \"\",\n",
    "    \"5. Train and Validate\",\n",
    "    \"   ‚Ä¢ Use appropriate metrics for the problem\",\n",
    "    \"   ‚Ä¢ Monitor for overfitting\",\n",
    "    \"   ‚Ä¢ Tune hyperparameters systematically\",\n",
    "    \"\",\n",
    "    \"6. Deploy and Monitor\",\n",
    "    \"   ‚Ä¢ Test in production-like environment\",\n",
    "    \"   ‚Ä¢ Monitor performance over time\",\n",
    "    \"   ‚Ä¢ Plan for model updates and retraining\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\n\\nüéØ SUCCESS FACTORS FOR REAL-WORLD PROJECTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "success_factors = [\n",
    "    \"Clear problem definition with measurable objectives\",\n",
    "    \"High-quality, representative training data\",\n",
    "    \"Appropriate choice of architecture and algorithms\",\n",
    "    \"Thorough validation and testing procedures\",\n",
    "    \"Consideration of production constraints early\",\n",
    "    \"Strong collaboration between technical and domain experts\",\n",
    "    \"Iterative approach with continuous improvement\",\n",
    "    \"Robust monitoring and maintenance processes\"\n",
    "]\n",
    "\n",
    "for i, factor in enumerate(success_factors, 1):\n",
    "    print(f\"{i}. {factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checklist\n",
    "\n",
    "Mark each concept as understood:\n",
    "\n",
    "- [ ] Framework for classifying machine learning problems\n",
    "- [ ] Analysis of real-world case studies and their characteristics\n",
    "- [ ] Data exploration and preprocessing requirements\n",
    "- [ ] Visual analysis techniques for different data types\n",
    "- [ ] Architecture selection guidelines and principles\n",
    "- [ ] Common real-world challenges and their solutions\n",
    "- [ ] Problem-solving workflow for ML projects\n",
    "- [ ] Success factors for production deployment\n",
    "- [ ] Understanding of business constraints and requirements\n",
    "- [ ] Practical skills for problem analysis and solution design\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues in Real-World Problems:\n",
    "\n",
    "**1. Unclear problem definition:**\n",
    "- Spend time with stakeholders to clarify objectives\n",
    "- Define success metrics before starting\n",
    "- Create simple prototypes to validate understanding\n",
    "\n",
    "**2. Poor data quality:**\n",
    "- Invest significant time in data exploration\n",
    "- Create data quality reports and monitoring\n",
    "- Work with data providers to improve collection\n",
    "\n",
    "**3. Mismatched expectations:**\n",
    "- Communicate limitations and uncertainties clearly\n",
    "- Provide realistic timelines and accuracy expectations\n",
    "- Create proof-of-concepts before full development\n",
    "\n",
    "**4. Technical debt in ML systems:**\n",
    "- Plan for model maintenance and updates\n",
    "- Create reproducible training pipelines\n",
    "- Document assumptions and design decisions\n",
    "\n",
    "**5. Production deployment challenges:**\n",
    "- Test thoroughly in staging environments\n",
    "- Plan for gradual rollouts and A/B testing\n",
    "- Monitor model performance continuously\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "1. **Problem Classification**: Understanding different types of ML problems and their characteristics\n",
    "2. **Data Analysis**: Systematic exploration of data quality, distribution, and patterns\n",
    "3. **Architecture Selection**: Matching network design to problem and data characteristics\n",
    "4. **Challenge Identification**: Recognizing common issues and having solutions ready\n",
    "5. **Workflow Management**: Following systematic approaches for consistent results\n",
    "6. **Business Integration**: Understanding constraints and requirements beyond technical aspects\n",
    "7. **Production Readiness**: Considering deployment and maintenance from the beginning\n",
    "8. **Iterative Improvement**: Building systems that can evolve and improve over time\n",
    "\n",
    "## Real-World Application Skills\n",
    "\n",
    "This lab has prepared you to:\n",
    "- **Analyze** new problems systematically\n",
    "- **Communicate** with non-technical stakeholders\n",
    "- **Design** appropriate solutions for different problem types\n",
    "- **Anticipate** common challenges and plan solutions\n",
    "- **Implement** robust ML systems for production use\n",
    "- **Monitor** and maintain models in production\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll put all of these concepts together to plan a complete course project that demonstrates real-world problem-solving skills.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've developed the analytical skills needed to tackle real-world neural network problems!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
