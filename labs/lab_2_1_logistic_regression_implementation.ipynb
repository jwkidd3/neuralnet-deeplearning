{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1: Logistic Regression Implementation\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the mathematical foundation of logistic regression\n",
    "- Implement logistic regression from scratch using NumPy\n",
    "- Apply sigmoid activation function in practice\n",
    "- Understand the relationship between linear and logistic regression\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Jupyter Notebook/Lab\n",
    "\n",
    "## Overview\n",
    "Logistic regression is a fundamental algorithm for binary classification. Unlike linear regression which predicts continuous values, logistic regression predicts probabilities between 0 and 1 using the sigmoid function. This lab will guide you through implementing logistic regression from scratch to build a solid foundation for understanding neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the Sigmoid Function\n",
    "\n",
    "The sigmoid function is the heart of logistic regression. It maps any real number to a value between 0 and 1, making it perfect for probability estimation.\n",
    "\n",
    "**Mathematical Formula:** Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "Let's implement and visualize the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "    \n",
    "    Returns:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    # Implement sigmoid function\n",
    "    # Tip: Use np.exp() for exponential function\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Test the sigmoid function\n",
    "test_values = np.array([-10, -5, -2, 0, 2, 5, 10])\n",
    "sigmoid_results = sigmoid(test_values)\n",
    "\n",
    "print(\"Input values:\", test_values)\n",
    "print(\"Sigmoid outputs:\", sigmoid_results)\n",
    "print(\"\\nKey observations:\")\n",
    "print(f\"- sigmoid(0) = {sigmoid(0):.3f}\")\n",
    "print(f\"- As z â†’ âˆž, sigmoid(z) â†’ {sigmoid(100):.6f}\")\n",
    "print(f\"- As z â†’ -âˆž, sigmoid(z) â†’ {sigmoid(-100):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "s = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, s, 'b-', linewidth=2, label='Sigmoid Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output (Ïƒ(z))')\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Sigmoid function implemented and visualized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Sample Dataset\n",
    "\n",
    "Let's create a binary classification dataset to work with. We'll use scikit-learn's `make_classification` function to generate a 2D dataset that's perfect for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary classification dataset\n",
    "def generate_dataset(n_samples=1000, n_features=2, noise=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate a binary classification dataset\n",
    "    \n",
    "    Arguments:\n",
    "    n_samples -- number of samples to generate\n",
    "    n_features -- number of features (we'll use 2 for visualization)\n",
    "    noise -- amount of noise to add\n",
    "    random_state -- random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    X -- feature matrix (n_samples, n_features)\n",
    "    y -- target vector (n_samples,)\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_redundant=0,\n",
    "        n_informative=2,\n",
    "        n_clusters_per_class=1,\n",
    "        noise=noise,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_dataset()\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset generated successfully!\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution in training: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution in test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot training data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "           c='red', marker='o', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "           c='blue', marker='s', alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], \n",
    "           c='red', marker='o', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], \n",
    "           c='blue', marker='s', alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Dataset visualized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Logistic Regression Class\n",
    "\n",
    "Now let's implement a complete logistic regression class from scratch. This will include:\n",
    "- Forward propagation (prediction)\n",
    "- Cost function computation\n",
    "- Backward propagation (gradients)\n",
    "- Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression implementation from scratch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initialize the logistic regression model\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate -- step size for gradient descent\n",
    "        max_iterations -- maximum number of training iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "    \n",
    "    def _add_bias_column(self, X):\n",
    "        \"\"\"\n",
    "        Add bias column to the feature matrix\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "        X_with_bias -- features with bias column (m, n+1)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        bias_column = np.ones((m, 1))\n",
    "        return np.concatenate([bias_column, X], axis=1)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function\n",
    "        \n",
    "        Arguments:\n",
    "        z -- input to sigmoid function\n",
    "        \n",
    "        Returns:\n",
    "        sigmoid output\n",
    "        \"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "        A -- predictions (probabilities) (m, 1)\n",
    "        \"\"\"\n",
    "        # Add bias column\n",
    "        X_with_bias = self._add_bias_column(X)\n",
    "        \n",
    "        # Compute linear combination: z = X * weights\n",
    "        z = np.dot(X_with_bias, self.weights)\n",
    "        \n",
    "        # Apply sigmoid activation\n",
    "        A = self._sigmoid(z)\n",
    "        \n",
    "        return A, X_with_bias\n",
    "    \n",
    "    def _compute_cost(self, A, y):\n",
    "        \"\"\"\n",
    "        Compute the logistic regression cost function\n",
    "        \n",
    "        Arguments:\n",
    "        A -- predictions (m, 1)\n",
    "        y -- true labels (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "        cost -- logistic regression cost\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        # Ensure A is in valid range to prevent log(0)\n",
    "        A = np.clip(A, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # Compute cost: J = -1/m * sum(y*log(A) + (1-y)*log(1-A))\n",
    "        cost = -1/m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def _backward_propagation(self, X_with_bias, A, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients\n",
    "        \n",
    "        Arguments:\n",
    "        X_with_bias -- input features with bias (m, n+1)\n",
    "        A -- predictions (m, 1)\n",
    "        y -- true labels (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "        dw -- gradients of weights (n+1, 1)\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        # Compute gradients: dw = 1/m * X^T * (A - y)\n",
    "        dw = 1/m * np.dot(X_with_bias.T, (A - y))\n",
    "        \n",
    "        return dw\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        \n",
    "        Arguments:\n",
    "        X -- training features (m, n)\n",
    "        y -- training labels (m,) or (m, 1)\n",
    "        \"\"\"\n",
    "        # Convert y to column vector if necessary\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros((n_features + 1, 1))  # +1 for bias\n",
    "        self.costs = []\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward propagation\n",
    "            A, X_with_bias = self._forward_propagation(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self._compute_cost(A, y)\n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dw = self._backward_propagation(X_with_bias, A, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "        probabilities -- predicted probabilities (m, 1)\n",
    "        \"\"\"\n",
    "        A, _ = self._forward_propagation(X)\n",
    "        return A\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make binary predictions\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input features (m, n)\n",
    "        threshold -- decision threshold\n",
    "        \n",
    "        Returns:\n",
    "        predictions -- binary predictions (m, 1)\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "\n",
    "print(\"âœ“ LogisticRegression class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "\n",
    "Now let's train our logistic regression model on the dataset we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression(learning_rate=0.1, max_iterations=1000)\n",
    "\n",
    "print(\"Training logistic regression model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Display final parameters\n",
    "print(f\"\\nFinal model parameters:\")\n",
    "print(f\"Bias (w0): {model.weights[0, 0]:.4f}\")\n",
    "print(f\"Weight 1 (w1): {model.weights[1, 0]:.4f}\")\n",
    "print(f\"Weight 2 (w2): {model.weights[2, 0]:.4f}\")\n",
    "print(f\"Final cost: {model.costs[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.costs, 'b-', linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training Cost Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training cost visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "train_probabilities = model.predict_proba(X_train)\n",
    "test_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "train_accuracy = calculate_accuracy(y_train, train_predictions)\n",
    "test_accuracy = calculate_accuracy(y_test, test_predictions)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Display some example predictions\n",
    "print(\"\\nExample Predictions (first 10 test samples):\")\n",
    "print(\"=\" * 50)\n",
    "print(\"True Label | Predicted | Probability\")\n",
    "print(\"-\" * 35)\n",
    "for i in range(10):\n",
    "    true_label = y_test[i]\n",
    "    pred_label = test_predictions[i, 0]\n",
    "    probability = test_probabilities[i, 0]\n",
    "    print(f\"    {true_label}      |     {pred_label}     |   {probability:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Decision Boundary\n",
    "\n",
    "Let's visualize how our logistic regression model separates the two classes by plotting the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of the logistic regression model\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input features (m, 2)\n",
    "    y -- true labels (m,)\n",
    "    model -- trained logistic regression model\n",
    "    title -- plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Create a mesh for plotting decision boundary\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict_proba(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and predictions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', \n",
    "               s=50, alpha=0.8, label='Class 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', \n",
    "               s=50, alpha=0.8, label='Class 1')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'{title} - Probability Map')\n",
    "    plt.legend()\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    \n",
    "    # Plot binary decision boundary\n",
    "    plt.subplot(1, 2, 2)\n",
    "    Z_binary = (Z >= 0.5).astype(int)\n",
    "    plt.contourf(xx, yy, Z_binary, levels=1, alpha=0.3, colors=['red', 'blue'])\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', \n",
    "               s=50, alpha=0.8, label='Class 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', \n",
    "               s=50, alpha=0.8, label='Class 1')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'{title} - Binary Classification')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for test data\n",
    "plot_decision_boundary(X_test, y_test, model, \"Test Set\")\n",
    "print(\"âœ“ Decision boundary visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Understanding the Mathematics\n",
    "\n",
    "Let's dive deeper into the mathematical concepts behind logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the decision boundary equation\n",
    "print(\"Mathematical Analysis of Our Trained Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "w0, w1, w2 = model.weights[0, 0], model.weights[1, 0], model.weights[2, 0]\n",
    "\n",
    "print(f\"Decision boundary equation: {w0:.3f} + {w1:.3f}*x1 + {w2:.3f}*x2 = 0\")\n",
    "print(f\"Rearranged: x2 = {-w0/w2:.3f} + {-w1/w2:.3f}*x1\")\n",
    "print(f\"\\nSlope of decision boundary: {-w1/w2:.3f}\")\n",
    "print(f\"Y-intercept of decision boundary: {-w0/w2:.3f}\")\n",
    "\n",
    "# Calculate odds ratio\n",
    "print(f\"\\nWeight interpretation:\")\n",
    "print(f\"- For 1 unit increase in Feature 1, odds multiply by: {np.exp(w1):.3f}\")\n",
    "print(f\"- For 1 unit increase in Feature 2, odds multiply by: {np.exp(w2):.3f}\")\n",
    "\n",
    "# Demonstrate prediction for a new point\n",
    "new_point = np.array([[0, 0]])\n",
    "prob = model.predict_proba(new_point)[0, 0]\n",
    "pred = model.predict(new_point)[0, 0]\n",
    "\n",
    "print(f\"\\nPrediction for point (0, 0):\")\n",
    "print(f\"Probability of Class 1: {prob:.3f}\")\n",
    "print(f\"Predicted Class: {pred}\")\n",
    "print(f\"Odds: {prob/(1-prob):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare with Different Learning Rates\n",
    "\n",
    "Let's explore how different learning rates affect the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training with different learning rates\n",
    "models = {}\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    \n",
    "    model_lr = LogisticRegression(learning_rate=lr, max_iterations=500)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    models[lr] = model_lr\n",
    "    \n",
    "    # Plot cost curve\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(model_lr.costs, color=colors[i], label=f'LR = {lr}', linewidth=2)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "\n",
    "# Compare accuracies\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_lr = models[lr]\n",
    "    train_acc = calculate_accuracy(y_train, model_lr.predict(X_train))\n",
    "    test_acc = calculate_accuracy(y_test, model_lr.predict(X_test))\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "x_pos = np.arange(len(learning_rates))\n",
    "plt.bar(x_pos - 0.2, train_accuracies, 0.4, label='Train', alpha=0.7, color='blue')\n",
    "plt.bar(x_pos + 0.2, test_accuracies, 0.4, label='Test', alpha=0.7, color='red')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy vs Learning Rate')\n",
    "plt.xticks(x_pos, learning_rates)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final costs comparison\n",
    "final_costs = [models[lr].costs[-1] for lr in learning_rates]\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(learning_rates, final_costs, alpha=0.7, color='green')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Cost')\n",
    "plt.title('Final Training Cost vs Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLearning Rate Comparison Results:\")\n",
    "print(\"=\" * 40)\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"LR = {lr:4.2f}: Train Acc = {train_accuracies[i]:5.1f}%, \"\n",
    "          f\"Test Acc = {test_accuracies[i]:5.1f}%, Final Cost = {final_costs[i]:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Learning rate comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] âœ… **Environment Setup**: Imported libraries and set up environment\n",
    "- [ ] âœ… **Sigmoid Function**: Implemented and visualized sigmoid function\n",
    "- [ ] âœ… **Dataset Generation**: Created and visualized binary classification dataset\n",
    "- [ ] âœ… **LogisticRegression Class**: Implemented complete logistic regression from scratch\n",
    "- [ ] âœ… **Model Training**: Trained the model and visualized cost function\n",
    "- [ ] âœ… **Model Evaluation**: Calculated accuracy and analyzed predictions\n",
    "- [ ] âœ… **Decision Boundary**: Visualized decision boundary and probability maps\n",
    "- [ ] âœ… **Mathematical Analysis**: Understood weights, odds ratios, and decision equation\n",
    "- [ ] âœ… **Learning Rate Study**: Compared different learning rates and their effects\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Core Concepts Learned:**\n",
    "1. **Sigmoid Function**: Maps any real number to probability (0-1 range)\n",
    "2. **Logistic Regression**: Uses sigmoid to model probability of binary classification\n",
    "3. **Cost Function**: Cross-entropy loss measures prediction quality\n",
    "4. **Gradient Descent**: Iteratively updates weights to minimize cost\n",
    "5. **Decision Boundary**: Linear separator in feature space where P(y=1) = 0.5\n",
    "6. **Learning Rate**: Controls step size in gradient descent optimization\n",
    "\n",
    "**Mathematical Relationships:**\n",
    "- Linear combination: z = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚\n",
    "- Probability: P(y=1) = Ïƒ(z) = 1/(1 + eâ»á¶»)\n",
    "- Cost: J = -1/m Î£[y log(Å·) + (1-y) log(1-Å·)]\n",
    "- Gradient: âˆ‚J/âˆ‚w = 1/m Xáµ€(A - y)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Overflow in Exponential Function**\n",
    "   - *Problem*: `np.exp()` returns inf for large negative values\n",
    "   - *Solution*: Clip input values to prevent overflow (implemented in our sigmoid)\n",
    "\n",
    "2. **Log of Zero in Cost Function**\n",
    "   - *Problem*: `np.log(0)` returns -inf\n",
    "   - *Solution*: Clip predictions to small epsilon values (1e-15 to 1-1e-15)\n",
    "\n",
    "3. **Slow Convergence**\n",
    "   - *Problem*: Cost decreases very slowly\n",
    "   - *Solutions*: Increase learning rate, normalize features, or increase iterations\n",
    "\n",
    "4. **Poor Performance**\n",
    "   - *Problem*: Low accuracy on test set\n",
    "   - *Solutions*: Check data quality, try feature engineering, or adjust threshold\n",
    "\n",
    "5. **Divergent Training**\n",
    "   - *Problem*: Cost increases instead of decreasing\n",
    "   - *Solution*: Reduce learning rate or check gradient computation\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Implementation:**\n",
    "1. âœ… Sigmoid function outputs values between 0 and 1\n",
    "2. âœ… Cost decreases over training iterations\n",
    "3. âœ… Model achieves reasonable accuracy (>70% for this dataset)\n",
    "4. âœ… Decision boundary is linear and separates classes reasonably\n",
    "5. âœ… Different learning rates show expected behavior\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save your notebook with results\n",
    "2. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del X_train, X_test, y_train, y_test\n",
    "   # del models\n",
    "   ```\n",
    "3. Close any open plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You have successfully implemented logistic regression from scratch! This foundation will be crucial for understanding:\n",
    "- Neural networks (next lab)\n",
    "- Deep learning architectures\n",
    "- Advanced optimization techniques\n",
    "- Multi-class classification\n",
    "\n",
    "**Congratulations! You've completed Lab 2.1 - Logistic Regression Implementation!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}