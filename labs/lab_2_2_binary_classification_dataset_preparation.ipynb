{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2: Binary Classification Dataset Preparation\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Load and preprocess real-world datasets for binary classification\n",
    "- Handle missing values, outliers, and data quality issues\n",
    "- Perform feature scaling and normalization\n",
    "- Split datasets appropriately for training and testing\n",
    "- Visualize and understand dataset characteristics\n",
    "- Apply data preprocessing techniques essential for neural networks\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy, Pandas, Matplotlib, Seaborn\n",
    "- Scikit-learn\n",
    "- Jupyter Notebook/Lab\n",
    "\n",
    "## Overview\n",
    "Real-world data is rarely clean and ready for machine learning models. This lab focuses on the crucial data preparation steps that often determine the success or failure of your machine learning project. You'll work with multiple datasets and learn industry-standard preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib and seaborn\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Real-World Dataset\n",
    "\n",
    "We'll start with the famous Breast Cancer Wisconsin dataset - a real medical dataset used for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "def load_breast_cancer_data():\n",
    "    \"\"\"\n",
    "    Load and return the breast cancer dataset with proper formatting\n",
    "    \n",
    "    Returns:\n",
    "    df -- pandas DataFrame with features and target\n",
    "    X -- feature matrix\n",
    "    y -- target vector\n",
    "    feature_names -- names of features\n",
    "    target_names -- names of target classes\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    data = load_breast_cancer()\n",
    "    \n",
    "    # Create DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    df['target'] = data.target\n",
    "    df['target_name'] = df['target'].map({0: 'malignant', 1: 'benign'})\n",
    "    \n",
    "    return df, data.data, data.target, data.feature_names, data.target_names\n",
    "\n",
    "# Load the dataset\n",
    "df, X_original, y_original, feature_names, target_names = load_breast_cancer_data()\n",
    "\n",
    "print(\"Breast Cancer Wisconsin Dataset Loaded\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Target classes: {target_names}\")\n",
    "print(f\"Class distribution:\\n{df['target_name'].value_counts()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows (selected columns):\")\n",
    "display_cols = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'target_name']\n",
    "print(df[display_cols].head())\n",
    "\n",
    "print(\"\\nâœ“ Dataset loaded and initial exploration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before preprocessing, let's understand our data through visualization and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical information\n",
    "print(\"Dataset Statistical Summary\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nMissing values: {missing_values.sum()} total\")\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"âœ“ No missing values found!\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Basic statistics for numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'target' in numerical_features:\n",
    "    numerical_features.remove('target')\n",
    "\n",
    "print(f\"\\nNumerical features: {len(numerical_features)}\")\n",
    "print(\"Basic statistics (first 5 features):\")\n",
    "print(df[numerical_features[:5]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target class distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Class distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "class_counts = df['target_name'].value_counts()\n",
    "plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution')\n",
    "\n",
    "# Class distribution bar plot\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.countplot(data=df, x='target_name')\n",
    "plt.title('Class Counts')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Target variable encoding\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(['Malignant (0)', 'Benign (1)'], [sum(y_original == 0), sum(y_original == 1)])\n",
    "plt.title('Encoded Target Distribution')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate class balance\n",
    "class_balance = df['target'].value_counts(normalize=True)\n",
    "print(f\"Class balance:\")\n",
    "print(f\"Benign (1): {class_balance[1]:.1%}\")\n",
    "print(f\"Malignant (0): {class_balance[0]:.1%}\")\n",
    "print(f\"Balance ratio: {class_balance[1]/class_balance[0]:.2f}:1\")\n",
    "\n",
    "print(\"\\nâœ“ Target variable analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "def plot_feature_distributions(df, features, target_col='target_name', max_features=8):\n",
    "    \"\"\"\n",
    "    Plot distribution of features by target class\n",
    "    \"\"\"\n",
    "    features_to_plot = features[:max_features]  # Limit to first max_features\n",
    "    n_features = len(features_to_plot)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(16, 4 * n_rows))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # Create separate histograms for each class\n",
    "        for class_name in df[target_col].unique():\n",
    "            data = df[df[target_col] == class_name][feature]\n",
    "            plt.hist(data, alpha=0.7, label=class_name, bins=20)\n",
    "        \n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{feature} Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for key features\n",
    "key_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', \n",
    "                'mean smoothness', 'mean compactness', 'mean concavity', 'mean symmetry']\n",
    "\n",
    "print(\"Feature Distributions by Class\")\n",
    "plot_feature_distributions(df, key_features)\n",
    "\n",
    "print(\"âœ“ Feature distribution analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Select a subset of features for correlation analysis (too many features make the plot unreadable)\n",
    "mean_features = [col for col in df.columns if 'mean' in col and col != 'target_name']\n",
    "correlation_features = mean_features + ['target']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[correlation_features].corr()\n",
    "mask = np.triu(correlation_matrix.corr())\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            mask=mask,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Feature Correlation Matrix (Mean Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_val > 0.8:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "print(f\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Correlation analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Handle Data Quality Issues\n",
    "\n",
    "Let's create a version of the dataset with some missing values and outliers to practice handling these common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with artificial missing values and outliers for practice\n",
    "def introduce_data_issues(df, missing_rate=0.05, outlier_rate=0.02):\n",
    "    \"\"\"\n",
    "    Introduce missing values and outliers to practice data cleaning\n",
    "    \n",
    "    Arguments:\n",
    "    df -- original DataFrame\n",
    "    missing_rate -- fraction of values to make missing\n",
    "    outlier_rate -- fraction of values to make outliers\n",
    "    \n",
    "    Returns:\n",
    "    df_with_issues -- DataFrame with data quality issues\n",
    "    \"\"\"\n",
    "    df_issues = df.copy()\n",
    "    \n",
    "    # Select only numerical columns (exclude target and target_name)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numerical_cols = [col for col in numerical_cols if col not in ['target', 'target_name']]\n",
    "    \n",
    "    # Introduce missing values\n",
    "    n_missing = int(len(df_issues) * len(numerical_cols) * missing_rate)\n",
    "    \n",
    "    for _ in range(n_missing):\n",
    "        row_idx = np.random.randint(0, len(df_issues))\n",
    "        col_idx = np.random.choice(numerical_cols)\n",
    "        df_issues.loc[row_idx, col_idx] = np.nan\n",
    "    \n",
    "    # Introduce outliers (extreme values)\n",
    "    n_outliers = int(len(df_issues) * len(numerical_cols) * outlier_rate)\n",
    "    \n",
    "    for _ in range(n_outliers):\n",
    "        row_idx = np.random.randint(0, len(df_issues))\n",
    "        col_idx = np.random.choice(numerical_cols)\n",
    "        \n",
    "        # Make outlier by multiplying by large random factor\n",
    "        current_val = df_issues.loc[row_idx, col_idx]\n",
    "        if not pd.isna(current_val):\n",
    "            outlier_factor = np.random.choice([0.01, 10.0])  # Very small or very large\n",
    "            df_issues.loc[row_idx, col_idx] = current_val * outlier_factor\n",
    "    \n",
    "    return df_issues\n",
    "\n",
    "# Create dataset with issues\n",
    "df_with_issues = introduce_data_issues(df, missing_rate=0.03, outlier_rate=0.01)\n",
    "\n",
    "print(\"Dataset with Artificial Data Quality Issues\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df_with_issues.isnull().sum()\n",
    "missing_features = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(f\"Features with missing values: {len(missing_features)}\")\n",
    "print(f\"Total missing values: {missing_counts.sum()}\")\n",
    "print(f\"Missing value rate: {missing_counts.sum() / (df_with_issues.shape[0] * df_with_issues.shape[1]):.1%}\")\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"\\nTop 5 features with missing values:\")\n",
    "    print(missing_features.head())\n",
    "\n",
    "print(\"\\nâœ“ Dataset with data quality issues created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and visualize outliers\n",
    "def detect_outliers_iqr(df, feature):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method\n",
    "    \n",
    "    Arguments:\n",
    "    df -- DataFrame\n",
    "    feature -- feature name to check for outliers\n",
    "    \n",
    "    Returns:\n",
    "    outlier_indices -- indices of outlier rows\n",
    "    lower_bound -- lower bound for normal values\n",
    "    upper_bound -- upper bound for normal values\n",
    "    \"\"\"\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_indices = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)].index\n",
    "    \n",
    "    return outlier_indices, lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers in a few key features\n",
    "outlier_analysis_features = ['mean radius', 'mean area', 'mean perimeter']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for i, feature in enumerate(outlier_analysis_features):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    \n",
    "    # Create box plot\n",
    "    plt.boxplot(df_with_issues[feature].dropna(), patch_artist=True)\n",
    "    plt.title(f'{feature}\\nOutlier Detection')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Detect outliers\n",
    "    outlier_indices, lower, upper = detect_outliers_iqr(df_with_issues, feature)\n",
    "    outlier_summary[feature] = {\n",
    "        'count': len(outlier_indices),\n",
    "        'percentage': len(outlier_indices) / len(df_with_issues) * 100,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    \n",
    "    # Add statistics text\n",
    "    plt.text(0.5, 0.95, f'Outliers: {len(outlier_indices)}', \n",
    "             transform=plt.gca().transAxes, ha='center', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier Analysis Summary\")\n",
    "print(\"=\" * 25)\n",
    "for feature, stats in outlier_summary.items():\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Outliers: {stats['count']} ({stats['percentage']:.1f}%)\")\n",
    "    print(f\"  Normal range: [{stats['lower_bound']:.2f}, {stats['upper_bound']:.2f}]\")\n",
    "\n",
    "print(\"\\nâœ“ Outlier detection completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Preprocessing Pipeline\n",
    "\n",
    "Now let's create a comprehensive preprocessing pipeline to handle missing values, outliers, and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete data preprocessing pipeline for binary classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, missing_strategy='mean', outlier_method='iqr', \n",
    "                 scaling_method='standard', outlier_factor=1.5):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "        \n",
    "        Arguments:\n",
    "        missing_strategy -- strategy for handling missing values ('mean', 'median', 'mode')\n",
    "        outlier_method -- method for outlier detection ('iqr', 'zscore')\n",
    "        scaling_method -- scaling method ('standard', 'minmax', 'none')\n",
    "        outlier_factor -- factor for outlier detection sensitivity\n",
    "        \"\"\"\n",
    "        self.missing_strategy = missing_strategy\n",
    "        self.outlier_method = outlier_method\n",
    "        self.scaling_method = scaling_method\n",
    "        self.outlier_factor = outlier_factor\n",
    "        \n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.feature_names = None\n",
    "        self.outlier_bounds = {}\n",
    "        self.preprocessing_stats = {}\n",
    "    \n",
    "    def _handle_missing_values(self, X_train, X_test=None):\n",
    "        \"\"\"\n",
    "        Handle missing values using imputation\n",
    "        \"\"\"\n",
    "        if self.missing_strategy == 'mean':\n",
    "            self.imputer = SimpleImputer(strategy='mean')\n",
    "        elif self.missing_strategy == 'median':\n",
    "            self.imputer = SimpleImputer(strategy='median')\n",
    "        elif self.missing_strategy == 'mode':\n",
    "            self.imputer = SimpleImputer(strategy='most_frequent')\n",
    "        \n",
    "        X_train_imputed = self.imputer.fit_transform(X_train)\n",
    "        X_test_imputed = self.imputer.transform(X_test) if X_test is not None else None\n",
    "        \n",
    "        return X_train_imputed, X_test_imputed\n",
    "    \n",
    "    def _detect_and_handle_outliers(self, X, method='iqr'):\n",
    "        \"\"\"\n",
    "        Detect and handle outliers\n",
    "        \"\"\"\n",
    "        X_cleaned = X.copy()\n",
    "        outlier_indices = set()\n",
    "        \n",
    "        for col_idx in range(X.shape[1]):\n",
    "            column_data = X[:, col_idx]\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = np.percentile(column_data, 25)\n",
    "                Q3 = np.percentile(column_data, 75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - self.outlier_factor * IQR\n",
    "                upper_bound = Q3 + self.outlier_factor * IQR\n",
    "                \n",
    "                outliers = np.where((column_data < lower_bound) | (column_data > upper_bound))[0]\n",
    "                outlier_indices.update(outliers)\n",
    "                \n",
    "                # Store bounds for this feature\n",
    "                self.outlier_bounds[col_idx] = (lower_bound, upper_bound)\n",
    "                \n",
    "                # Cap outliers to bounds\n",
    "                X_cleaned[:, col_idx] = np.clip(column_data, lower_bound, upper_bound)\n",
    "            \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((column_data - np.mean(column_data)) / np.std(column_data))\n",
    "                outliers = np.where(z_scores > 3)[0]\n",
    "                outlier_indices.update(outliers)\n",
    "                \n",
    "                # Replace outliers with median\n",
    "                median_val = np.median(column_data)\n",
    "                X_cleaned[outliers, col_idx] = median_val\n",
    "        \n",
    "        return X_cleaned, list(outlier_indices)\n",
    "    \n",
    "    def _scale_features(self, X_train, X_test=None):\n",
    "        \"\"\"\n",
    "        Scale features using specified method\n",
    "        \"\"\"\n",
    "        if self.scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif self.scaling_method == 'none':\n",
    "            return X_train, X_test\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test) if X_test is not None else None\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled\n",
    "    \n",
    "    def fit_transform(self, X_train, X_test=None, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit the preprocessor and transform the data\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training features\n",
    "        X_test -- test features (optional)\n",
    "        feature_names -- names of features (optional)\n",
    "        \n",
    "        Returns:\n",
    "        X_train_processed -- processed training features\n",
    "        X_test_processed -- processed test features (if provided)\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        print(\"Starting data preprocessing pipeline...\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Step 1: Handle missing values\n",
    "        print(f\"Step 1: Handling missing values using {self.missing_strategy} strategy\")\n",
    "        X_train_step1, X_test_step1 = self._handle_missing_values(X_train, X_test)\n",
    "        missing_before = np.isnan(X_train).sum()\n",
    "        missing_after = np.isnan(X_train_step1).sum()\n",
    "        print(f\"  Missing values: {missing_before} â†’ {missing_after}\")\n",
    "        \n",
    "        # Step 2: Handle outliers\n",
    "        print(f\"Step 2: Handling outliers using {self.outlier_method} method\")\n",
    "        X_train_step2, train_outlier_indices = self._detect_and_handle_outliers(\n",
    "            X_train_step1, self.outlier_method)\n",
    "        if X_test_step1 is not None:\n",
    "            X_test_step2, test_outlier_indices = self._detect_and_handle_outliers(\n",
    "                X_test_step1, self.outlier_method)\n",
    "        else:\n",
    "            X_test_step2 = None\n",
    "            test_outlier_indices = []\n",
    "        \n",
    "        print(f\"  Train outliers detected and handled: {len(train_outlier_indices)}\")\n",
    "        if X_test is not None:\n",
    "            print(f\"  Test outliers detected and handled: {len(test_outlier_indices)}\")\n",
    "        \n",
    "        # Step 3: Feature scaling\n",
    "        print(f\"Step 3: Scaling features using {self.scaling_method} method\")\n",
    "        X_train_final, X_test_final = self._scale_features(X_train_step2, X_test_step2)\n",
    "        \n",
    "        if self.scaling_method != 'none':\n",
    "            print(f\"  Features scaled successfully\")\n",
    "        else:\n",
    "            print(f\"  No scaling applied\")\n",
    "        \n",
    "        # Store preprocessing statistics\n",
    "        self.preprocessing_stats = {\n",
    "            'missing_values_imputed': missing_before,\n",
    "            'train_outliers_handled': len(train_outlier_indices),\n",
    "            'test_outliers_handled': len(test_outlier_indices),\n",
    "            'scaling_method': self.scaling_method,\n",
    "            'final_train_shape': X_train_final.shape,\n",
    "            'final_test_shape': X_test_final.shape if X_test_final is not None else None\n",
    "        }\n",
    "        \n",
    "        print(\"=\" * 40)\n",
    "        print(\"âœ“ Preprocessing pipeline completed!\")\n",
    "        \n",
    "        return X_train_final, X_test_final\n",
    "    \n",
    "    def get_preprocessing_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary of preprocessing steps applied\n",
    "        \"\"\"\n",
    "        return self.preprocessing_stats\n",
    "\n",
    "print(\"âœ“ DataPreprocessor class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply Preprocessing Pipeline\n",
    "\n",
    "Let's apply our preprocessing pipeline to the dataset with data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for preprocessing\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in df_with_issues.columns \n",
    "                  if col not in ['target', 'target_name']]\n",
    "\n",
    "X_with_issues = df_with_issues[feature_columns].values\n",
    "y_with_issues = df_with_issues['target'].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_with_issues, y_with_issues, test_size=0.2, random_state=42, stratify=y_with_issues\n",
    ")\n",
    "\n",
    "print(\"Data prepared for preprocessing\")\n",
    "print(\"=\" * 32)\n",
    "print(f\"Training set shape: {X_train_raw.shape}\")\n",
    "print(f\"Test set shape: {X_test_raw.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Missing values in train set: {np.isnan(X_train_raw).sum()}\")\n",
    "print(f\"Missing values in test set: {np.isnan(X_test_raw).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different preprocessing strategies\n",
    "preprocessing_configs = [\n",
    "    {'name': 'Basic', 'missing': 'mean', 'outlier': 'iqr', 'scaling': 'standard'},\n",
    "    {'name': 'Robust', 'missing': 'median', 'outlier': 'iqr', 'scaling': 'standard'},\n",
    "    {'name': 'MinMax', 'missing': 'mean', 'outlier': 'iqr', 'scaling': 'minmax'},\n",
    "    {'name': 'Z-score Outliers', 'missing': 'mean', 'outlier': 'zscore', 'scaling': 'standard'}\n",
    "]\n",
    "\n",
    "preprocessing_results = {}\n",
    "\n",
    "for config in preprocessing_configs:\n",
    "    print(f\"\\nTesting {config['name']} preprocessing strategy:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        missing_strategy=config['missing'],\n",
    "        outlier_method=config['outlier'],\n",
    "        scaling_method=config['scaling']\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_train_processed, X_test_processed = preprocessor.fit_transform(\n",
    "        X_train_raw.copy(), X_test_raw.copy(), feature_columns\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    preprocessing_results[config['name']] = {\n",
    "        'preprocessor': preprocessor,\n",
    "        'X_train': X_train_processed,\n",
    "        'X_test': X_test_processed,\n",
    "        'stats': preprocessor.get_preprocessing_summary()\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ“ All preprocessing strategies tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Preprocessing Results\n",
    "\n",
    "Let's visualize and compare the effects of different preprocessing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare preprocessing statistics\n",
    "print(\"Preprocessing Strategies Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Strategy':<15} {'Missing':<8} {'Outliers':<10} {'Scaling':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for name, result in preprocessing_results.items():\n",
    "    stats = result['stats']\n",
    "    print(f\"{name:<15} {stats['missing_values_imputed']:<8} \"\n",
    "          f\"{stats['train_outliers_handled']:<10} {stats['scaling_method']:<10}\")\n",
    "\n",
    "# Visualize the effect of different scaling methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Select a feature for comparison\n",
    "feature_idx = 0  # First feature\n",
    "feature_name = feature_columns[feature_idx]\n",
    "\n",
    "for i, (name, result) in enumerate(preprocessing_results.items()):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distribution of the selected feature after preprocessing\n",
    "    feature_data = result['X_train'][:, feature_idx]\n",
    "    \n",
    "    ax.hist(feature_data, bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'{name} Strategy\\n{feature_name}')\n",
    "    ax.set_xlabel('Scaled Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = np.mean(feature_data)\n",
    "    std_val = np.std(feature_data)\n",
    "    ax.text(0.05, 0.95, f'Î¼={mean_val:.2f}\\nÏƒ={std_val:.2f}', \n",
    "           transform=ax.transAxes, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Preprocessing comparison visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize before and after preprocessing for key features\n",
    "def plot_before_after_preprocessing(X_before, X_after, feature_indices, feature_names, title):\n",
    "    \"\"\"\n",
    "    Plot feature distributions before and after preprocessing\n",
    "    \"\"\"\n",
    "    n_features = len(feature_indices)\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(12, 4 * n_features))\n",
    "    \n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feat_idx in enumerate(feature_indices):\n",
    "        # Before preprocessing\n",
    "        axes[i][0].hist(X_before[:, feat_idx], bins=30, alpha=0.7, \n",
    "                       color='red', edgecolor='black')\n",
    "        axes[i][0].set_title(f'Before: {feature_names[feat_idx]}')\n",
    "        axes[i][0].set_ylabel('Frequency')\n",
    "        axes[i][0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics for before\n",
    "        before_data = X_before[:, feat_idx]\n",
    "        before_data_clean = before_data[~np.isnan(before_data)]\n",
    "        mean_before = np.mean(before_data_clean)\n",
    "        std_before = np.std(before_data_clean)\n",
    "        axes[i][0].text(0.05, 0.95, f'Î¼={mean_before:.2f}\\nÏƒ={std_before:.2f}', \n",
    "                       transform=axes[i][0].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # After preprocessing\n",
    "        axes[i][1].hist(X_after[:, feat_idx], bins=30, alpha=0.7, \n",
    "                       color='blue', edgecolor='black')\n",
    "        axes[i][1].set_title(f'After: {feature_names[feat_idx]}')\n",
    "        axes[i][1].set_ylabel('Frequency')\n",
    "        axes[i][1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics for after\n",
    "        mean_after = np.mean(X_after[:, feat_idx])\n",
    "        std_after = np.std(X_after[:, feat_idx])\n",
    "        axes[i][1].text(0.05, 0.95, f'Î¼={mean_after:.2f}\\nÏƒ={std_after:.2f}', \n",
    "                       transform=axes[i][1].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select the 'Basic' preprocessing strategy for detailed comparison\n",
    "basic_result = preprocessing_results['Basic']\n",
    "X_train_processed = basic_result['X_train']\n",
    "\n",
    "# Show before/after for selected features\n",
    "selected_feature_indices = [0, 3, 10]  # First few features\n",
    "selected_feature_names = [feature_columns[i] for i in selected_feature_indices]\n",
    "\n",
    "print(\"Before vs After Preprocessing (Basic Strategy)\")\n",
    "plot_before_after_preprocessing(\n",
    "    X_train_raw, X_train_processed, \n",
    "    selected_feature_indices, feature_columns,\n",
    "    \"Feature Distributions: Before vs After Preprocessing\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Before/after preprocessing visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Analysis and Selection\n",
    "\n",
    "Let's analyze the processed features and understand their importance for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance using correlation with target\n",
    "def analyze_feature_importance(X, y, feature_names, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze feature importance based on correlation with target\n",
    "    \n",
    "    Arguments:\n",
    "    X -- feature matrix\n",
    "    y -- target vector\n",
    "    feature_names -- list of feature names\n",
    "    top_k -- number of top features to display\n",
    "    \n",
    "    Returns:\n",
    "    feature_importance -- DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        # Calculate correlation between feature and target\n",
    "        corr = np.corrcoef(X[:, i], y)[0, 1]\n",
    "        correlations.append(abs(corr))  # Use absolute correlation\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'correlation': correlations\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('correlation', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance for preprocessed data\n",
    "X_train_processed = preprocessing_results['Basic']['X_train']\n",
    "feature_importance = analyze_feature_importance(X_train_processed, y_train, feature_columns, top_k=15)\n",
    "\n",
    "print(\"Feature Importance Analysis (Top 15)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Rank':<5} {'Feature':<25} {'Correlation':<12}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "for idx, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
    "    print(f\"{idx+1:<5} {row['feature']:<25} {row['correlation']:.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(10)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.barh(range(len(top_features)), top_features['correlation'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of feature importance\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(feature_importance['correlation'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Distribution of Feature Importance Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Feature importance analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Final Processed Dataset\n",
    "\n",
    "Let's create the final processed dataset that's ready for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final processed dataset using the best preprocessing strategy\n",
    "def create_final_dataset(preprocessing_results, strategy_name='Basic'):\n",
    "    \"\"\"\n",
    "    Create final processed dataset ready for ML models\n",
    "    \n",
    "    Arguments:\n",
    "    preprocessing_results -- dictionary of preprocessing results\n",
    "    strategy_name -- name of the preprocessing strategy to use\n",
    "    \n",
    "    Returns:\n",
    "    final_data -- dictionary containing processed data and metadata\n",
    "    \"\"\"\n",
    "    result = preprocessing_results[strategy_name]\n",
    "    \n",
    "    final_data = {\n",
    "        'X_train': result['X_train'],\n",
    "        'X_test': result['X_test'],\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': feature_columns,\n",
    "        'target_names': target_names,\n",
    "        'preprocessing_stats': result['stats'],\n",
    "        'preprocessor': result['preprocessor'],\n",
    "        'strategy_used': strategy_name\n",
    "    }\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "# Create final dataset\n",
    "final_dataset = create_final_dataset(preprocessing_results, 'Basic')\n",
    "\n",
    "print(\"Final Processed Dataset Summary\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Training set shape: {final_dataset['X_train'].shape}\")\n",
    "print(f\"Test set shape: {final_dataset['X_test'].shape}\")\n",
    "print(f\"Number of features: {len(final_dataset['feature_names'])}\")\n",
    "print(f\"Number of classes: {len(final_dataset['target_names'])}\")\n",
    "print(f\"Preprocessing strategy: {final_dataset['strategy_used']}\")\n",
    "\n",
    "# Data quality verification\n",
    "X_train_final = final_dataset['X_train']\n",
    "X_test_final = final_dataset['X_test']\n",
    "\n",
    "print(\"\\nData Quality Verification:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Missing values in train: {np.isnan(X_train_final).sum()}\")\n",
    "print(f\"Missing values in test: {np.isnan(X_test_final).sum()}\")\n",
    "print(f\"Infinite values in train: {np.isinf(X_train_final).sum()}\")\n",
    "print(f\"Infinite values in test: {np.isinf(X_test_final).sum()}\")\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\nFeature Statistics (after preprocessing):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Train set - Mean: {np.mean(X_train_final):.4f}, Std: {np.std(X_train_final):.4f}\")\n",
    "print(f\"Train set - Min: {np.min(X_train_final):.4f}, Max: {np.max(X_train_final):.4f}\")\n",
    "print(f\"Test set - Mean: {np.mean(X_test_final):.4f}, Std: {np.std(X_test_final):.4f}\")\n",
    "print(f\"Test set - Min: {np.min(X_test_final):.4f}, Max: {np.max(X_test_final):.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Final dataset created and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Processed Data for Future Use\n",
    "\n",
    "Let's save our processed dataset so it can be easily loaded for the next labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save processed dataset\n",
    "def save_processed_dataset(final_dataset, filename_prefix='processed_breast_cancer'):\n",
    "    \"\"\"\n",
    "    Save processed dataset to numpy files\n",
    "    \n",
    "    Arguments:\n",
    "    final_dataset -- dictionary containing processed data\n",
    "    filename_prefix -- prefix for saved files\n",
    "    \"\"\"\n",
    "    # Save arrays\n",
    "    np.save(f'{filename_prefix}_X_train.npy', final_dataset['X_train'])\n",
    "    np.save(f'{filename_prefix}_X_test.npy', final_dataset['X_test'])\n",
    "    np.save(f'{filename_prefix}_y_train.npy', final_dataset['y_train'])\n",
    "    np.save(f'{filename_prefix}_y_test.npy', final_dataset['y_test'])\n",
    "    \n",
    "    # Save metadata as text file\n",
    "    with open(f'{filename_prefix}_metadata.txt', 'w') as f:\n",
    "        f.write(f\"Dataset: Breast Cancer Wisconsin (Processed)\\n\")\n",
    "        f.write(f\"Training samples: {final_dataset['X_train'].shape[0]}\\n\")\n",
    "        f.write(f\"Test samples: {final_dataset['X_test'].shape[0]}\\n\")\n",
    "        f.write(f\"Features: {final_dataset['X_train'].shape[1]}\\n\")\n",
    "        f.write(f\"Classes: {len(final_dataset['target_names'])}\\n\")\n",
    "        f.write(f\"Preprocessing strategy: {final_dataset['strategy_used']}\\n\")\n",
    "        f.write(f\"Target names: {', '.join(final_dataset['target_names'])}\\n\")\n",
    "        f.write(f\"\\nFeature names:\\n\")\n",
    "        for i, name in enumerate(final_dataset['feature_names']):\n",
    "            f.write(f\"{i:2d}: {name}\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved with prefix: {filename_prefix}\")\n",
    "    print(f\"Files created:\")\n",
    "    print(f\"  - {filename_prefix}_X_train.npy\")\n",
    "    print(f\"  - {filename_prefix}_X_test.npy\")\n",
    "    print(f\"  - {filename_prefix}_y_train.npy\")\n",
    "    print(f\"  - {filename_prefix}_y_test.npy\")\n",
    "    print(f\"  - {filename_prefix}_metadata.txt\")\n",
    "\n",
    "# Save the processed dataset\n",
    "save_processed_dataset(final_dataset)\n",
    "\n",
    "print(\"\\nâœ“ Processed dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] âœ… **Environment Setup**: Imported libraries and configured environment\n",
    "- [ ] âœ… **Dataset Loading**: Loaded real-world breast cancer dataset\n",
    "- [ ] âœ… **Exploratory Data Analysis**: Analyzed distributions, correlations, and class balance\n",
    "- [ ] âœ… **Data Quality Issues**: Introduced and detected missing values and outliers\n",
    "- [ ] âœ… **Preprocessing Pipeline**: Implemented comprehensive DataPreprocessor class\n",
    "- [ ] âœ… **Pipeline Application**: Applied different preprocessing strategies\n",
    "- [ ] âœ… **Results Comparison**: Compared effects of different preprocessing approaches\n",
    "- [ ] âœ… **Feature Importance**: Analyzed feature importance and correlations\n",
    "- [ ] âœ… **Final Dataset**: Created clean, processed dataset ready for ML\n",
    "- [ ] âœ… **Data Saving**: Saved processed data for future use\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Data Preprocessing Fundamentals:**\n",
    "1. **Missing Values**: Imputation strategies (mean, median, mode) preserve data integrity\n",
    "2. **Outlier Detection**: IQR and Z-score methods identify anomalous values\n",
    "3. **Feature Scaling**: StandardScaler (mean=0, std=1) and MinMaxScaler (range 0-1)\n",
    "4. **Data Quality**: Verification ensures no missing/infinite values remain\n",
    "5. **Feature Importance**: Correlation analysis identifies most predictive features\n",
    "\n",
    "**Preprocessing Pipeline Benefits:**\n",
    "- Consistent data transformation across train/test sets\n",
    "- Reproducible preprocessing steps\n",
    "- Proper handling of data leakage (fit on train, transform on test)\n",
    "- Scalable to new data\n",
    "\n",
    "**Best Practices Applied:**\n",
    "- Always fit preprocessors on training data only\n",
    "- Handle missing values before outlier detection\n",
    "- Scale features after outlier handling\n",
    "- Verify data quality after each step\n",
    "- Document all preprocessing decisions\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Memory Issues with Large Datasets**\n",
    "   - *Problem*: Dataset too large to fit in memory\n",
    "   - *Solution*: Process data in chunks or use data generators\n",
    "\n",
    "2. **Scaling Issues**\n",
    "   - *Problem*: Features have very different scales\n",
    "   - *Solution*: Always apply feature scaling; try robust scaling for outliers\n",
    "\n",
    "3. **Missing Value Patterns**\n",
    "   - *Problem*: Missing values not random (MNAR)\n",
    "   - *Solution*: Analyze missingness patterns; consider domain-specific imputation\n",
    "\n",
    "4. **Outlier Sensitivity**\n",
    "   - *Problem*: Too many/few outliers detected\n",
    "   - *Solution*: Adjust outlier_factor parameter or try different methods\n",
    "\n",
    "5. **Data Leakage**\n",
    "   - *Problem*: Information from test set used in preprocessing\n",
    "   - *Solution*: Always fit preprocessors on training data only\n",
    "\n",
    "6. **Categorical Variables**\n",
    "   - *Problem*: Non-numeric data types\n",
    "   - *Solution*: Use LabelEncoder or OneHotEncoder before other preprocessing\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Preprocessing:**\n",
    "1. âœ… No missing values in final dataset\n",
    "2. âœ… No infinite values in final dataset\n",
    "3. âœ… Features are properly scaled (check mean/std for StandardScaler)\n",
    "4. âœ… Train and test sets have consistent preprocessing\n",
    "5. âœ… Feature importance analysis makes domain sense\n",
    "6. âœ… Dataset saved successfully for future use\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save your notebook with all results\n",
    "2. Keep processed data files for next labs\n",
    "3. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del df_with_issues, X_train_raw, X_test_raw\n",
    "   # del preprocessing_results\n",
    "   ```\n",
    "4. Close plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Your processed dataset is now ready for machine learning! In the next lab, you'll:\n",
    "- Load this processed data\n",
    "- Implement gradient descent from scratch\n",
    "- Train models on clean, scaled features\n",
    "- Compare model performance with/without preprocessing\n",
    "\n",
    "The preprocessing pipeline you built is reusable and can be applied to new datasets with similar characteristics.\n",
    "\n",
    "**Congratulations! You've completed Lab 2.2 - Binary Classification Dataset Preparation!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}