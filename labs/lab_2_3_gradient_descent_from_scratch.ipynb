{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.3: Gradient Descent from Scratch\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the mathematical foundations of gradient descent optimization\n",
    "- Implement gradient descent algorithms from scratch using NumPy\n",
    "- Compare different variants: batch, stochastic, and mini-batch gradient descent\n",
    "- Visualize the optimization process and understand convergence behavior\n",
    "- Apply gradient descent to optimize real machine learning problems\n",
    "- Analyze the effect of learning rates and other hyperparameters\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy, Matplotlib\n",
    "- Understanding of derivatives and basic calculus\n",
    "- Completed Lab 2.1 (Logistic Regression)\n",
    "\n",
    "## Overview\n",
    "Gradient descent is the backbone of machine learning optimization. This lab provides a deep dive into implementing gradient descent from scratch, understanding its variants, and visualizing how it finds optimal solutions. You'll build intuition for why gradient descent works and how to tune it effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Mathematical Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "\n",
    "# Mathematical foundation reminder\n",
    "print(\"\\nGradient Descent Mathematical Foundation:\")\n",
    "print(\"=========================================\")\n",
    "print(\"Goal: Minimize cost function J(θ)\")\n",
    "print(\"Update rule: θ = θ - α * ∇J(θ)\")\n",
    "print(\"Where:\")\n",
    "print(\"  θ = parameters (weights)\")\n",
    "print(\"  α = learning rate\")\n",
    "print(\"  ∇J(θ) = gradient of cost function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simple 1D Gradient Descent Visualization\n",
    "\n",
    "Let's start with a simple 1D example to build intuition about how gradient descent works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_quadratic_function(x):\n",
    "    \"\"\"\n",
    "    Simple quadratic function: f(x) = (x - 2)^2 + 1\n",
    "    Minimum at x = 2, f(2) = 1\n",
    "    \"\"\"\n",
    "    return (x - 2)**2 + 1\n",
    "\n",
    "def simple_quadratic_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of the quadratic function: f'(x) = 2(x - 2)\n",
    "    \"\"\"\n",
    "    return 2 * (x - 2)\n",
    "\n",
    "def gradient_descent_1d(start_x, learning_rate, num_iterations, func, grad_func):\n",
    "    \"\"\"\n",
    "    1D gradient descent implementation\n",
    "    \n",
    "    Arguments:\n",
    "    start_x -- starting point\n",
    "    learning_rate -- step size\n",
    "    num_iterations -- number of iterations\n",
    "    func -- function to minimize\n",
    "    grad_func -- gradient function\n",
    "    \n",
    "    Returns:\n",
    "    x_history -- history of x values\n",
    "    cost_history -- history of function values\n",
    "    \"\"\"\n",
    "    x = start_x\n",
    "    x_history = [x]\n",
    "    cost_history = [func(x)]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate gradient\n",
    "        gradient = grad_func(x)\n",
    "        \n",
    "        # Update parameter\n",
    "        x = x - learning_rate * gradient\n",
    "        \n",
    "        # Record history\n",
    "        x_history.append(x)\n",
    "        cost_history.append(func(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(cost_history)\n",
    "\n",
    "# Test with different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.9]\n",
    "start_point = -2.0\n",
    "iterations = 50\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the function\n",
    "x_range = np.linspace(-4, 6, 1000)\n",
    "y_range = simple_quadratic_function(x_range)\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    \n",
    "    # Plot function\n",
    "    plt.plot(x_range, y_range, 'b-', linewidth=2, alpha=0.7, label='f(x) = (x-2)² + 1')\n",
    "    \n",
    "    # Run gradient descent\n",
    "    x_hist, cost_hist = gradient_descent_1d(\n",
    "        start_point, lr, iterations, \n",
    "        simple_quadratic_function, simple_quadratic_derivative\n",
    "    )\n",
    "    \n",
    "    # Plot optimization path\n",
    "    plt.plot(x_hist, cost_hist, 'ro-', markersize=4, linewidth=1, \n",
    "            alpha=0.8, label=f'Optimization Path')\n",
    "    \n",
    "    # Mark start and end points\n",
    "    plt.plot(x_hist[0], cost_hist[0], 'go', markersize=8, label='Start')\n",
    "    plt.plot(x_hist[-1], cost_hist[-1], 'ro', markersize=8, label='End')\n",
    "    \n",
    "    # Mark true minimum\n",
    "    plt.plot(2, 1, 'k*', markersize=15, label='True Minimum')\n",
    "    \n",
    "    plt.title(f'Learning Rate = {lr}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xlim(-4, 6)\n",
    "    plt.ylim(0, 20)\n",
    "    \n",
    "    # Print final results\n",
    "    final_x = x_hist[-1]\n",
    "    final_cost = cost_hist[-1]\n",
    "    print(f\"LR={lr}: Final x={final_x:.4f}, Final cost={final_cost:.4f}, \"\n",
    "          f\"Iterations to converge: {len(x_hist)-1}\")\n",
    "\n",
    "plt.suptitle('1D Gradient Descent with Different Learning Rates', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ 1D gradient descent visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Gradient Descent for Linear Regression\n",
    "\n",
    "Now let's implement gradient descent for a real machine learning problem: linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate -- step size for gradient descent\n",
    "        max_iterations -- maximum number of iterations\n",
    "        tolerance -- convergence tolerance\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        self.converged_at = None\n",
    "    \n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"\n",
    "        Add bias column to feature matrix\n",
    "        \"\"\"\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error cost\n",
    "        \n",
    "        Arguments:\n",
    "        X -- features with bias column\n",
    "        y -- target values\n",
    "        weights -- model parameters\n",
    "        \n",
    "        Returns:\n",
    "        cost -- MSE cost\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(weights)\n",
    "        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def _compute_gradients(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute gradients for linear regression\n",
    "        \n",
    "        Arguments:\n",
    "        X -- features with bias column\n",
    "        y -- target values\n",
    "        weights -- current model parameters\n",
    "        \n",
    "        Returns:\n",
    "        gradients -- gradients with respect to weights\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(weights)\n",
    "        errors = predictions - y\n",
    "        gradients = (1 / m) * X.T.dot(errors)\n",
    "        return gradients\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "        \n",
    "        Arguments:\n",
    "        X -- training features (m, n)\n",
    "        y -- training targets (m,)\n",
    "        \"\"\"\n",
    "        # Ensure y is column vector\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = self._add_bias(X)\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        n_features = X_with_bias.shape[1]\n",
    "        self.weights = np.random.normal(0, 0.01, (n_features, 1))\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        for i in range(self.max_iterations):\n",
    "            # Compute cost\n",
    "            cost = self._compute_cost(X_with_bias, y, self.weights)\n",
    "            self.cost_history.append(cost)\n",
    "            self.weight_history.append(self.weights.copy())\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients = self._compute_gradients(X_with_bias, y, self.weights)\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights = self.weights - self.learning_rate * gradients\n",
    "            \n",
    "            # Check for convergence\n",
    "            if len(self.cost_history) > 1:\n",
    "                cost_change = abs(self.cost_history[-2] - self.cost_history[-1])\n",
    "                if cost_change < self.tolerance:\n",
    "                    self.converged_at = i\n",
    "                    print(f\"Converged at iteration {i}\")\n",
    "                    break\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        # Separate bias and weights\n",
    "        self.bias = self.weights[0, 0]\n",
    "        self.weights = self.weights[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input features\n",
    "        \n",
    "        Returns:\n",
    "        predictions -- predicted values\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "\n",
    "print(\"✓ LinearRegressionGD class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression dataset\n",
    "def create_regression_dataset(n_samples=200, n_features=1, noise=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic regression dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise*100,  # Scale noise appropriately\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "# Create dataset\n",
    "X_reg, y_reg = create_regression_dataset(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Standardize features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_reg_scaled = scaler.fit_transform(X_reg)\n",
    "\n",
    "print(f\"Regression dataset created:\")\n",
    "print(f\"Shape: {X_reg_scaled.shape}\")\n",
    "print(f\"Target range: [{np.min(y_reg):.2f}, {np.max(y_reg):.2f}]\")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_reg_scaled[:, 0], y_reg, alpha=0.7, s=50)\n",
    "plt.xlabel('Feature (scaled)')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Synthetic Regression Dataset')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Regression dataset created and visualized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "models = {}\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = LinearRegressionGD(learning_rate=lr, max_iterations=1000)\n",
    "    model.fit(X_reg_scaled, y_reg)\n",
    "    \n",
    "    models[lr] = model\n",
    "    \n",
    "    # Calculate performance\n",
    "    train_score = model.score(X_reg_scaled, y_reg)\n",
    "    final_cost = model.cost_history[-1]\n",
    "    \n",
    "    print(f\"Final R² score: {train_score:.4f}\")\n",
    "    print(f\"Final cost: {final_cost:.6f}\")\n",
    "    print(f\"Learned weight: {model.weights[0, 0]:.4f}\")\n",
    "    print(f\"Learned bias: {model.bias:.4f}\")\n",
    "    \n",
    "    # Plot cost history\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(model.cost_history, linewidth=2)\n",
    "    plt.title(f'Learning Rate = {lr}')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost (MSE)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.suptitle('Training Cost vs Iterations for Different Learning Rates', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Linear regression training with different learning rates completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Gradient Descent in 2D Parameter Space\n",
    "\n",
    "Let's visualize how gradient descent navigates the parameter space for a 2D linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_gradient_descent():\n",
    "    \"\"\"\n",
    "    Visualize gradient descent in 2D parameter space\n",
    "    \"\"\"\n",
    "    # Create simple 2D dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 50\n",
    "    X_simple = np.random.randn(n_samples, 2)\n",
    "    true_weights = np.array([2.5, -1.5])\n",
    "    true_bias = 1.0\n",
    "    y_simple = X_simple.dot(true_weights) + true_bias + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Standardize features\n",
    "    X_simple = (X_simple - np.mean(X_simple, axis=0)) / np.std(X_simple, axis=0)\n",
    "    \n",
    "    print(f\"True parameters: weights={true_weights}, bias={true_bias:.1f}\")\n",
    "    \n",
    "    # Train model and track weight history\n",
    "    model = LinearRegressionGD(learning_rate=0.1, max_iterations=100)\n",
    "    model.fit(X_simple, y_simple)\n",
    "    \n",
    "    # Extract weight history (bias and first weight only for 2D visualization)\n",
    "    weight_history = np.array(model.weight_history)\n",
    "    bias_history = weight_history[:, 0, 0]  # bias\n",
    "    w1_history = weight_history[:, 1, 0]    # first weight\n",
    "    \n",
    "    # Create cost surface\n",
    "    bias_range = np.linspace(bias_history.min() - 1, bias_history.max() + 1, 50)\n",
    "    w1_range = np.linspace(w1_history.min() - 1, w1_history.max() + 1, 50)\n",
    "    B, W1 = np.meshgrid(bias_range, w1_range)\n",
    "    \n",
    "    # Compute cost surface (fixing other weights to learned values)\n",
    "    X_with_bias = np.c_[np.ones((X_simple.shape[0], 1)), X_simple]\n",
    "    y_simple_reshaped = y_simple.reshape(-1, 1)\n",
    "    \n",
    "    Z = np.zeros_like(B)\n",
    "    for i in range(B.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            # Create weight vector with current bias and w1, keeping w2 fixed\n",
    "            test_weights = np.array([[B[i, j]], [W1[i, j]], [model.weights[1, 0]]])\n",
    "            predictions = X_with_bias.dot(test_weights)\n",
    "            cost = (1 / (2 * len(y_simple))) * np.sum((predictions - y_simple_reshaped) ** 2)\n",
    "            Z[i, j] = cost\n",
    "    \n",
    "    # Plot the optimization path\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 2D contour plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    contour = plt.contour(B, W1, Z, levels=20, alpha=0.6)\n",
    "    plt.contourf(B, W1, Z, levels=20, alpha=0.3, cmap='viridis')\n",
    "    plt.colorbar(label='Cost')\n",
    "    \n",
    "    # Plot optimization path\n",
    "    plt.plot(bias_history, w1_history, 'ro-', markersize=4, linewidth=2, \n",
    "            alpha=0.8, label='Gradient Descent Path')\n",
    "    plt.plot(bias_history[0], w1_history[0], 'go', markersize=10, label='Start')\n",
    "    plt.plot(bias_history[-1], w1_history[-1], 'ro', markersize=10, label='End')\n",
    "    \n",
    "    plt.xlabel('Bias')\n",
    "    plt.ylabel('Weight 1')\n",
    "    plt.title('Gradient Descent Path in Parameter Space')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cost over iterations\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(model.cost_history, 'b-', linewidth=2)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost vs Iterations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Parameter convergence\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(bias_history, label='Bias', linewidth=2)\n",
    "    plt.plot(w1_history, label='Weight 1', linewidth=2)\n",
    "    plt.axhline(y=true_bias, color='red', linestyle='--', alpha=0.7, label=f'True Bias ({true_bias})')\n",
    "    plt.axhline(y=true_weights[0], color='orange', linestyle='--', alpha=0.7, label=f'True Weight 1 ({true_weights[0]})')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title('Parameter Convergence')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"\\nFinal learned parameters:\")\n",
    "    print(f\"Bias: {model.bias:.4f} (true: {true_bias:.1f})\")\n",
    "    print(f\"Weight 1: {model.weights[0, 0]:.4f} (true: {true_weights[0]:.1f})\")\n",
    "    print(f\"Weight 2: {model.weights[1, 0]:.4f} (true: {true_weights[1]:.1f})\")\n",
    "    print(f\"Final R² score: {model.score(X_simple, y_simple):.4f}\")\n",
    "\n",
    "visualize_2d_gradient_descent()\n",
    "print(\"\\n✓ 2D gradient descent visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stochastic and Mini-Batch Gradient Descent\n",
    "\n",
    "Let's implement different variants of gradient descent and compare their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentVariants:\n",
    "    \"\"\"\n",
    "    Implementation of different gradient descent variants\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute MSE cost\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(weights)\n",
    "        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def batch_gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        Standard (batch) gradient descent\n",
    "        Uses the entire dataset for each update\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.random.normal(0, 0.01, (X_with_bias.shape[1], 1))\n",
    "        self.cost_history = []\n",
    "        \n",
    "        m = len(y)\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            predictions = X_with_bias.dot(self.weights)\n",
    "            cost = self._compute_cost(X_with_bias, y, self.weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients using entire dataset\n",
    "            errors = predictions - y\n",
    "            gradients = (1 / m) * X_with_bias.T.dot(errors)\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights -= self.learning_rate * gradients\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Batch GD - Iteration {i}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        return self.weights, self.cost_history\n",
    "    \n",
    "    def stochastic_gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        Stochastic gradient descent\n",
    "        Uses one sample at a time for each update\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.random.normal(0, 0.01, (X_with_bias.shape[1], 1))\n",
    "        self.cost_history = []\n",
    "        \n",
    "        m = len(y)\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            # Shuffle data for each epoch\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_with_bias[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_cost = 0\n",
    "            \n",
    "            # Process one sample at a time\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i:i+1]  # Single sample\n",
    "                yi = y_shuffled[i:i+1]\n",
    "                \n",
    "                # Forward pass\n",
    "                prediction = xi.dot(self.weights)\n",
    "                error = prediction - yi\n",
    "                \n",
    "                # Compute gradient for single sample\n",
    "                gradient = xi.T.dot(error)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "                \n",
    "                # Accumulate cost for epoch\n",
    "                epoch_cost += (1 / (2 * m)) * (error ** 2)\n",
    "            \n",
    "            self.cost_history.append(epoch_cost[0, 0])\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"SGD - Epoch {epoch}: Cost = {epoch_cost[0, 0]:.6f}\")\n",
    "        \n",
    "        return self.weights, self.cost_history\n",
    "    \n",
    "    def mini_batch_gradient_descent(self, X, y, batch_size=32):\n",
    "        \"\"\"\n",
    "        Mini-batch gradient descent\n",
    "        Uses small batches for each update\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.random.normal(0, 0.01, (X_with_bias.shape[1], 1))\n",
    "        self.cost_history = []\n",
    "        \n",
    "        m = len(y)\n",
    "        n_batches = (m + batch_size - 1) // batch_size\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_with_bias[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_cost = 0\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min((batch_idx + 1) * batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                batch_size_actual = len(X_batch)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = X_batch.dot(self.weights)\n",
    "                errors = predictions - y_batch\n",
    "                \n",
    "                # Compute gradients for batch\n",
    "                gradients = (1 / batch_size_actual) * X_batch.T.dot(errors)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * gradients\n",
    "                \n",
    "                # Accumulate cost\n",
    "                batch_cost = (1 / (2 * batch_size_actual)) * np.sum(errors ** 2)\n",
    "                epoch_cost += batch_cost * batch_size_actual / m\n",
    "            \n",
    "            self.cost_history.append(epoch_cost)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Mini-batch GD (batch_size={batch_size}) - Epoch {epoch}: Cost = {epoch_cost:.6f}\")\n",
    "        \n",
    "        return self.weights, self.cost_history\n",
    "\n",
    "print(\"✓ GradientDescentVariants class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset to better see the differences\n",
    "X_large, y_large = create_regression_dataset(n_samples=500, n_features=2, noise=0.1)\n",
    "scaler_large = StandardScaler()\n",
    "X_large_scaled = scaler_large.fit_transform(X_large)\n",
    "\n",
    "print(f\"Large dataset created: {X_large_scaled.shape}\")\n",
    "\n",
    "# Compare different gradient descent variants\n",
    "variants = {\n",
    "    'Batch GD': ('batch_gradient_descent', {}),\n",
    "    'Stochastic GD': ('stochastic_gradient_descent', {}),\n",
    "    'Mini-batch GD (32)': ('mini_batch_gradient_descent', {'batch_size': 32}),\n",
    "    'Mini-batch GD (64)': ('mini_batch_gradient_descent', {'batch_size': 64})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "learning_rate = 0.01\n",
    "max_iter = 200\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (name, (method_name, kwargs)) in enumerate(variants.items()):\n",
    "    print(f\"\\nRunning {name}...\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create model\n",
    "    model = GradientDescentVariants(learning_rate=learning_rate, max_iterations=max_iter)\n",
    "    \n",
    "    # Get method and run it\n",
    "    method = getattr(model, method_name)\n",
    "    weights, cost_history = method(X_large_scaled, y_large, **kwargs)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'weights': weights,\n",
    "        'cost_history': cost_history,\n",
    "        'final_cost': cost_history[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
    "    \n",
    "    # Plot cost history\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(cost_history, linewidth=2, label=name)\n",
    "    plt.title(f'{name}')\n",
    "    plt.xlabel('Iterations/Epochs')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "plt.suptitle('Comparison of Gradient Descent Variants', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare all variants in one plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    plt.plot(result['cost_history'], color=colors[i], linewidth=2, \n",
    "            label=f\"{name} (final: {result['final_cost']:.4f})\")\n",
    "\n",
    "plt.xlabel('Iterations/Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Convergence: Gradient Descent Variants Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Gradient descent variants comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Gradient Descent Analysis\n",
    "\n",
    "Let's analyze how gradient descent behaves with different conditions and understand common challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_learning_rate_effects():\n",
    "    \"\"\"\n",
    "    Analyze the effects of different learning rates in detail\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    X_analysis, y_analysis = create_regression_dataset(n_samples=200, n_features=1, noise=0.05)\n",
    "    X_analysis_scaled = StandardScaler().fit_transform(X_analysis)\n",
    "    \n",
    "    # Test a wide range of learning rates\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.3, 0.5, 0.8, 1.0, 1.5]\n",
    "    max_iterations = 300\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        model = GradientDescentVariants(learning_rate=lr, max_iterations=max_iterations)\n",
    "        \n",
    "        try:\n",
    "            weights, cost_history = model.batch_gradient_descent(X_analysis_scaled, y_analysis)\n",
    "            \n",
    "            results[lr] = {\n",
    "                'cost_history': cost_history,\n",
    "                'converged': True,\n",
    "                'final_cost': cost_history[-1],\n",
    "                'weights': weights\n",
    "            }\n",
    "            \n",
    "            # Check for divergence\n",
    "            if np.any(np.isnan(cost_history)) or np.any(np.isinf(cost_history)):\n",
    "                results[lr]['converged'] = False\n",
    "                results[lr]['final_cost'] = np.inf\n",
    "            \n",
    "        except:\n",
    "            results[lr] = {\n",
    "                'cost_history': [np.inf] * max_iterations,\n",
    "                'converged': False,\n",
    "                'final_cost': np.inf,\n",
    "                'weights': None\n",
    "            }\n",
    "        \n",
    "        # Plot cost curves\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        \n",
    "        if results[lr]['converged'] and results[lr]['final_cost'] != np.inf:\n",
    "            plt.plot(cost_history, 'b-', linewidth=2)\n",
    "            plt.title(f'LR = {lr}\\n✓ Converged')\n",
    "            plt.yscale('log')\n",
    "        else:\n",
    "            plt.plot(cost_history[:10], 'r-', linewidth=2)\n",
    "            plt.title(f'LR = {lr}\\n✗ Diverged')\n",
    "        \n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(bottom=1e-8)\n",
    "    \n",
    "    plt.suptitle('Learning Rate Effects on Convergence', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(\"Learning Rate Analysis Summary:\")\n",
    "    print(\"=\" * 35)\n",
    "    print(f\"{'LR':<6} {'Converged':<10} {'Final Cost':<12} {'Status':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        result = results[lr]\n",
    "        converged = \"Yes\" if result['converged'] else \"No\"\n",
    "        final_cost = f\"{result['final_cost']:.6f}\" if result['final_cost'] != np.inf else \"Inf\"\n",
    "        \n",
    "        if not result['converged']:\n",
    "            status = \"Diverged\"\n",
    "        elif lr < 0.01:\n",
    "            status = \"Too Slow\"\n",
    "        elif lr > 0.5:\n",
    "            status = \"Unstable\"\n",
    "        else:\n",
    "            status = \"Good\"\n",
    "        \n",
    "        print(f\"{lr:<6} {converged:<10} {final_cost:<12} {status:<15}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "lr_analysis_results = analyze_learning_rate_effects()\n",
    "print(\"\\n✓ Learning rate analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_local_minima():\n",
    "    \"\"\"\n",
    "    Demonstrate gradient descent behavior with non-convex functions\n",
    "    \"\"\"\n",
    "    def non_convex_function(x):\n",
    "        \"\"\"\n",
    "        Non-convex function with multiple local minima\n",
    "        f(x) = x^4 - 4x^3 + 4x^2 + 2\n",
    "        \"\"\"\n",
    "        return x**4 - 4*x**3 + 4*x**2 + 2\n",
    "    \n",
    "    def non_convex_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of non-convex function\n",
    "        f'(x) = 4x^3 - 12x^2 + 8x\n",
    "        \"\"\"\n",
    "        return 4*x**3 - 12*x**2 + 8*x\n",
    "    \n",
    "    # Test different starting points\n",
    "    starting_points = [-0.5, 0.5, 1.5, 2.5, 3.5]\n",
    "    learning_rate = 0.01\n",
    "    max_iterations = 1000\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot the function\n",
    "    x_range = np.linspace(-1, 4, 1000)\n",
    "    y_range = non_convex_function(x_range)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x_range, y_range, 'b-', linewidth=3, alpha=0.7, label='f(x) = x⁴ - 4x³ + 4x² + 2')\n",
    "    \n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    results = []\n",
    "    \n",
    "    for i, start_x in enumerate(starting_points):\n",
    "        # Run gradient descent\n",
    "        x_hist, cost_hist = gradient_descent_1d(\n",
    "            start_x, learning_rate, max_iterations,\n",
    "            non_convex_function, non_convex_derivative\n",
    "        )\n",
    "        \n",
    "        results.append((start_x, x_hist, cost_hist))\n",
    "        \n",
    "        # Plot optimization path\n",
    "        plt.plot(x_hist, cost_hist, 'o-', color=colors[i], \n",
    "                markersize=3, linewidth=1, alpha=0.8,\n",
    "                label=f'Start: {start_x}, End: {x_hist[-1]:.3f}')\n",
    "        \n",
    "        # Mark start and end points\n",
    "        plt.plot(start_x, non_convex_function(start_x), 'o', \n",
    "                color=colors[i], markersize=10, markeredgecolor='black', markeredgewidth=2)\n",
    "        plt.plot(x_hist[-1], cost_hist[-1], 's', \n",
    "                color=colors[i], markersize=8, markeredgecolor='black', markeredgewidth=1)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title('Gradient Descent on Non-Convex Function\\n(Different Starting Points Lead to Different Minima)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-1, 4)\n",
    "    \n",
    "    # Plot cost histories\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, (start_x, x_hist, cost_hist) in enumerate(results):\n",
    "        plt.plot(cost_hist, color=colors[i], linewidth=2, \n",
    "                label=f'Start: {start_x} → {x_hist[-1]:.3f}')\n",
    "    \n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost Convergence from Different Starting Points')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"Local Minima Analysis:\")\n",
    "    print(\"=\" * 25)\n",
    "    print(f\"{'Start':<8} {'End':<8} {'Final Cost':<12} {'Local Minimum':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for start_x, x_hist, cost_hist in results:\n",
    "        final_x = x_hist[-1]\n",
    "        final_cost = cost_hist[-1]\n",
    "        \n",
    "        # Determine which local minimum was reached\n",
    "        if abs(final_x - 0) < 0.1:\n",
    "            local_min = \"x ≈ 0\"\n",
    "        elif abs(final_x - 2) < 0.1:\n",
    "            local_min = \"x ≈ 2\"\n",
    "        else:\n",
    "            local_min = f\"x ≈ {final_x:.2f}\"\n",
    "        \n",
    "        print(f\"{start_x:<8.1f} {final_x:<8.3f} {final_cost:<12.4f} {local_min:<15}\")\n",
    "\n",
    "demonstrate_local_minima()\n",
    "print(\"\\n✓ Local minima demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Gradient Descent for Logistic Regression\n",
    "\n",
    "Let's apply gradient descent to logistic regression and compare with our previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \"\"\"\n",
    "    Logistic Regression with explicit gradient descent implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        self.gradient_norms = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute logistic regression cost (cross-entropy)\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        z = X.dot(weights)\n",
    "        A = self._sigmoid(z)\n",
    "        \n",
    "        # Prevent log(0) by clipping\n",
    "        A = np.clip(A, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        cost = -1/m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "        return cost\n",
    "    \n",
    "    def _compute_gradients(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute gradients for logistic regression\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        z = X.dot(weights)\n",
    "        A = self._sigmoid(z)\n",
    "        \n",
    "        gradients = 1/m * X.T.dot(A - y)\n",
    "        return gradients\n",
    "    \n",
    "    def fit(self, X, y, variant='batch', batch_size=32):\n",
    "        \"\"\"\n",
    "        Train logistic regression using gradient descent\n",
    "        \n",
    "        Arguments:\n",
    "        X -- features\n",
    "        y -- binary target (0 or 1)\n",
    "        variant -- 'batch', 'stochastic', or 'mini-batch'\n",
    "        batch_size -- size for mini-batch variant\n",
    "        \"\"\"\n",
    "        # Ensure y is column vector\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.random.normal(0, 0.01, (X_with_bias.shape[1], 1))\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        if variant == 'batch':\n",
    "            self._batch_gradient_descent(X_with_bias, y)\n",
    "        elif variant == 'stochastic':\n",
    "            self._stochastic_gradient_descent(X_with_bias, y)\n",
    "        elif variant == 'mini-batch':\n",
    "            self._mini_batch_gradient_descent(X_with_bias, y, batch_size)\n",
    "    \n",
    "    def _batch_gradient_descent(self, X, y):\n",
    "        \"\"\"Batch gradient descent for logistic regression\"\"\"\n",
    "        for i in range(self.max_iterations):\n",
    "            # Compute cost and gradients\n",
    "            cost = self._compute_cost(X, y, self.weights)\n",
    "            gradients = self._compute_gradients(X, y, self.weights)\n",
    "            \n",
    "            # Track progress\n",
    "            self.cost_history.append(cost)\n",
    "            self.weight_history.append(self.weights.copy())\n",
    "            grad_norm = np.linalg.norm(gradients)\n",
    "            self.gradient_norms.append(grad_norm)\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights -= self.learning_rate * gradients\n",
    "            \n",
    "            # Check convergence\n",
    "            if grad_norm < self.tolerance:\n",
    "                print(f\"Converged at iteration {i} (gradient norm: {grad_norm:.2e})\")\n",
    "                break\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Cost = {cost:.6f}, Grad norm = {grad_norm:.2e}\")\n",
    "    \n",
    "    def _mini_batch_gradient_descent(self, X, y, batch_size):\n",
    "        \"\"\"Mini-batch gradient descent for logistic regression\"\"\"\n",
    "        m = len(y)\n",
    "        n_batches = (m + batch_size - 1) // batch_size\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_cost = 0\n",
    "            epoch_grad_norm = 0\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Compute gradients for batch\n",
    "                gradients = self._compute_gradients(X_batch, y_batch, self.weights)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * gradients\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                batch_cost = self._compute_cost(X_batch, y_batch, self.weights)\n",
    "                epoch_cost += batch_cost * len(X_batch) / m\n",
    "                epoch_grad_norm += np.linalg.norm(gradients) * len(X_batch) / m\n",
    "            \n",
    "            # Track progress\n",
    "            self.cost_history.append(epoch_cost)\n",
    "            self.weight_history.append(self.weights.copy())\n",
    "            self.gradient_norms.append(epoch_grad_norm)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Cost = {epoch_cost:.6f}, Grad norm = {epoch_grad_norm:.2e}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        z = X_with_bias.dot(self.weights)\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "\n",
    "print(\"✓ LogisticRegressionGD class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_cls, y_cls = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_cls = StandardScaler()\n",
    "X_cls_scaled = scaler_cls.fit_transform(X_cls)\n",
    "\n",
    "print(f\"Classification dataset: {X_cls_scaled.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_cls)}\")\n",
    "\n",
    "# Compare different gradient descent variants for logistic regression\n",
    "variants_lr = ['batch', 'mini-batch']\n",
    "lr_models = {}\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, variant in enumerate(variants_lr):\n",
    "    print(f\"\\nTraining Logistic Regression with {variant} gradient descent...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model = LogisticRegressionGD(learning_rate=0.1, max_iterations=300)\n",
    "    model.fit(X_cls_scaled, y_cls, variant=variant, batch_size=32)\n",
    "    \n",
    "    lr_models[variant] = model\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = model.predict(X_cls_scaled)\n",
    "    accuracy = np.mean(predictions.ravel() == y_cls) * 100\n",
    "    print(f\"Final accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot cost history\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(model.cost_history, linewidth=2, label=f'{variant.title()} GD')\n",
    "    plt.xlabel('Iterations/Epochs')\n",
    "    plt.ylabel('Cost (Cross-entropy)')\n",
    "    plt.title(f'{variant.title()} Gradient Descent')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Plot gradient norms\n",
    "    plt.subplot(2, 3, i + 3)\n",
    "    plt.plot(model.gradient_norms, linewidth=2, color='red')\n",
    "    plt.xlabel('Iterations/Epochs')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title(f'Gradient Norm - {variant.title()}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "\n",
    "# Compare cost convergence\n",
    "plt.subplot(2, 3, 3)\n",
    "for variant, model in lr_models.items():\n",
    "    plt.plot(model.cost_history, linewidth=2, label=f'{variant.title()} GD')\n",
    "\n",
    "plt.xlabel('Iterations/Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Convergence Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Decision boundary visualization\n",
    "plt.subplot(2, 3, 6)\n",
    "model_viz = lr_models['batch']\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X_cls_scaled[:, 0].min() - 1, X_cls_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_cls_scaled[:, 1].min() - 1, X_cls_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                    np.arange(y_min, y_max, h))\n",
    "\n",
    "# Get predictions on mesh\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = model_viz.predict_proba(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and data\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
    "scatter = plt.scatter(X_cls_scaled[:, 0], X_cls_scaled[:, 1], c=y_cls, \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary\\n(Batch GD)')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "\n",
    "plt.suptitle('Logistic Regression with Gradient Descent', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Logistic regression with gradient descent completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] ✅ **Environment Setup**: Set up environment and reviewed mathematical foundations\n",
    "- [ ] ✅ **1D Visualization**: Implemented and visualized 1D gradient descent\n",
    "- [ ] ✅ **Linear Regression GD**: Built complete gradient descent for linear regression\n",
    "- [ ] ✅ **2D Parameter Space**: Visualized gradient descent navigation in parameter space\n",
    "- [ ] ✅ **GD Variants**: Implemented batch, stochastic, and mini-batch gradient descent\n",
    "- [ ] ✅ **Learning Rate Analysis**: Analyzed effects of different learning rates\n",
    "- [ ] ✅ **Local Minima Demo**: Demonstrated behavior with non-convex functions\n",
    "- [ ] ✅ **Logistic Regression GD**: Applied gradient descent to logistic regression\n",
    "- [ ] ✅ **Performance Comparison**: Compared different variants and visualized results\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Gradient Descent Fundamentals:**\n",
    "1. **Core Algorithm**: θ = θ - α∇J(θ) iteratively minimizes cost function\n",
    "2. **Learning Rate**: Controls step size; too small = slow, too large = divergence\n",
    "3. **Convergence**: Algorithm stops when gradients become sufficiently small\n",
    "4. **Global vs Local**: Convex functions guarantee global minimum; non-convex may get stuck\n",
    "\n",
    "**Gradient Descent Variants:**\n",
    "- **Batch GD**: Uses entire dataset; stable but slow for large data\n",
    "- **Stochastic GD**: Uses one sample; fast but noisy updates\n",
    "- **Mini-batch GD**: Best of both; uses small batches for balanced performance\n",
    "\n",
    "**Learning Rate Effects:**\n",
    "- **Too small (< 0.01)**: Very slow convergence, may not reach minimum in time\n",
    "- **Good range (0.01-0.5)**: Stable convergence with reasonable speed\n",
    "- **Too large (> 0.5)**: Oscillations, instability, potential divergence\n",
    "\n",
    "**Practical Insights:**\n",
    "- Feature scaling improves convergence speed and stability\n",
    "- Different starting points may lead to different local minima\n",
    "- Monitoring gradient norm helps detect convergence\n",
    "- Mini-batch GD often provides best practical performance\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Slow Convergence**\n",
    "   - *Problem*: Cost decreases very slowly\n",
    "   - *Solutions*: Increase learning rate, scale features, check for proper initialization\n",
    "\n",
    "2. **Divergent Training**\n",
    "   - *Problem*: Cost increases or oscillates wildly\n",
    "   - *Solutions*: Reduce learning rate, check gradient computation, ensure proper data scaling\n",
    "\n",
    "3. **Numerical Instability**\n",
    "   - *Problem*: NaN or infinite values appear\n",
    "   - *Solutions*: Clip gradients/values, reduce learning rate, check for division by zero\n",
    "\n",
    "4. **Stuck in Local Minimum**\n",
    "   - *Problem*: Algorithm converges to suboptimal solution\n",
    "   - *Solutions*: Try different initializations, use momentum, add regularization\n",
    "\n",
    "5. **Memory Issues**\n",
    "   - *Problem*: Large datasets don't fit in memory\n",
    "   - *Solutions*: Use mini-batch or stochastic GD, implement data generators\n",
    "\n",
    "6. **Overfitting**\n",
    "   - *Problem*: Model memorizes training data\n",
    "   - *Solutions*: Add regularization, use validation set, implement early stopping\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Implementation:**\n",
    "1. ✅ Cost function decreases monotonically (for batch GD)\n",
    "2. ✅ Gradient norms decrease over time\n",
    "3. ✅ Different learning rates show expected behavior patterns\n",
    "4. ✅ Mini-batch performance balances speed and stability\n",
    "5. ✅ Final predictions achieve reasonable accuracy\n",
    "6. ✅ Parameter values converge to sensible ranges\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save notebook with all visualizations and results\n",
    "2. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del X_large_scaled, lr_analysis_results\n",
    "   # del lr_models, results\n",
    "   ```\n",
    "3. Close all plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You now have a deep understanding of gradient descent! This knowledge is fundamental for:\n",
    "- Neural network training (next lab)\n",
    "- Advanced optimizers (Adam, RMSprop)\n",
    "- Deep learning frameworks\n",
    "- Custom model development\n",
    "\n",
    "The gradient descent principles you've learned apply to all machine learning optimization problems, making this one of the most important labs in the course.\n",
    "\n",
    "**Congratulations! You've completed Lab 2.3 - Gradient Descent from Scratch!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}