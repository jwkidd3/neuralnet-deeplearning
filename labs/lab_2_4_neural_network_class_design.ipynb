{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4: Neural Network Class Design\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Design and implement a flexible neural network class from scratch\n",
    "- Understand object-oriented programming principles for ML models\n",
    "- Implement forward propagation for multi-layer networks\n",
    "- Create modular, reusable components for neural networks\n",
    "- Build a foundation for implementing various network architectures\n",
    "- Apply software engineering best practices to machine learning code\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy, Matplotlib\n",
    "- Understanding of neural network fundamentals\n",
    "- Completed previous labs (especially Lab 2.1 and 2.3)\n",
    "\n",
    "## Overview\n",
    "Creating well-structured, maintainable neural network code is crucial for real-world applications. This lab focuses on designing a robust, flexible neural network class that can handle different architectures, activation functions, and training procedures. You'll learn to build modular components that can be easily extended and modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Design Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Optional, Callable, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "print(\"\\nNeural Network Design Principles:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"1. Modularity: Separate concerns (layers, activations, optimizers)\")\n",
    "print(\"2. Flexibility: Support different architectures and hyperparameters\")\n",
    "print(\"3. Extensibility: Easy to add new components\")\n",
    "print(\"4. Maintainability: Clean, readable, well-documented code\")\n",
    "print(\"5. Performance: Efficient NumPy operations\")\n",
    "print(\"6. Debugging: Built-in logging and diagnostic tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Activation Function Components\n",
    "\n",
    "Let's start by creating a modular activation function system that can be easily extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for activation functions\n",
    "    Defines the interface that all activation functions must implement\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the activation function\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "        \n",
    "        Returns:\n",
    "            Activated output\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function\n",
    "        \n",
    "        Args:\n",
    "            x: Input array (pre-activation values)\n",
    "        \n",
    "        Returns:\n",
    "            Derivative of activation function\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Return the name of the activation function\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Clip to prevent overflow\n",
    "        x_clipped = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        s = self.forward(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Sigmoid\"\n",
    "\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Tanh\"\n",
    "\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit: ReLU(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"ReLU\"\n",
    "\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Leaky ReLU: LeakyReLU(x) = max(αx, x) where α is a small constant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1.0, self.alpha)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return f\"LeakyReLU(α={self.alpha})\"\n",
    "\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Linear activation function: f(x) = x (identity function)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Linear\"\n",
    "\n",
    "\n",
    "# Activation function factory\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'sigmoid': Sigmoid,\n",
    "    'tanh': Tanh,\n",
    "    'relu': ReLU,\n",
    "    'leaky_relu': LeakyReLU,\n",
    "    'linear': Linear\n",
    "}\n",
    "\n",
    "\n",
    "def get_activation(name: str, **kwargs) -> ActivationFunction:\n",
    "    \"\"\"\n",
    "    Factory function to create activation functions\n",
    "    \n",
    "    Args:\n",
    "        name: Name of activation function\n",
    "        **kwargs: Additional parameters for activation function\n",
    "    \n",
    "    Returns:\n",
    "        Activation function instance\n",
    "    \"\"\"\n",
    "    if name.lower() not in ACTIVATION_FUNCTIONS:\n",
    "        raise ValueError(f\"Unknown activation function: {name}. Available: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "    \n",
    "    return ACTIVATION_FUNCTIONS[name.lower()](**kwargs)\n",
    "\n",
    "\n",
    "print(\"✓ Activation function components implemented successfully!\")\n",
    "print(f\"Available activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Layer Components\n",
    "\n",
    "Now let's create a modular layer system that forms the building blocks of our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Neural network layer with weights, biases, and activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, \n",
    "                 activation: str = 'sigmoid',\n",
    "                 weight_init: str = 'xavier',\n",
    "                 **activation_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize layer\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input neurons\n",
    "            output_size: Number of output neurons\n",
    "            activation: Name of activation function\n",
    "            weight_init: Weight initialization method\n",
    "            **activation_kwargs: Additional activation function parameters\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = get_activation(activation, **activation_kwargs)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = self._initialize_weights(weight_init)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store intermediate values for backpropagation\n",
    "        self.last_input = None\n",
    "        self.last_z = None\n",
    "        self.last_activation = None\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def _initialize_weights(self, method: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Initialize weights using specified method\n",
    "        \n",
    "        Args:\n",
    "            method: Initialization method ('xavier', 'he', 'random', 'zeros')\n",
    "        \n",
    "        Returns:\n",
    "            Initialized weight matrix\n",
    "        \"\"\"\n",
    "        if method == 'xavier':\n",
    "            # Xavier/Glorot initialization\n",
    "            limit = np.sqrt(6 / (self.input_size + self.output_size))\n",
    "            return np.random.uniform(-limit, limit, (self.input_size, self.output_size))\n",
    "        \n",
    "        elif method == 'he':\n",
    "            # He initialization (good for ReLU)\n",
    "            return np.random.normal(0, np.sqrt(2 / self.input_size), \n",
    "                                  (self.input_size, self.output_size))\n",
    "        \n",
    "        elif method == 'random':\n",
    "            # Simple random initialization\n",
    "            return np.random.normal(0, 0.01, (self.input_size, self.output_size))\n",
    "        \n",
    "        elif method == 'zeros':\n",
    "            # Zero initialization (not recommended for hidden layers)\n",
    "            return np.zeros((self.input_size, self.output_size))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight initialization method: {method}\")\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward propagation through the layer\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Layer output (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Store input for backpropagation\n",
    "        self.last_input = X\n",
    "        \n",
    "        # Linear transformation: Z = XW + b\n",
    "        self.last_z = np.dot(X, self.weights) + self.biases\n",
    "        \n",
    "        # Apply activation function\n",
    "        self.last_activation = self.activation.forward(self.last_z)\n",
    "        \n",
    "        return self.last_activation\n",
    "    \n",
    "    def backward(self, dA: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward propagation through the layer\n",
    "        \n",
    "        Args:\n",
    "            dA: Gradient of loss with respect to layer output\n",
    "        \n",
    "        Returns:\n",
    "            Gradient of loss with respect to layer input\n",
    "        \"\"\"\n",
    "        m = self.last_input.shape[0]\n",
    "        \n",
    "        # Gradient of activation function\n",
    "        dZ = dA * self.activation.backward(self.last_z)\n",
    "        \n",
    "        # Gradients of weights and biases\n",
    "        self.dW = (1/m) * np.dot(self.last_input.T, dZ)\n",
    "        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to input (for previous layer)\n",
    "        dA_prev = np.dot(dZ, self.weights.T)\n",
    "        \n",
    "        return dA_prev\n",
    "    \n",
    "    def update_parameters(self, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Update layer parameters using gradients\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.dW\n",
    "        self.biases -= learning_rate * self.db\n",
    "    \n",
    "    def get_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get layer parameters\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing weights and biases\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'weights': self.weights.copy(),\n",
    "            'biases': self.biases.copy()\n",
    "        }\n",
    "    \n",
    "    def set_parameters(self, params: Dict[str, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Set layer parameters\n",
    "        \n",
    "        Args:\n",
    "            params: Dictionary containing weights and biases\n",
    "        \"\"\"\n",
    "        self.weights = params['weights'].copy()\n",
    "        self.biases = params['biases'].copy()\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"Layer(input={self.input_size}, output={self.output_size}, \"\n",
    "                f\"activation={self.activation.name})\")\n",
    "\n",
    "\n",
    "print(\"✓ Layer components implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Loss Function Components\n",
    "\n",
    "Let's create a modular loss function system for different types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for loss functions\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted values\n",
    "            y_true: True values\n",
    "        \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of loss with respect to predictions\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted values\n",
    "            y_true: True values\n",
    "        \n",
    "        Returns:\n",
    "            Gradient of loss\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Return the name of the loss function\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss: MSE = (1/2m) * Σ(y_pred - y_true)²\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        m = y_true.shape[0]\n",
    "        return (1/(2*m)) * np.sum((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        m = y_true.shape[0]\n",
    "        return (1/m) * (y_pred - y_true)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Mean Squared Error\"\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(LossFunction):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy loss: BCE = -1/m * Σ[y*log(p) + (1-y)*log(1-p)]\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        return -1/m * np.sum(y_true * np.log(y_pred_clipped) + \n",
    "                           (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Clip predictions to prevent division by zero\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        return -1/m * (y_true / y_pred_clipped - (1 - y_true) / (1 - y_pred_clipped))\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Binary Cross Entropy\"\n",
    "\n",
    "\n",
    "class MeanAbsoluteError(LossFunction):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error loss: MAE = 1/m * Σ|y_pred - y_true|\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        m = y_true.shape[0]\n",
    "        return (1/m) * np.sum(np.abs(y_pred - y_true))\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        m = y_true.shape[0]\n",
    "        return (1/m) * np.sign(y_pred - y_true)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Mean Absolute Error\"\n",
    "\n",
    "\n",
    "# Loss function factory\n",
    "LOSS_FUNCTIONS = {\n",
    "    'mse': MeanSquaredError,\n",
    "    'binary_crossentropy': BinaryCrossEntropy,\n",
    "    'mae': MeanAbsoluteError\n",
    "}\n",
    "\n",
    "\n",
    "def get_loss_function(name: str) -> LossFunction:\n",
    "    \"\"\"\n",
    "    Factory function to create loss functions\n",
    "    \n",
    "    Args:\n",
    "        name: Name of loss function\n",
    "    \n",
    "    Returns:\n",
    "        Loss function instance\n",
    "    \"\"\"\n",
    "    if name.lower() not in LOSS_FUNCTIONS:\n",
    "        raise ValueError(f\"Unknown loss function: {name}. Available: {list(LOSS_FUNCTIONS.keys())}\")\n",
    "    \n",
    "    return LOSS_FUNCTIONS[name.lower()]()\n",
    "\n",
    "\n",
    "print(\"✓ Loss function components implemented successfully!\")\n",
    "print(f\"Available loss functions: {list(LOSS_FUNCTIONS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Neural Network Class Design\n",
    "\n",
    "Now let's create the main neural network class that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Flexible Neural Network class supporting multiple layers and configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, architecture: List[Dict[str, Any]], \n",
    "                 loss_function: str = 'mse',\n",
    "                 learning_rate: float = 0.01,\n",
    "                 random_state: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize neural network\n",
    "        \n",
    "        Args:\n",
    "            architecture: List of layer configurations\n",
    "                Example: [\n",
    "                    {'input_size': 4, 'output_size': 8, 'activation': 'relu'},\n",
    "                    {'input_size': 8, 'output_size': 4, 'activation': 'relu'},\n",
    "                    {'input_size': 4, 'output_size': 1, 'activation': 'sigmoid'}\n",
    "                ]\n",
    "            loss_function: Name of loss function to use\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        self.architecture = architecture\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = get_loss_function(loss_function)\n",
    "        \n",
    "        # Build network layers\n",
    "        self.layers = self._build_layers()\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'loss': [],\n",
    "            'accuracy': []\n",
    "        }\n",
    "        \n",
    "        # Network properties\n",
    "        self.is_trained = False\n",
    "        self.n_parameters = self._count_parameters()\n",
    "    \n",
    "    def _build_layers(self) -> List[Layer]:\n",
    "        \"\"\"\n",
    "        Build network layers from architecture specification\n",
    "        \n",
    "        Returns:\n",
    "            List of Layer objects\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        for layer_config in self.architecture:\n",
    "            # Extract layer parameters\n",
    "            input_size = layer_config['input_size']\n",
    "            output_size = layer_config['output_size']\n",
    "            activation = layer_config.get('activation', 'sigmoid')\n",
    "            weight_init = layer_config.get('weight_init', 'xavier')\n",
    "            \n",
    "            # Extract activation function parameters\n",
    "            activation_kwargs = {}\n",
    "            if 'activation_params' in layer_config:\n",
    "                activation_kwargs = layer_config['activation_params']\n",
    "            \n",
    "            # Create layer\n",
    "            layer = Layer(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                activation=activation,\n",
    "                weight_init=weight_init,\n",
    "                **activation_kwargs\n",
    "            )\n",
    "            \n",
    "            layers.append(layer)\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def _count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Count total number of trainable parameters\n",
    "        \n",
    "        Returns:\n",
    "            Total number of parameters\n",
    "        \"\"\"\n",
    "        total_params = 0\n",
    "        for layer in self.layers:\n",
    "            # Weights + biases\n",
    "            total_params += layer.weights.size + layer.biases.size\n",
    "        return total_params\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward propagation through the entire network\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_features)\n",
    "        \n",
    "        Returns:\n",
    "            Network output (batch_size, output_features)\n",
    "        \"\"\"\n",
    "        activation = X\n",
    "        \n",
    "        # Pass through each layer\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(activation)\n",
    "        \n",
    "        return activation\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        \"\"\"\n",
    "        Backward propagation through the entire network\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Network predictions\n",
    "            y_true: True target values\n",
    "        \"\"\"\n",
    "        # Compute loss gradient\n",
    "        dA = self.loss_function.backward(y_pred, y_true)\n",
    "        \n",
    "        # Backpropagate through layers (in reverse order)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Update all layer parameters using computed gradients\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(self.learning_rate)\n",
    "    \n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Perform one training step (forward + backward + update)\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (loss, accuracy)\n",
    "        \"\"\"\n",
    "        # Forward propagation\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_function.forward(y_pred, y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        self.backward(y_pred, y)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = self._calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def _calculate_accuracy(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate accuracy based on problem type\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted values\n",
    "            y_true: True values\n",
    "        \n",
    "        Returns:\n",
    "            Accuracy score\n",
    "        \"\"\"\n",
    "        if self.loss_function.name == 'Binary Cross Entropy':\n",
    "            # Binary classification accuracy\n",
    "            predictions = (y_pred > 0.5).astype(int)\n",
    "            return np.mean(predictions == y_true)\n",
    "        else:\n",
    "            # For regression, we'll use R² score as \"accuracy\"\n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            return max(0, r2)  # Return 0 if R² is negative\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, \n",
    "            epochs: int = 1000,\n",
    "            batch_size: Optional[int] = None,\n",
    "            validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n",
    "            verbose: int = 1) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \n",
    "        Args:\n",
    "            X: Training features\n",
    "            y: Training targets\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size (None = full batch)\n",
    "            validation_data: Validation data tuple (X_val, y_val)\n",
    "            verbose: Verbosity level (0=silent, 1=progress bar, 2=detailed)\n",
    "        \n",
    "        Returns:\n",
    "            Training history dictionary\n",
    "        \"\"\"\n",
    "        # Ensure correct shapes\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Initialize history\n",
    "        self.training_history = {\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # Batch training\n",
    "            if batch_size is None or batch_size >= n_samples:\n",
    "                # Full batch\n",
    "                loss, accuracy = self.train_step(X, y)\n",
    "                epoch_loss = loss\n",
    "                epoch_accuracy = accuracy\n",
    "                n_batches = 1\n",
    "            else:\n",
    "                # Mini-batch training\n",
    "                indices = np.random.permutation(n_samples)\n",
    "                \n",
    "                for i in range(0, n_samples, batch_size):\n",
    "                    batch_indices = indices[i:i+batch_size]\n",
    "                    X_batch = X[batch_indices]\n",
    "                    y_batch = y[batch_indices]\n",
    "                    \n",
    "                    loss, accuracy = self.train_step(X_batch, y_batch)\n",
    "                    epoch_loss += loss\n",
    "                    epoch_accuracy += accuracy\n",
    "                    n_batches += 1\n",
    "                \n",
    "                epoch_loss /= n_batches\n",
    "                epoch_accuracy /= n_batches\n",
    "            \n",
    "            # Record training metrics\n",
    "            self.training_history['loss'].append(epoch_loss)\n",
    "            self.training_history['accuracy'].append(epoch_accuracy)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                if y_val.ndim == 1:\n",
    "                    y_val = y_val.reshape(-1, 1)\n",
    "                \n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self.loss_function.forward(y_val_pred, y_val)\n",
    "                val_accuracy = self._calculate_accuracy(y_val_pred, y_val)\n",
    "                \n",
    "                self.training_history['val_loss'].append(val_loss)\n",
    "                self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose > 0 and (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "                print(f\"  Loss: {epoch_loss:.6f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "                \n",
    "                if validation_data is not None:\n",
    "                    print(f\"  Val Loss: {val_loss:.6f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "                print()\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return self.training_history\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "        \n",
    "        Returns:\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the network on test data\n",
    "        \n",
    "        Args:\n",
    "            X: Test features\n",
    "            y: Test targets\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        loss = self.loss_function.forward(y_pred, y)\n",
    "        accuracy = self._calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Print network architecture summary\n",
    "        \"\"\"\n",
    "        print(\"Neural Network Architecture Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Loss Function: {self.loss_function.name}\")\n",
    "        print(f\"Learning Rate: {self.learning_rate}\")\n",
    "        print(f\"Total Parameters: {self.n_parameters:,}\")\n",
    "        print(f\"Trained: {'Yes' if self.is_trained else 'No'}\")\n",
    "        print(\"\\nLayer Architecture:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        total_params = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_params = layer.weights.size + layer.biases.size\n",
    "            total_params += layer_params\n",
    "            \n",
    "            print(f\"Layer {i + 1}: {layer.input_size:3d} → {layer.output_size:3d} \"\n",
    "                  f\"({layer.activation.name:<10}) [{layer_params:,} params]\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Total: {total_params:,} parameters\")\n",
    "        \n",
    "        if self.is_trained:\n",
    "            print(f\"\\nTraining Results:\")\n",
    "            print(f\"Final Loss: {self.training_history['loss'][-1]:.6f}\")\n",
    "            print(f\"Final Accuracy: {self.training_history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"✓ Neural Network class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Neural Network Design\n",
    "\n",
    "Let's test our neural network design with different architectures and problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple Binary Classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Test 1: Binary Classification with Simple Architecture\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=4,\n",
    "    n_redundant=0,\n",
    "    n_informative=4,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset: {X_train_scaled.shape[0]} training, {X_test_scaled.shape[0]} test samples\")\n",
    "print(f\"Features: {X_train_scaled.shape[1]}, Classes: {len(np.unique(y))}\")\n",
    "\n",
    "# Define network architecture\n",
    "architecture_simple = [\n",
    "    {'input_size': 4, 'output_size': 8, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 8, 'output_size': 4, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 4, 'output_size': 1, 'activation': 'sigmoid', 'weight_init': 'xavier'}\n",
    "]\n",
    "\n",
    "# Create and train network\n",
    "nn_simple = NeuralNetwork(\n",
    "    architecture=architecture_simple,\n",
    "    loss_function='binary_crossentropy',\n",
    "    learning_rate=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "nn_simple.summary()\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "# Train the network\n",
    "history = nn_simple.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = nn_simple.evaluate(X_train_scaled, y_train)\n",
    "test_metrics = nn_simple.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training - Loss: {train_metrics['loss']:.6f}, Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test - Loss: {test_metrics['loss']:.6f}, Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Test 1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Regression with Deep Architecture\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "print(\"\\nTest 2: Regression with Deep Architecture\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=400,\n",
    "    n_features=6,\n",
    "    noise=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Scale targets too for better training\n",
    "y_scaler = StandardScaler()\n",
    "y_train_reg_scaled = y_scaler.fit_transform(y_train_reg.reshape(-1, 1)).ravel()\n",
    "y_test_reg_scaled = y_scaler.transform(y_test_reg.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"Dataset: {X_train_reg_scaled.shape[0]} training, {X_test_reg_scaled.shape[0]} test samples\")\n",
    "print(f\"Features: {X_train_reg_scaled.shape[1]}\")\n",
    "\n",
    "# Define deeper architecture\n",
    "architecture_deep = [\n",
    "    {'input_size': 6, 'output_size': 16, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 16, 'output_size': 12, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 12, 'output_size': 8, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 8, 'output_size': 4, 'activation': 'relu', 'weight_init': 'he'},\n",
    "    {'input_size': 4, 'output_size': 1, 'activation': 'linear', 'weight_init': 'xavier'}\n",
    "]\n",
    "\n",
    "# Create and train network\n",
    "nn_deep = NeuralNetwork(\n",
    "    architecture=architecture_deep,\n",
    "    loss_function='mse',\n",
    "    learning_rate=0.001,  # Lower learning rate for regression\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "nn_deep.summary()\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "# Train the network\n",
    "history_deep = nn_deep.fit(\n",
    "    X_train_reg_scaled, y_train_reg_scaled,\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test_reg_scaled, y_test_reg_scaled),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_reg = nn_deep.evaluate(X_train_reg_scaled, y_train_reg_scaled)\n",
    "test_metrics_reg = nn_deep.evaluate(X_test_reg_scaled, y_test_reg_scaled)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training - Loss: {train_metrics_reg['loss']:.6f}, R²: {train_metrics_reg['accuracy']:.4f}\")\n",
    "print(f\"Test - Loss: {test_metrics_reg['loss']:.6f}, R²: {test_metrics_reg['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Test 2 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Network Performance and Architecture\n",
    "\n",
    "Let's create visualizations to analyze our neural network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function for training history\n",
    "def plot_training_history(history: Dict[str, List[float]], title: str = \"Training History\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "        axes[0].plot(history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Model Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    if 'val_accuracy' in history and history['val_accuracy']:\n",
    "        axes[1].plot(history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy/R²')\n",
    "    axes[1].set_title('Model Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "print(\"Training History Visualizations\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "plot_training_history(history, \"Binary Classification Network\")\n",
    "plot_training_history(history_deep, \"Deep Regression Network\")\n",
    "\n",
    "print(\"✓ Training history visualizations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network architecture\n",
    "def visualize_network_architecture(network: NeuralNetwork, title: str = \"Network Architecture\"):\n",
    "    \"\"\"\n",
    "    Visualize network architecture\n",
    "    \n",
    "    Args:\n",
    "        network: Neural network instance\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Architecture diagram\n",
    "    ax1 = axes[0]\n",
    "    layer_sizes = []\n",
    "    activation_names = []\n",
    "    \n",
    "    for i, layer in enumerate(network.layers):\n",
    "        if i == 0:\n",
    "            layer_sizes.append(layer.input_size)\n",
    "        layer_sizes.append(layer.output_size)\n",
    "        activation_names.append(layer.activation.name)\n",
    "    \n",
    "    # Plot layer sizes as bars\n",
    "    x_pos = np.arange(len(layer_sizes))\n",
    "    bars = ax1.bar(x_pos, layer_sizes, alpha=0.7, color=['lightblue' if i == 0 or i == len(layer_sizes)-1 else 'lightcoral' for i in range(len(layer_sizes))])\n",
    "    \n",
    "    # Add labels\n",
    "    ax1.set_xlabel('Layer')\n",
    "    ax1.set_ylabel('Number of Neurons')\n",
    "    ax1.set_title('Layer Sizes')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    layer_labels = ['Input'] + [f'Hidden {i}' for i in range(1, len(layer_sizes)-1)] + ['Output']\n",
    "    ax1.set_xticklabels(layer_labels, rotation=45)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, size in zip(bars, layer_sizes):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{size}', ha='center', va='bottom')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter distribution\n",
    "    ax2 = axes[1]\n",
    "    param_counts = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for i, layer in enumerate(network.layers):\n",
    "        param_count = layer.weights.size + layer.biases.size\n",
    "        param_counts.append(param_count)\n",
    "        layer_names.append(f'Layer {i+1}\\n({layer.activation.name})')\n",
    "    \n",
    "    # Create pie chart for parameter distribution\n",
    "    ax2.pie(param_counts, labels=layer_names, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Parameter Distribution')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed layer information\n",
    "    print(f\"\\nDetailed Layer Information for {title}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, layer in enumerate(network.layers):\n",
    "        param_count = layer.weights.size + layer.biases.size\n",
    "        print(f\"Layer {i+1}: {layer.input_size} → {layer.output_size}\")\n",
    "        print(f\"  Activation: {layer.activation.name}\")\n",
    "        print(f\"  Parameters: {param_count:,} (Weights: {layer.weights.size:,}, Biases: {layer.biases.size})\")\n",
    "        print(f\"  Weight shape: {layer.weights.shape}\")\n",
    "        print(f\"  Weight range: [{layer.weights.min():.4f}, {layer.weights.max():.4f}]\")\n",
    "        print()\n",
    "\n",
    "# Visualize both networks\n",
    "print(\"Network Architecture Visualizations\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "visualize_network_architecture(nn_simple, \"Binary Classification Network\")\n",
    "visualize_network_architecture(nn_deep, \"Deep Regression Network\")\n",
    "\n",
    "print(\"\\n✓ Architecture visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Different Activation Functions and Architectures\n",
    "\n",
    "Let's compare how different activation functions and architectures perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function comparison\n",
    "print(\"Activation Function Comparison\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create a simple dataset for comparison\n",
    "X_comp, y_comp = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=3,\n",
    "    n_redundant=0,\n",
    "    n_informative=3,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
    "    X_comp, y_comp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_comp = StandardScaler()\n",
    "X_train_comp_scaled = scaler_comp.fit_transform(X_train_comp)\n",
    "X_test_comp_scaled = scaler_comp.transform(X_test_comp)\n",
    "\n",
    "# Test different activation functions\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
    "activation_results = {}\n",
    "\n",
    "for activation in activations_to_test:\n",
    "    print(f\"\\nTesting {activation} activation...\")\n",
    "    \n",
    "    # Define architecture with current activation\n",
    "    architecture_test = [\n",
    "        {'input_size': 3, 'output_size': 6, 'activation': activation, 'weight_init': 'he' if activation == 'relu' else 'xavier'},\n",
    "        {'input_size': 6, 'output_size': 4, 'activation': activation, 'weight_init': 'he' if activation == 'relu' else 'xavier'},\n",
    "        {'input_size': 4, 'output_size': 1, 'activation': 'sigmoid', 'weight_init': 'xavier'}\n",
    "    ]\n",
    "    \n",
    "    # Create and train network\n",
    "    nn_test = NeuralNetwork(\n",
    "        architecture=architecture_test,\n",
    "        loss_function='binary_crossentropy',\n",
    "        learning_rate=0.01,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train with reduced epochs for comparison\n",
    "    history_test = nn_test.fit(\n",
    "        X_train_comp_scaled, y_train_comp,\n",
    "        epochs=150,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_comp_scaled, y_test_comp),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_metrics = nn_test.evaluate(X_test_comp_scaled, y_test_comp)\n",
    "    \n",
    "    activation_results[activation] = {\n",
    "        'network': nn_test,\n",
    "        'history': history_test,\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'final_train_loss': history_test['loss'][-1],\n",
    "        'final_train_accuracy': history_test['accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Test Loss: {test_metrics['loss']:.6f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nActivation Function Comparison Results:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Activation':<12} {'Test Acc':<10} {'Test Loss':<12} {'Train Acc':<10} {'Train Loss':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for activation, results in activation_results.items():\n",
    "    print(f\"{activation:<12} {results['test_accuracy']:<10.4f} {results['test_loss']:<12.6f} \"\n",
    "          f\"{results['final_train_accuracy']:<10.4f} {results['final_train_loss']:<12.6f}\")\n",
    "\n",
    "print(\"\\n✓ Activation function comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation function comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training loss comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for activation, results in activation_results.items():\n",
    "    plt.plot(results['history']['loss'], label=f'{activation.title()}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss by Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Training accuracy comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "for activation, results in activation_results.items():\n",
    "    plt.plot(results['history']['accuracy'], label=f'{activation.title()}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy by Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final test performance comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "activations = list(activation_results.keys())\n",
    "test_accuracies = [activation_results[act]['test_accuracy'] for act in activations]\n",
    "test_losses = [activation_results[act]['test_loss'] for act in activations]\n",
    "\n",
    "x_pos = np.arange(len(activations))\n",
    "plt.bar(x_pos, test_accuracies, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xticks(x_pos, [act.title() for act in activations])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, acc in enumerate(test_accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Test loss comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(x_pos, test_losses, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss Comparison')\n",
    "plt.xticks(x_pos, [act.title() for act in activations])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, loss in enumerate(test_losses):\n",
    "    plt.text(i, loss + max(test_losses) * 0.02, f'{loss:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Activation Function Performance Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Activation function comparison plots completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] ✅ **Environment Setup**: Imported libraries and established design principles\n",
    "- [ ] ✅ **Activation Functions**: Built modular activation function system with abstract base class\n",
    "- [ ] ✅ **Layer Components**: Implemented flexible Layer class with forward/backward propagation\n",
    "- [ ] ✅ **Loss Functions**: Created modular loss function system for different problem types\n",
    "- [ ] ✅ **Neural Network Class**: Built comprehensive NeuralNetwork class with training capabilities\n",
    "- [ ] ✅ **Binary Classification Test**: Successfully tested network on classification problem\n",
    "- [ ] ✅ **Regression Test**: Validated network performance on regression problem\n",
    "- [ ] ✅ **Visualization Tools**: Created functions to visualize training and architecture\n",
    "- [ ] ✅ **Activation Comparison**: Compared different activation functions systematically\n",
    "- [ ] ✅ **Performance Analysis**: Analyzed and visualized network performance metrics\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Object-Oriented Design Principles:**\n",
    "1. **Abstraction**: Base classes define interfaces for components\n",
    "2. **Encapsulation**: Components manage their own state and behavior\n",
    "3. **Modularity**: Separate classes for layers, activations, and loss functions\n",
    "4. **Extensibility**: Easy to add new components without changing existing code\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "- **Layers**: Basic building blocks with weights, biases, and activation functions\n",
    "- **Forward Pass**: Data flows through layers sequentially\n",
    "- **Backward Pass**: Gradients propagate back through layers for training\n",
    "- **Parameter Updates**: Weights and biases updated using computed gradients\n",
    "\n",
    "**Design Benefits:**\n",
    "- **Flexibility**: Support for different architectures and hyperparameters\n",
    "- **Reusability**: Components can be reused across different networks\n",
    "- **Maintainability**: Clean, well-structured code that's easy to debug\n",
    "- **Scalability**: Architecture can be easily extended for complex networks\n",
    "\n",
    "**Best Practices Implemented:**\n",
    "- Type hints for better code documentation\n",
    "- Abstract base classes for consistent interfaces\n",
    "- Factory functions for component creation\n",
    "- Comprehensive error handling and validation\n",
    "- Built-in visualization and debugging tools\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Architecture Specification Errors**\n",
    "   - *Problem*: Incorrect layer dimensions or missing parameters\n",
    "   - *Solution*: Validate architecture specification, ensure input/output sizes match\n",
    "\n",
    "2. **Training Instability**\n",
    "   - *Problem*: Loss explodes or vanishes during training\n",
    "   - *Solutions*: Adjust learning rate, use appropriate weight initialization, check activation functions\n",
    "\n",
    "3. **Memory Issues**\n",
    "   - *Problem*: Large networks consume too much memory\n",
    "   - *Solutions*: Use smaller batch sizes, reduce network size, implement gradient checkpointing\n",
    "\n",
    "4. **Slow Convergence**\n",
    "   - *Problem*: Network takes too long to train\n",
    "   - *Solutions*: Increase learning rate, use better weight initialization, normalize inputs\n",
    "\n",
    "5. **Poor Performance**\n",
    "   - *Problem*: Network doesn't achieve good accuracy\n",
    "   - *Solutions*: Try different architectures, activation functions, or preprocessing steps\n",
    "\n",
    "6. **Gradient Issues**\n",
    "   - *Problem*: Vanishing or exploding gradients\n",
    "   - *Solutions*: Use appropriate activation functions, gradient clipping, better initialization\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Implementation:**\n",
    "1. ✅ Network architecture builds correctly from specification\n",
    "2. ✅ Forward propagation produces expected output shapes\n",
    "3. ✅ Backward propagation computes gradients without errors\n",
    "4. ✅ Training reduces loss consistently\n",
    "5. ✅ Different activation functions work as expected\n",
    "6. ✅ Network generalizes well to test data\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save your notebook with all implementations and results\n",
    "2. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del nn_simple, nn_deep, activation_results\n",
    "   # del X_train_scaled, X_test_scaled\n",
    "   ```\n",
    "3. Close plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Your neural network class design is now ready for advanced applications! This foundation enables:\n",
    "- **Complex Architectures**: Build deeper networks, CNNs, RNNs\n",
    "- **Advanced Optimizers**: Implement Adam, RMSprop, momentum\n",
    "- **Regularization**: Add dropout, batch normalization, L1/L2 regularization\n",
    "- **Custom Components**: Create specialized layers and loss functions\n",
    "- **Production Deployment**: Scale to real-world applications\n",
    "\n",
    "The modular design you've created follows industry best practices and will serve as a solid foundation for more advanced deep learning implementations.\n",
    "\n",
    "**Congratulations! You've completed Lab 2.4 - Neural Network Class Design!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
