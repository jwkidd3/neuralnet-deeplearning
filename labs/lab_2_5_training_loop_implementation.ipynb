{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5: Training Loop Implementation\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement comprehensive training loops with advanced features\n",
    "- Add early stopping, learning rate scheduling, and model checkpointing\n",
    "- Implement different batch processing strategies\n",
    "- Create monitoring and logging systems for training progress\n",
    "- Handle overfitting through validation monitoring\n",
    "- Build robust training pipelines for production use\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy, Matplotlib\n",
    "- Completed Lab 2.4 (Neural Network Class Design)\n",
    "- Understanding of gradient descent and backpropagation\n",
    "\n",
    "## Overview\n",
    "A well-implemented training loop is crucial for successfully training neural networks. This lab focuses on building sophisticated training loops that include modern techniques like early stopping, learning rate scheduling, and comprehensive monitoring. You'll learn to create training systems that are both robust and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Import Previous Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "print(\"\\nTraining Loop Components:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"1. Batch Processing: Handle different batch sizes and data loading\")\n",
    "print(\"2. Early Stopping: Prevent overfitting by monitoring validation metrics\")\n",
    "print(\"3. Learning Rate Scheduling: Adaptive learning rate adjustment\")\n",
    "print(\"4. Model Checkpointing: Save best models during training\")\n",
    "print(\"5. Progress Monitoring: Real-time tracking of training metrics\")\n",
    "print(\"6. Validation Evaluation: Regular validation set assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Learning Rate Schedulers\n",
    "\n",
    "Let's implement various learning rate scheduling strategies to improve training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for learning rate schedulers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        \"\"\"\n",
    "        Update learning rate based on epoch and metrics\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch number\n",
    "            metrics: Dictionary of current metrics\n",
    "        \n",
    "        Returns:\n",
    "            Updated learning rate\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class ConstantLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Constant learning rate (no scheduling)\n",
    "    \"\"\"\n",
    "    \n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Step decay learning rate scheduler\n",
    "    Reduces learning rate by gamma every step_size epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, step_size: int, gamma: float = 0.1):\n",
    "        super().__init__(initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        if epoch > 0 and epoch % self.step_size == 0:\n",
    "            self.current_lr *= self.gamma\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class ExponentialLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Exponential decay learning rate scheduler\n",
    "    lr = initial_lr * (gamma ^ epoch)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, gamma: float = 0.95):\n",
    "        super().__init__(initial_lr)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        self.current_lr = self.initial_lr * (self.gamma ** epoch)\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(LRScheduler):\n",
    "    \"\"\"\n",
    "    Reduce learning rate when a metric has stopped improving\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, metric: str = 'val_loss', \n",
    "                 mode: str = 'min', factor: float = 0.5, \n",
    "                 patience: int = 10, min_delta: float = 1e-4,\n",
    "                 min_lr: float = 1e-8):\n",
    "        super().__init__(initial_lr)\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.wait = 0\n",
    "        self.reductions = 0\n",
    "    \n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        if metrics is None or self.metric not in metrics:\n",
    "            return self.current_lr\n",
    "        \n",
    "        current_metric = metrics[self.metric]\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = current_metric < (self.best_metric - self.min_delta)\n",
    "        else:\n",
    "            improved = current_metric > (self.best_metric + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_metric = current_metric\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "            if self.wait >= self.patience:\n",
    "                if self.current_lr > self.min_lr:\n",
    "                    self.current_lr = max(self.current_lr * self.factor, self.min_lr)\n",
    "                    self.reductions += 1\n",
    "                    print(f\"  Learning rate reduced to {self.current_lr:.2e} (reduction #{self.reductions})\")\n",
    "                self.wait = 0\n",
    "        \n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class CosineAnnealingLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing learning rate scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, T_max: int, eta_min: float = 0):\n",
    "        super().__init__(initial_lr)\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def step(self, epoch: int, metrics: Optional[Dict[str, float]] = None) -> float:\n",
    "        self.current_lr = self.eta_min + (self.initial_lr - self.eta_min) * \\\n",
    "                         (1 + np.cos(np.pi * epoch / self.T_max)) / 2\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "# Scheduler factory\n",
    "SCHEDULERS = {\n",
    "    'constant': ConstantLR,\n",
    "    'step': StepLR,\n",
    "    'exponential': ExponentialLR,\n",
    "    'plateau': ReduceLROnPlateau,\n",
    "    'cosine': CosineAnnealingLR\n",
    "}\n",
    "\n",
    "\n",
    "def get_scheduler(name: str, initial_lr: float, **kwargs) -> LRScheduler:\n",
    "    \"\"\"\n",
    "    Factory function to create learning rate schedulers\n",
    "    \n",
    "    Args:\n",
    "        name: Name of scheduler\n",
    "        initial_lr: Initial learning rate\n",
    "        **kwargs: Additional scheduler parameters\n",
    "    \n",
    "    Returns:\n",
    "        Learning rate scheduler instance\n",
    "    \"\"\"\n",
    "    if name.lower() not in SCHEDULERS:\n",
    "        raise ValueError(f\"Unknown scheduler: {name}. Available: {list(SCHEDULERS.keys())}\")\n",
    "    \n",
    "    return SCHEDULERS[name.lower()](initial_lr, **kwargs)\n",
    "\n",
    "\n",
    "print(\"✓ Learning rate schedulers implemented successfully!\")\n",
    "print(f\"Available schedulers: {list(SCHEDULERS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Early Stopping and Model Checkpointing\n",
    "\n",
    "Let's implement early stopping to prevent overfitting and model checkpointing to save the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric: str = 'val_loss', mode: str = 'min',\n",
    "                 patience: int = 20, min_delta: float = 1e-4,\n",
    "                 restore_best_weights: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize early stopping\n",
    "        \n",
    "        Args:\n",
    "            metric: Metric to monitor\n",
    "            mode: 'min' or 'max' - whether to minimize or maximize metric\n",
    "            patience: Number of epochs to wait before stopping\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "            restore_best_weights: Whether to restore best weights when stopping\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        \n",
    "        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best_weights = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, epoch: int, metrics: Dict[str, float], model=None) -> bool:\n",
    "        \"\"\"\n",
    "        Check if training should stop\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch\n",
    "            metrics: Current metrics\n",
    "            model: Model to save weights from (if restore_best_weights=True)\n",
    "        \n",
    "        Returns:\n",
    "            True if training should stop\n",
    "        \"\"\"\n",
    "        if self.metric not in metrics:\n",
    "            return False\n",
    "        \n",
    "        current_metric = metrics[self.metric]\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = current_metric < (self.best_metric - self.min_delta)\n",
    "        else:\n",
    "            improved = current_metric > (self.best_metric + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_metric = current_metric\n",
    "            self.wait = 0\n",
    "            \n",
    "            # Save best weights\n",
    "            if self.restore_best_weights and model is not None:\n",
    "                self.best_weights = self._save_weights(model)\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.should_stop = True\n",
    "                \n",
    "                # Restore best weights\n",
    "                if self.restore_best_weights and self.best_weights is not None:\n",
    "                    self._restore_weights(model, self.best_weights)\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "                    print(f\"Restored weights from best epoch (best {self.metric}: {self.best_metric:.6f})\")\n",
    "                \n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _save_weights(self, model) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Save model weights\n",
    "        \n",
    "        Args:\n",
    "            model: Model to save weights from\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of saved weights\n",
    "        \"\"\"\n",
    "        weights = {}\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            weights[f'layer_{i}'] = {\n",
    "                'weights': layer.weights.copy(),\n",
    "                'biases': layer.biases.copy()\n",
    "            }\n",
    "        return weights\n",
    "    \n",
    "    def _restore_weights(self, model, weights: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Restore model weights\n",
    "        \n",
    "        Args:\n",
    "            model: Model to restore weights to\n",
    "            weights: Dictionary of weights to restore\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in weights:\n",
    "                layer.weights = weights[layer_key]['weights'].copy()\n",
    "                layer.biases = weights[layer_key]['biases'].copy()\n",
    "\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Save model checkpoints during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str, metric: str = 'val_loss', \n",
    "                 mode: str = 'min', save_best_only: bool = True,\n",
    "                 save_weights_only: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize model checkpointing\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to save checkpoints\n",
    "            metric: Metric to monitor for best model\n",
    "            mode: 'min' or 'max' for metric optimization\n",
    "            save_best_only: Only save when metric improves\n",
    "            save_weights_only: Only save weights, not full model\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        \n",
    "        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, epoch: int, metrics: Dict[str, float], model) -> bool:\n",
    "        \"\"\"\n",
    "        Check if model should be saved\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch\n",
    "            metrics: Current metrics\n",
    "            model: Model to potentially save\n",
    "        \n",
    "        Returns:\n",
    "            True if model was saved\n",
    "        \"\"\"\n",
    "        if self.save_best_only:\n",
    "            if self.metric not in metrics:\n",
    "                return False\n",
    "            \n",
    "            current_metric = metrics[self.metric]\n",
    "            \n",
    "            if self.mode == 'min':\n",
    "                improved = current_metric < self.best_metric\n",
    "            else:\n",
    "                improved = current_metric > self.best_metric\n",
    "            \n",
    "            if not improved:\n",
    "                return False\n",
    "            \n",
    "            self.best_metric = current_metric\n",
    "            self.best_epoch = epoch\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'metrics': metrics,\n",
    "            'model_state': self._get_model_state(model)\n",
    "        }\n",
    "        \n",
    "        # In a real implementation, you would save to disk\n",
    "        # For this lab, we'll just store in memory\n",
    "        self.last_checkpoint = checkpoint\n",
    "        \n",
    "        print(f\"  Checkpoint saved at epoch {epoch + 1} ({self.metric}: {metrics.get(self.metric, 'N/A')})\")\n",
    "        return True\n",
    "    \n",
    "    def _get_model_state(self, model) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get model state for saving\n",
    "        \n",
    "        Args:\n",
    "            model: Model to get state from\n",
    "        \n",
    "        Returns:\n",
    "            Model state dictionary\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'architecture': model.architecture,\n",
    "            'learning_rate': model.learning_rate,\n",
    "            'loss_function': model.loss_function.name\n",
    "        }\n",
    "        \n",
    "        if self.save_weights_only:\n",
    "            state['weights'] = {}\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                state['weights'][f'layer_{i}'] = {\n",
    "                    'weights': layer.weights.copy(),\n",
    "                    'biases': layer.biases.copy()\n",
    "                }\n",
    "        \n",
    "        return state\n",
    "\n",
    "\n",
    "print(\"✓ Early stopping and model checkpointing implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Advanced Training Loop with Monitoring\n",
    "\n",
    "Let's create a sophisticated training loop that incorporates all our advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Monitor and log training progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_to_track: List[str]):\n",
    "        self.metrics_to_track = metrics_to_track\n",
    "        self.history = defaultdict(list)\n",
    "        self.epoch_times = []\n",
    "        self.start_time = None\n",
    "        \n",
    "        # For moving averages\n",
    "        self.moving_averages = {}\n",
    "        self.window_size = 10\n",
    "    \n",
    "    def start_training(self):\n",
    "        \"\"\"Mark start of training\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(\"Training started...\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def start_epoch(self):\n",
    "        \"\"\"Mark start of epoch\"\"\"\n",
    "        self.epoch_start_time = time.time()\n",
    "    \n",
    "    def end_epoch(self, epoch: int, metrics: Dict[str, float], \n",
    "                  verbose: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"Mark end of epoch and log metrics\"\"\"\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Store metrics\n",
    "        for metric, value in metrics.items():\n",
    "            self.history[metric].append(value)\n",
    "            \n",
    "            # Update moving average\n",
    "            if metric not in self.moving_averages:\n",
    "                self.moving_averages[metric] = deque(maxlen=self.window_size)\n",
    "            self.moving_averages[metric].append(value)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_time = time.time() - self.start_time\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        \n",
    "        enhanced_metrics = metrics.copy()\n",
    "        enhanced_metrics.update({\n",
    "            'epoch_time': epoch_time,\n",
    "            'total_time': total_time,\n",
    "            'avg_epoch_time': avg_epoch_time\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose > 0:\n",
    "            self._print_epoch_summary(epoch, enhanced_metrics)\n",
    "        \n",
    "        return enhanced_metrics\n",
    "    \n",
    "    def _print_epoch_summary(self, epoch: int, metrics: Dict[str, float]):\n",
    "        \"\"\"Print epoch summary\"\"\"\n",
    "        # Main metrics line\n",
    "        main_metrics = []\n",
    "        for metric in ['loss', 'accuracy', 'val_loss', 'val_accuracy']:\n",
    "            if metric in metrics:\n",
    "                main_metrics.append(f\"{metric}: {metrics[metric]:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1:3d} - {' - '.join(main_metrics)} - \"\n",
    "              f\"time: {metrics['epoch_time']:.2f}s\")\n",
    "        \n",
    "        # Moving averages (every 10 epochs)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"  Moving averages (last 10 epochs):\")\n",
    "            for metric in ['loss', 'val_loss']:\n",
    "                if metric in self.moving_averages:\n",
    "                    avg_val = np.mean(list(self.moving_averages[metric]))\n",
    "                    print(f\"    {metric}: {avg_val:.4f}\")\n",
    "    \n",
    "    def get_history(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Get training history\"\"\"\n",
    "        return dict(self.history)\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get training summary statistics\"\"\"\n",
    "        total_time = time.time() - self.start_time if self.start_time else 0\n",
    "        \n",
    "        summary = {\n",
    "            'total_epochs': len(self.epoch_times),\n",
    "            'total_time': total_time,\n",
    "            'avg_epoch_time': np.mean(self.epoch_times) if self.epoch_times else 0,\n",
    "            'best_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Find best values for each metric\n",
    "        for metric, values in self.history.items():\n",
    "            if values:\n",
    "                if 'loss' in metric.lower():\n",
    "                    summary['best_metrics'][f'best_{metric}'] = min(values)\n",
    "                else:\n",
    "                    summary['best_metrics'][f'best_{metric}'] = max(values)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    \"\"\"\n",
    "    Advanced training loop with all modern features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, \n",
    "                 scheduler: Optional[LRScheduler] = None,\n",
    "                 early_stopping: Optional[EarlyStopping] = None,\n",
    "                 checkpoint: Optional[ModelCheckpoint] = None):\n",
    "        \"\"\"\n",
    "        Initialize advanced trainer\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model\n",
    "            scheduler: Learning rate scheduler\n",
    "            early_stopping: Early stopping callback\n",
    "            checkpoint: Model checkpointing callback\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.scheduler = scheduler\n",
    "        self.early_stopping = early_stopping\n",
    "        self.checkpoint = checkpoint\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.is_training = False\n",
    "        self.training_stopped_early = False\n",
    "    \n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "            validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n",
    "            epochs: int = 100,\n",
    "            batch_size: int = 32,\n",
    "            verbose: int = 1,\n",
    "            metrics_to_track: Optional[List[str]] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Advanced training method with all features\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            validation_data: Validation data tuple\n",
    "            epochs: Number of epochs\n",
    "            batch_size: Batch size\n",
    "            verbose: Verbosity level\n",
    "            metrics_to_track: Additional metrics to track\n",
    "        \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        # Setup\n",
    "        if y_train.ndim == 1:\n",
    "            y_train = y_train.reshape(-1, 1)\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            X_val, y_val = validation_data\n",
    "            if y_val.ndim == 1:\n",
    "                y_val = y_val.reshape(-1, 1)\n",
    "        else:\n",
    "            X_val, y_val = None, None\n",
    "        \n",
    "        # Initialize monitor\n",
    "        default_metrics = ['loss', 'accuracy']\n",
    "        if validation_data is not None:\n",
    "            default_metrics.extend(['val_loss', 'val_accuracy'])\n",
    "        \n",
    "        if metrics_to_track:\n",
    "            default_metrics.extend(metrics_to_track)\n",
    "        \n",
    "        monitor = TrainingMonitor(default_metrics)\n",
    "        monitor.start_training()\n",
    "        \n",
    "        self.is_training = True\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.current_epoch = epoch\n",
    "            monitor.start_epoch()\n",
    "            \n",
    "            # Training phase\n",
    "            epoch_metrics = self._train_epoch(X_train, y_train, batch_size)\n",
    "            \n",
    "            # Validation phase\n",
    "            if validation_data is not None:\n",
    "                val_metrics = self._validate_epoch(X_val, y_val)\n",
    "                epoch_metrics.update(val_metrics)\n",
    "            \n",
    "            # Update learning rate\n",
    "            if self.scheduler is not None:\n",
    "                old_lr = self.model.learning_rate\n",
    "                new_lr = self.scheduler.step(epoch, epoch_metrics)\n",
    "                self.model.learning_rate = new_lr\n",
    "                \n",
    "                if abs(new_lr - old_lr) > 1e-8 and verbose > 1:\n",
    "                    print(f\"  Learning rate updated: {old_lr:.2e} → {new_lr:.2e}\")\n",
    "            \n",
    "            # End epoch monitoring\n",
    "            enhanced_metrics = monitor.end_epoch(epoch, epoch_metrics, verbose)\n",
    "            \n",
    "            # Model checkpointing\n",
    "            if self.checkpoint is not None:\n",
    "                self.checkpoint(epoch, epoch_metrics, self.model)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if self.early_stopping is not None:\n",
    "                if self.early_stopping(epoch, epoch_metrics, self.model):\n",
    "                    self.training_stopped_early = True\n",
    "                    break\n",
    "        \n",
    "        self.is_training = False\n",
    "        \n",
    "        # Final summary\n",
    "        if verbose > 0:\n",
    "            self._print_training_summary(monitor)\n",
    "        \n",
    "        return monitor.get_history()\n",
    "    \n",
    "    def _train_epoch(self, X: np.ndarray, y: np.ndarray, \n",
    "                    batch_size: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Train for one epoch\n",
    "        \n",
    "        Args:\n",
    "            X: Training features\n",
    "            y: Training targets\n",
    "            batch_size: Batch size\n",
    "        \n",
    "        Returns:\n",
    "            Training metrics\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, n_samples)\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Train step\n",
    "            loss, accuracy = self.model.train_step(X_batch, y_batch)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            batch_size_actual = len(X_batch)\n",
    "            total_loss += loss * batch_size_actual / n_samples\n",
    "            total_accuracy += accuracy * batch_size_actual / n_samples\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'accuracy': total_accuracy\n",
    "        }\n",
    "    \n",
    "    def _validate_epoch(self, X_val: np.ndarray, \n",
    "                       y_val: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Validate for one epoch\n",
    "        \n",
    "        Args:\n",
    "            X_val: Validation features\n",
    "            y_val: Validation targets\n",
    "        \n",
    "        Returns:\n",
    "            Validation metrics\n",
    "        \"\"\"\n",
    "        val_metrics = self.model.evaluate(X_val, y_val)\n",
    "        \n",
    "        return {\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        }\n",
    "    \n",
    "    def _print_training_summary(self, monitor: TrainingMonitor):\n",
    "        \"\"\"\n",
    "        Print training summary\n",
    "        \n",
    "        Args:\n",
    "            monitor: Training monitor instance\n",
    "        \"\"\"\n",
    "        print(\"\\nTraining Summary:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        summary = monitor.get_summary()\n",
    "        \n",
    "        print(f\"Total epochs: {summary['total_epochs']}\")\n",
    "        print(f\"Total time: {summary['total_time']:.2f} seconds\")\n",
    "        print(f\"Average time per epoch: {summary['avg_epoch_time']:.2f} seconds\")\n",
    "        \n",
    "        if self.training_stopped_early:\n",
    "            print(f\"Training stopped early at epoch {self.current_epoch + 1}\")\n",
    "        \n",
    "        if self.scheduler is not None:\n",
    "            print(f\"Final learning rate: {self.model.learning_rate:.2e}\")\n",
    "        \n",
    "        print(\"\\nBest metrics:\")\n",
    "        for metric, value in summary['best_metrics'].items():\n",
    "            print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "\n",
    "print(\"✓ Advanced training loop implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Import Neural Network Components from Previous Lab\n",
    "\n",
    "Let's recreate the essential components from the previous lab to test our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified versions of components from Lab 2.4\n",
    "class ActivationFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"ReLU\"\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_clipped = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "    \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        s = self.forward(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Sigmoid\"\n",
    "\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(LossFunction):\n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        m = y_true.shape[0]\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -1/m * np.sum(y_true * np.log(y_pred_clipped) + \n",
    "                           (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        m = y_true.shape[0]\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -1/m * (y_true / y_pred_clipped - (1 - y_true) / (1 - y_pred_clipped))\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Binary Cross Entropy\"\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size: int, output_size: int, activation: ActivationFunction):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        \n",
    "        # He initialization for ReLU, Xavier for others\n",
    "        if isinstance(activation, ReLU):\n",
    "            self.weights = np.random.normal(0, np.sqrt(2 / input_size), (input_size, output_size))\n",
    "        else:\n",
    "            limit = np.sqrt(6 / (input_size + output_size))\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "        \n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        \n",
    "        self.last_input = None\n",
    "        self.last_z = None\n",
    "        self.last_activation = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.last_input = X\n",
    "        self.last_z = np.dot(X, self.weights) + self.biases\n",
    "        self.last_activation = self.activation.forward(self.last_z)\n",
    "        return self.last_activation\n",
    "    \n",
    "    def backward(self, dA: np.ndarray) -> np.ndarray:\n",
    "        m = self.last_input.shape[0]\n",
    "        dZ = dA * self.activation.backward(self.last_z)\n",
    "        self.dW = (1/m) * np.dot(self.last_input.T, dZ)\n",
    "        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        dA_prev = np.dot(dZ, self.weights.T)\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_parameters(self, learning_rate: float):\n",
    "        self.weights -= learning_rate * self.dW\n",
    "        self.biases -= learning_rate * self.db\n",
    "\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, layers: List[Layer], loss_function: LossFunction, learning_rate: float = 0.01):\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = [{'input_size': layer.input_size, 'output_size': layer.output_size} for layer in layers]\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        activation = X\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "    \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        dA = self.loss_function.backward(y_pred, y_true)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(self.learning_rate)\n",
    "    \n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.loss_function.forward(y_pred, y)\n",
    "        self.backward(y_pred, y)\n",
    "        self.update_parameters()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        if self.loss_function.name == 'Binary Cross Entropy':\n",
    "            predictions = (y_pred > 0.5).astype(int)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "        else:\n",
    "            ss_res = np.sum((y - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "            accuracy = max(0, 1 - (ss_res / ss_tot))\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        loss = self.loss_function.forward(y_pred, y)\n",
    "        \n",
    "        if self.loss_function.name == 'Binary Cross Entropy':\n",
    "            predictions = (y_pred > 0.5).astype(int)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "        else:\n",
    "            ss_res = np.sum((y - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "            accuracy = max(0, 1 - (ss_res / ss_tot))\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "print(\"✓ Neural network components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Advanced Training Loop\n",
    "\n",
    "Let's test our advanced training loop with all features on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced training loop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Testing Advanced Training Loop\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_redundant=0,\n",
    "    n_informative=8,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Training: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"  Validation: {X_val_scaled.shape[0]} samples\")\n",
    "print(f\"  Test: {X_test_scaled.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "# Create neural network\n",
    "layers = [\n",
    "    Layer(10, 16, ReLU()),\n",
    "    Layer(16, 12, ReLU()),\n",
    "    Layer(12, 8, ReLU()),\n",
    "    Layer(8, 1, Sigmoid())\n",
    "]\n",
    "\n",
    "model = SimpleNeuralNetwork(layers, BinaryCrossEntropy(), learning_rate=0.01)\n",
    "print(f\"\\nModel created with {len(layers)} layers\")\n",
    "print(f\"Architecture: {[layer.input_size for layer in layers] + [layers[-1].output_size]}\")\n",
    "\n",
    "print(\"\\n✓ Test setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Training with Early Stopping and Plateau Scheduler\n",
    "print(\"\\nTest 1: Early Stopping + Plateau LR Scheduler\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create components\n",
    "scheduler = get_scheduler('plateau', initial_lr=0.01, \n",
    "                         metric='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    patience=15,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.pkl',\n",
    "    metric='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    scheduler=scheduler,\n",
    "    early_stopping=early_stopping,\n",
    "    checkpoint=checkpoint\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history1 = trainer.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "test_results = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"\\nFinal test results:\")\n",
    "print(f\"Test Loss: {test_results['loss']:.6f}\")\n",
    "print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Test 1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Different Scheduler Comparison\n",
    "print(\"\\nTest 2: Comparing Different Learning Rate Schedulers\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create fresh models for comparison\n",
    "def create_fresh_model():\n",
    "    layers = [\n",
    "        Layer(10, 12, ReLU()),\n",
    "        Layer(12, 8, ReLU()),\n",
    "        Layer(8, 1, Sigmoid())\n",
    "    ]\n",
    "    return SimpleNeuralNetwork(layers, BinaryCrossEntropy(), learning_rate=0.01)\n",
    "\n",
    "# Test different schedulers\n",
    "scheduler_configs = {\n",
    "    'constant': {'name': 'constant'},\n",
    "    'step': {'name': 'step', 'step_size': 20, 'gamma': 0.5},\n",
    "    'exponential': {'name': 'exponential', 'gamma': 0.95},\n",
    "    'cosine': {'name': 'cosine', 'T_max': 50}\n",
    "}\n",
    "\n",
    "scheduler_results = {}\n",
    "epochs_short = 50  # Shorter training for comparison\n",
    "\n",
    "for scheduler_name, config in scheduler_configs.items():\n",
    "    print(f\"\\nTraining with {scheduler_name} scheduler...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create model and scheduler\n",
    "    model_test = create_fresh_model()\n",
    "    scheduler_test = get_scheduler(config['name'], initial_lr=0.01, \n",
    "                                  **{k: v for k, v in config.items() if k != 'name'})\n",
    "    \n",
    "    # Create trainer (no early stopping for fair comparison)\n",
    "    trainer_test = AdvancedTrainer(model=model_test, scheduler=scheduler_test)\n",
    "    \n",
    "    # Train\n",
    "    history_test = trainer_test.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=epochs_short,\n",
    "        batch_size=32,\n",
    "        verbose=0  # Quiet training\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_metrics = model_test.evaluate(X_test_scaled, y_test)\n",
    "    \n",
    "    scheduler_results[scheduler_name] = {\n",
    "        'history': history_test,\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'final_lr': model_test.learning_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"Final test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Final learning rate: {model_test.learning_rate:.2e}\")\n",
    "\n",
    "print(\"\\n✓ Scheduler comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Results and Scheduler Comparison\n",
    "\n",
    "Let's create comprehensive visualizations of our training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function for advanced training results\n",
    "def plot_advanced_training_results(history: Dict[str, List[float]], title: str = \"Training Results\"):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training results\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    if 'val_loss' in history:\n",
    "        axes[0, 0].plot(history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    if 'val_accuracy' in history:\n",
    "        axes[0, 1].plot(history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training time per epoch\n",
    "    if 'epoch_time' in history:\n",
    "        axes[1, 0].plot(history['epoch_time'], 'g-', linewidth=2)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].set_title('Training Time per Epoch')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No timing data available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Training Time per Epoch')\n",
    "    \n",
    "    # Loss difference (overfitting indicator)\n",
    "    if 'val_loss' in history:\n",
    "        loss_diff = np.array(history['val_loss']) - np.array(history['loss'])\n",
    "        axes[1, 1].plot(loss_diff, 'purple', linewidth=2)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Validation Loss - Training Loss')\n",
    "        axes[1, 1].set_title('Overfitting Indicator')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add interpretation text\n",
    "        final_diff = loss_diff[-1]\n",
    "        if final_diff > 0.1:\n",
    "            status = \"High overfitting\"\n",
    "        elif final_diff > 0.05:\n",
    "            status = \"Moderate overfitting\"\n",
    "        elif final_diff > -0.05:\n",
    "            status = \"Well balanced\"\n",
    "        else:\n",
    "            status = \"Possible underfitting\"\n",
    "        \n",
    "        axes[1, 1].text(0.02, 0.98, f'Status: {status}', \n",
    "                        transform=axes[1, 1].transAxes, \n",
    "                        verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No validation data available', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Overfitting Indicator')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results from Test 1\n",
    "print(\"Advanced Training Results Visualization\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "plot_advanced_training_results(history1, \"Early Stopping + Plateau Scheduler Results\")\n",
    "\n",
    "print(\"\\n✓ Advanced training visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler comparison visualization\n",
    "def plot_scheduler_comparison(scheduler_results: Dict[str, Dict]):\n",
    "    \"\"\"\n",
    "    Plot comparison of different learning rate schedulers\n",
    "    \n",
    "    Args:\n",
    "        scheduler_results: Results from different schedulers\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Training loss comparison\n",
    "    axes[0, 0].set_title('Training Loss Comparison')\n",
    "    for scheduler_name, results in scheduler_results.items():\n",
    "        axes[0, 0].plot(results['history']['loss'], linewidth=2, label=scheduler_name.title())\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Validation loss comparison\n",
    "    axes[0, 1].set_title('Validation Loss Comparison')\n",
    "    for scheduler_name, results in scheduler_results.items():\n",
    "        axes[0, 1].plot(results['history']['val_loss'], linewidth=2, label=scheduler_name.title())\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # Validation accuracy comparison\n",
    "    axes[0, 2].set_title('Validation Accuracy Comparison')\n",
    "    for scheduler_name, results in scheduler_results.items():\n",
    "        axes[0, 2].plot(results['history']['val_accuracy'], linewidth=2, label=scheduler_name.title())\n",
    "    \n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final test accuracy comparison\n",
    "    scheduler_names = list(scheduler_results.keys())\n",
    "    test_accuracies = [scheduler_results[name]['test_accuracy'] for name in scheduler_names]\n",
    "    test_losses = [scheduler_results[name]['test_loss'] for name in scheduler_names]\n",
    "    final_lrs = [scheduler_results[name]['final_lr'] for name in scheduler_names]\n",
    "    \n",
    "    axes[1, 0].bar(scheduler_names, test_accuracies, alpha=0.7, color='skyblue')\n",
    "    axes[1, 0].set_ylabel('Test Accuracy')\n",
    "    axes[1, 0].set_title('Final Test Accuracy')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, acc in enumerate(test_accuracies):\n",
    "        axes[1, 0].text(i, acc + 0.005, f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Final learning rates\n",
    "    axes[1, 1].bar(scheduler_names, final_lrs, alpha=0.7, color='lightcoral')\n",
    "    axes[1, 1].set_ylabel('Final Learning Rate')\n",
    "    axes[1, 1].set_title('Final Learning Rates')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, lr in enumerate(final_lrs):\n",
    "        axes[1, 1].text(i, lr * 1.2, f'{lr:.2e}', ha='center', va='bottom', rotation=45)\n",
    "    \n",
    "    # Learning rate evolution (show one example)\n",
    "    # Create a mock learning rate schedule for visualization\n",
    "    epochs = range(50)\n",
    "    \n",
    "    # Simulate learning rate schedules\n",
    "    constant_lrs = [0.01] * 50\n",
    "    step_lrs = [0.01 if e < 20 else 0.005 if e < 40 else 0.0025 for e in epochs]\n",
    "    exp_lrs = [0.01 * (0.95 ** e) for e in epochs]\n",
    "    cosine_lrs = [0 + (0.01 - 0) * (1 + np.cos(np.pi * e / 50)) / 2 for e in epochs]\n",
    "    \n",
    "    axes[1, 2].plot(epochs, constant_lrs, linewidth=2, label='Constant')\n",
    "    axes[1, 2].plot(epochs, step_lrs, linewidth=2, label='Step')\n",
    "    axes[1, 2].plot(epochs, exp_lrs, linewidth=2, label='Exponential')\n",
    "    axes[1, 2].plot(epochs, cosine_lrs, linewidth=2, label='Cosine')\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Learning Rate')\n",
    "    axes[1, 2].set_title('Learning Rate Schedules')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    \n",
    "    plt.suptitle('Learning Rate Scheduler Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nScheduler Comparison Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Scheduler':<12} {'Test Acc':<10} {'Test Loss':<12} {'Final LR':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name in scheduler_names:\n",
    "        result = scheduler_results[name]\n",
    "        print(f\"{name.title():<12} {result['test_accuracy']:<10.4f} \"\n",
    "              f\"{result['test_loss']:<12.6f} {result['final_lr']:<12.2e}\")\n",
    "    \n",
    "    # Find best performer\n",
    "    best_scheduler = max(scheduler_names, key=lambda x: scheduler_results[x]['test_accuracy'])\n",
    "    print(f\"\\nBest performer: {best_scheduler.title()} \"\n",
    "          f\"(Test Accuracy: {scheduler_results[best_scheduler]['test_accuracy']:.4f})\")\n",
    "\n",
    "# Plot scheduler comparison\n",
    "print(\"\\nLearning Rate Scheduler Comparison\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "plot_scheduler_comparison(scheduler_results)\n",
    "\n",
    "print(\"\\n✓ Scheduler comparison visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] ✅ **Environment Setup**: Imported libraries and established training loop principles\n",
    "- [ ] ✅ **Learning Rate Schedulers**: Implemented various scheduling strategies (constant, step, exponential, plateau, cosine)\n",
    "- [ ] ✅ **Early Stopping**: Built early stopping system to prevent overfitting\n",
    "- [ ] ✅ **Model Checkpointing**: Created checkpoint system to save best models\n",
    "- [ ] ✅ **Training Monitor**: Implemented comprehensive progress monitoring and logging\n",
    "- [ ] ✅ **Advanced Trainer**: Built sophisticated training loop with all modern features\n",
    "- [ ] ✅ **Neural Network Components**: Imported and adapted components from previous lab\n",
    "- [ ] ✅ **Training Tests**: Successfully tested training loop with real datasets\n",
    "- [ ] ✅ **Scheduler Comparison**: Compared different learning rate scheduling strategies\n",
    "- [ ] ✅ **Visualization**: Created comprehensive training results and comparison visualizations\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Modern Training Loop Components:**\n",
    "1. **Learning Rate Scheduling**: Adaptive adjustment of learning rates during training\n",
    "2. **Early Stopping**: Prevents overfitting by monitoring validation metrics\n",
    "3. **Model Checkpointing**: Saves best models based on specified criteria\n",
    "4. **Progress Monitoring**: Real-time tracking of training metrics and performance\n",
    "5. **Batch Processing**: Efficient handling of different batch sizes and data loading\n",
    "\n",
    "**Learning Rate Schedulers:**\n",
    "- **Constant**: Maintains fixed learning rate throughout training\n",
    "- **Step**: Reduces learning rate at fixed intervals\n",
    "- **Exponential**: Smooth exponential decay of learning rate\n",
    "- **Plateau**: Reduces learning rate when metric stops improving\n",
    "- **Cosine**: Cosine annealing for smooth learning rate variation\n",
    "\n",
    "**Training Best Practices:**\n",
    "- Monitor both training and validation metrics\n",
    "- Use early stopping to prevent overfitting\n",
    "- Save model checkpoints regularly\n",
    "- Implement learning rate scheduling for better convergence\n",
    "- Track training time and computational efficiency\n",
    "- Visualize training progress for debugging\n",
    "\n",
    "**Production Considerations:**\n",
    "- Robust error handling and recovery\n",
    "- Comprehensive logging and monitoring\n",
    "- Efficient memory and computational resource usage\n",
    "- Reproducible training with proper seed management\n",
    "- Scalable batch processing strategies\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Training Instability**\n",
    "   - *Problem*: Loss oscillates or explodes\n",
    "   - *Solutions*: Reduce learning rate, implement gradient clipping, check data preprocessing\n",
    "\n",
    "2. **Slow Convergence**\n",
    "   - *Problem*: Training takes too long to converge\n",
    "   - *Solutions*: Increase learning rate, use learning rate scheduling, improve initialization\n",
    "\n",
    "3. **Overfitting**\n",
    "   - *Problem*: Validation loss increases while training loss decreases\n",
    "   - *Solutions*: Use early stopping, add regularization, increase validation monitoring\n",
    "\n",
    "4. **Memory Issues**\n",
    "   - *Problem*: Out of memory errors during training\n",
    "   - *Solutions*: Reduce batch size, implement gradient accumulation, use data generators\n",
    "\n",
    "5. **Learning Rate Scheduling Issues**\n",
    "   - *Problem*: Learning rate reduces too quickly or slowly\n",
    "   - *Solutions*: Adjust scheduler parameters, monitor learning rate changes, try different schedulers\n",
    "\n",
    "6. **Early Stopping Too Aggressive**\n",
    "   - *Problem*: Training stops before optimal convergence\n",
    "   - *Solutions*: Increase patience, reduce min_delta, check validation data quality\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Implementation:**\n",
    "1. ✅ Training loss consistently decreases over epochs\n",
    "2. ✅ Learning rate scheduling works as expected\n",
    "3. ✅ Early stopping activates when validation metrics plateau\n",
    "4. ✅ Model checkpoints save at appropriate times\n",
    "5. ✅ Training monitor logs all metrics correctly\n",
    "6. ✅ Batch processing handles different sizes properly\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save your notebook with all training results and visualizations\n",
    "2. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del history1, scheduler_results\n",
    "   # del X_train_scaled, X_val_scaled, X_test_scaled\n",
    "   # del model, trainer\n",
    "   ```\n",
    "3. Close plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Your advanced training loop implementation provides a solid foundation for:\n",
    "- **Production ML Systems**: Robust, monitored training pipelines\n",
    "- **Hyperparameter Optimization**: Integration with automated tuning systems\n",
    "- **Distributed Training**: Extension to multi-GPU and multi-node training\n",
    "- **Advanced Optimizers**: Integration with Adam, RMSprop, and other optimizers\n",
    "- **Deep Learning Frameworks**: Understanding of how PyTorch/TensorFlow training works\n",
    "\n",
    "The training loop patterns you've learned are used in all modern deep learning frameworks and are essential for successful machine learning projects.\n",
    "\n",
    "**Congratulations! You've completed Lab 2.5 - Training Loop Implementation!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}