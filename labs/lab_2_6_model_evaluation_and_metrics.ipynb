{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.6: Model Evaluation and Metrics\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement comprehensive model evaluation metrics for different problem types\n",
    "- Create confusion matrices and classification reports\n",
    "- Understand and calculate ROC curves, AUC, and precision-recall curves\n",
    "- Implement cross-validation for robust model assessment\n",
    "- Build model comparison frameworks\n",
    "- Create visualization tools for model performance analysis\n",
    "- Handle class imbalance in evaluation metrics\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- NumPy, Matplotlib, Seaborn\n",
    "- Completed previous labs (especially Lab 2.4 and 2.5)\n",
    "- Understanding of classification and regression metrics\n",
    "\n",
    "## Overview\n",
    "Proper model evaluation is crucial for understanding model performance, comparing different approaches, and making informed decisions about model deployment. This lab covers comprehensive evaluation techniques including advanced metrics, visualization methods, and statistical analysis tools that are essential for professional machine learning practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Core Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib and seaborn\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "\n",
    "print(\"\\nModel Evaluation Components:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. Classification Metrics: Accuracy, Precision, Recall, F1-Score\")\n",
    "print(\"2. Regression Metrics: MAE, MSE, RMSE, RÂ²\")\n",
    "print(\"3. Advanced Metrics: ROC-AUC, PR-AUC, Cohen's Kappa\")\n",
    "print(\"4. Visualization: Confusion Matrix, ROC Curves, Learning Curves\")\n",
    "print(\"5. Cross-Validation: K-Fold, Stratified, Time Series\")\n",
    "print(\"6. Model Comparison: Statistical Tests, Confidence Intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Classification Metrics Implementation\n",
    "\n",
    "Let's implement comprehensive classification metrics from scratch to understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive classification metrics implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                 y_pred_proba: Optional[np.ndarray] = None,\n",
    "                 class_names: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Initialize classification metrics\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            y_pred_proba: Predicted probabilities (for ROC/AUC metrics)\n",
    "            class_names: Names of classes\n",
    "        \"\"\"\n",
    "        self.y_true = y_true.flatten() if y_true.ndim > 1 else y_true\n",
    "        self.y_pred = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
    "        self.y_pred_proba = y_pred_proba.flatten() if y_pred_proba is not None and y_pred_proba.ndim > 1 else y_pred_proba\n",
    "        \n",
    "        self.classes = np.unique(np.concatenate([self.y_true, self.y_pred]))\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.class_names = class_names if class_names else [f'Class {i}' for i in self.classes]\n",
    "        \n",
    "        # Validate inputs\n",
    "        assert len(self.y_true) == len(self.y_pred), \"y_true and y_pred must have same length\"\n",
    "        if self.y_pred_proba is not None:\n",
    "            assert len(self.y_true) == len(self.y_pred_proba), \"y_true and y_pred_proba must have same length\"\n",
    "    \n",
    "    def confusion_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate confusion matrix\n",
    "        \n",
    "        Returns:\n",
    "            Confusion matrix\n",
    "        \"\"\"\n",
    "        cm = np.zeros((self.n_classes, self.n_classes), dtype=int)\n",
    "        \n",
    "        for true_class in self.classes:\n",
    "            for pred_class in self.classes:\n",
    "                true_idx = np.where(self.classes == true_class)[0][0]\n",
    "                pred_idx = np.where(self.classes == pred_class)[0][0]\n",
    "                \n",
    "                count = np.sum((self.y_true == true_class) & (self.y_pred == pred_class))\n",
    "                cm[true_idx, pred_idx] = count\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    def accuracy(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        \n",
    "        Returns:\n",
    "            Accuracy score\n",
    "        \"\"\"\n",
    "        return np.mean(self.y_true == self.y_pred)\n",
    "    \n",
    "    def precision_recall_f1(self, average: str = 'weighted') -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score\n",
    "        \n",
    "        Args:\n",
    "            average: Averaging strategy ('micro', 'macro', 'weighted', 'binary')\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (precision, recall, f1_score)\n",
    "        \"\"\"\n",
    "        cm = self.confusion_matrix()\n",
    "        \n",
    "        if average == 'binary' and self.n_classes == 2:\n",
    "            # Binary classification - positive class is class 1\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            return precision, recall, f1\n",
    "        \n",
    "        # Multi-class metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        supports = []\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            tp = cm[i, i]\n",
    "            fp = np.sum(cm[:, i]) - tp\n",
    "            fn = np.sum(cm[i, :]) - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n",
    "            supports.append(np.sum(self.y_true == self.classes[i]))\n",
    "        \n",
    "        precisions = np.array(precisions)\n",
    "        recalls = np.array(recalls)\n",
    "        f1s = np.array(f1s)\n",
    "        supports = np.array(supports)\n",
    "        \n",
    "        if average == 'macro':\n",
    "            return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n",
    "        elif average == 'weighted':\n",
    "            total_support = np.sum(supports)\n",
    "            weighted_precision = np.sum(precisions * supports) / total_support\n",
    "            weighted_recall = np.sum(recalls * supports) / total_support\n",
    "            weighted_f1 = np.sum(f1s * supports) / total_support\n",
    "            return weighted_precision, weighted_recall, weighted_f1\n",
    "        elif average == 'micro':\n",
    "            # For multi-class, micro-average precision/recall equals accuracy\n",
    "            acc = self.accuracy()\n",
    "            return acc, acc, acc\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown average type: {average}\")\n",
    "    \n",
    "    def roc_curve(self, pos_class: int = 1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate ROC curve for binary classification\n",
    "        \n",
    "        Args:\n",
    "            pos_class: Positive class label\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (fpr, tpr, thresholds)\n",
    "        \"\"\"\n",
    "        if self.y_pred_proba is None:\n",
    "            raise ValueError(\"y_pred_proba is required for ROC curve calculation\")\n",
    "        \n",
    "        if self.n_classes != 2:\n",
    "            raise ValueError(\"ROC curve is only supported for binary classification\")\n",
    "        \n",
    "        # Convert to binary problem\n",
    "        y_binary = (self.y_true == pos_class).astype(int)\n",
    "        \n",
    "        # Get unique thresholds\n",
    "        thresholds = np.unique(self.y_pred_proba)\n",
    "        thresholds = np.concatenate([thresholds, [thresholds[-1] + 1]])\n",
    "        thresholds = np.sort(thresholds)[::-1]\n",
    "        \n",
    "        tprs = []\n",
    "        fprs = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred_thresh = (self.y_pred_proba >= threshold).astype(int)\n",
    "            \n",
    "            tp = np.sum((y_binary == 1) & (y_pred_thresh == 1))\n",
    "            fp = np.sum((y_binary == 0) & (y_pred_thresh == 1))\n",
    "            tn = np.sum((y_binary == 0) & (y_pred_thresh == 0))\n",
    "            fn = np.sum((y_binary == 1) & (y_pred_thresh == 0))\n",
    "            \n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            \n",
    "            tprs.append(tpr)\n",
    "            fprs.append(fpr)\n",
    "        \n",
    "        return np.array(fprs), np.array(tprs), thresholds\n",
    "    \n",
    "    def auc_score(self, pos_class: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Area Under the ROC Curve (AUC)\n",
    "        \n",
    "        Args:\n",
    "            pos_class: Positive class label\n",
    "        \n",
    "        Returns:\n",
    "            AUC score\n",
    "        \"\"\"\n",
    "        fpr, tpr, _ = self.roc_curve(pos_class)\n",
    "        \n",
    "        # Calculate AUC using trapezoidal rule\n",
    "        auc = 0\n",
    "        for i in range(1, len(fpr)):\n",
    "            auc += (fpr[i] - fpr[i-1]) * (tpr[i] + tpr[i-1]) / 2\n",
    "        \n",
    "        return auc\n",
    "    \n",
    "    def precision_recall_curve(self, pos_class: int = 1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate Precision-Recall curve\n",
    "        \n",
    "        Args:\n",
    "            pos_class: Positive class label\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (precision, recall, thresholds)\n",
    "        \"\"\"\n",
    "        if self.y_pred_proba is None:\n",
    "            raise ValueError(\"y_pred_proba is required for PR curve calculation\")\n",
    "        \n",
    "        # Convert to binary problem\n",
    "        y_binary = (self.y_true == pos_class).astype(int)\n",
    "        \n",
    "        # Get unique thresholds\n",
    "        thresholds = np.unique(self.y_pred_proba)\n",
    "        thresholds = np.sort(thresholds)[::-1]\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred_thresh = (self.y_pred_proba >= threshold).astype(int)\n",
    "            \n",
    "            tp = np.sum((y_binary == 1) & (y_pred_thresh == 1))\n",
    "            fp = np.sum((y_binary == 0) & (y_pred_thresh == 1))\n",
    "            fn = np.sum((y_binary == 1) & (y_pred_thresh == 0))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        return np.array(precisions), np.array(recalls), thresholds\n",
    "    \n",
    "    def classification_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate comprehensive classification report\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing all classification metrics\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'accuracy': self.accuracy(),\n",
    "            'confusion_matrix': self.confusion_matrix(),\n",
    "            'class_names': self.class_names,\n",
    "            'n_samples': len(self.y_true),\n",
    "            'n_classes': self.n_classes\n",
    "        }\n",
    "        \n",
    "        # Per-class metrics\n",
    "        cm = self.confusion_matrix()\n",
    "        class_metrics = {}\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            tp = cm[i, i]\n",
    "            fp = np.sum(cm[:, i]) - tp\n",
    "            fn = np.sum(cm[i, :]) - tp\n",
    "            tn = np.sum(cm) - tp - fp - fn\n",
    "            support = np.sum(self.y_true == self.classes[i])\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            class_metrics[class_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'specificity': specificity,\n",
    "                'support': support,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn\n",
    "            }\n",
    "        \n",
    "        report['per_class'] = class_metrics\n",
    "        \n",
    "        # Averaged metrics\n",
    "        macro_prec, macro_rec, macro_f1 = self.precision_recall_f1('macro')\n",
    "        weighted_prec, weighted_rec, weighted_f1 = self.precision_recall_f1('weighted')\n",
    "        \n",
    "        report['macro_avg'] = {\n",
    "            'precision': macro_prec,\n",
    "            'recall': macro_rec,\n",
    "            'f1_score': macro_f1\n",
    "        }\n",
    "        \n",
    "        report['weighted_avg'] = {\n",
    "            'precision': weighted_prec,\n",
    "            'recall': weighted_rec,\n",
    "            'f1_score': weighted_f1\n",
    "        }\n",
    "        \n",
    "        # Binary classification specific metrics\n",
    "        if self.n_classes == 2 and self.y_pred_proba is not None:\n",
    "            try:\n",
    "                auc = self.auc_score()\n",
    "                report['auc_score'] = auc\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "print(\"â Classification metrics implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Regression Metrics Implementation\n",
    "\n",
    "Let's implement comprehensive regression metrics for evaluating continuous predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive regression metrics implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize regression metrics\n",
    "        \n",
    "        Args:\n",
    "            y_true: True values\n",
    "            y_pred: Predicted values\n",
    "        \"\"\"\n",
    "        self.y_true = y_true.flatten() if y_true.ndim > 1 else y_true\n",
    "        self.y_pred = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
    "        \n",
    "        # Validate inputs\n",
    "        assert len(self.y_true) == len(self.y_pred), \"y_true and y_pred must have same length\"\n",
    "        \n",
    "        self.n_samples = len(self.y_true)\n",
    "        self.residuals = self.y_true - self.y_pred\n",
    "    \n",
    "    def mean_absolute_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Error (MAE)\n",
    "        \n",
    "        Returns:\n",
    "            MAE score\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def mean_squared_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Squared Error (MSE)\n",
    "        \n",
    "        Returns:\n",
    "            MSE score\n",
    "        \"\"\"\n",
    "        return np.mean(self.residuals ** 2)\n",
    "    \n",
    "    def root_mean_squared_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Squared Error (RMSE)\n",
    "        \n",
    "        Returns:\n",
    "            RMSE score\n",
    "        \"\"\"\n",
    "        return np.sqrt(self.mean_squared_error())\n",
    "    \n",
    "    def r2_score(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate R-squared (coefficient of determination)\n",
    "        \n",
    "        Returns:\n",
    "            RÂ² score\n",
    "        \"\"\"\n",
    "        ss_res = np.sum(self.residuals ** 2)\n",
    "        ss_tot = np.sum((self.y_true - np.mean(self.y_true)) ** 2)\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    def adjusted_r2_score(self, n_features: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Adjusted R-squared\n",
    "        \n",
    "        Args:\n",
    "            n_features: Number of features in the model\n",
    "        \n",
    "        Returns:\n",
    "            Adjusted RÂ² score\n",
    "        \"\"\"\n",
    "        r2 = self.r2_score()\n",
    "        n = self.n_samples\n",
    "        p = n_features\n",
    "        \n",
    "        if n <= p + 1:\n",
    "            return float('nan')  # Undefined when n <= p + 1\n",
    "        \n",
    "        adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "        return adj_r2\n",
    "    \n",
    "    def mean_absolute_percentage_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Percentage Error (MAPE)\n",
    "        \n",
    "        Returns:\n",
    "            MAPE score (as percentage)\n",
    "        \"\"\"\n",
    "        # Avoid division by zero\n",
    "        mask = self.y_true != 0\n",
    "        if not np.any(mask):\n",
    "            return float('inf')\n",
    "        \n",
    "        mape = np.mean(np.abs((self.y_true[mask] - self.y_pred[mask]) / self.y_true[mask])) * 100\n",
    "        return mape\n",
    "    \n",
    "    def symmetric_mean_absolute_percentage_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "        \n",
    "        Returns:\n",
    "            SMAPE score (as percentage)\n",
    "        \"\"\"\n",
    "        denominator = (np.abs(self.y_true) + np.abs(self.y_pred)) / 2\n",
    "        mask = denominator != 0\n",
    "        \n",
    "        if not np.any(mask):\n",
    "            return 0.0\n",
    "        \n",
    "        smape = np.mean(np.abs(self.residuals[mask]) / denominator[mask]) * 100\n",
    "        return smape\n",
    "    \n",
    "    def explained_variance_score(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Explained Variance Score\n",
    "        \n",
    "        Returns:\n",
    "            Explained variance score\n",
    "        \"\"\"\n",
    "        var_residual = np.var(self.residuals)\n",
    "        var_true = np.var(self.y_true)\n",
    "        \n",
    "        if var_true == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 1 - (var_residual / var_true)\n",
    "    \n",
    "    def max_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Maximum Error\n",
    "        \n",
    "        Returns:\n",
    "            Maximum absolute error\n",
    "        \"\"\"\n",
    "        return np.max(np.abs(self.residuals))\n",
    "    \n",
    "    def residual_statistics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate residual statistics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of residual statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(self.residuals),\n",
    "            'std': np.std(self.residuals),\n",
    "            'min': np.min(self.residuals),\n",
    "            'max': np.max(self.residuals),\n",
    "            'median': np.median(self.residuals),\n",
    "            'q1': np.percentile(self.residuals, 25),\n",
    "            'q3': np.percentile(self.residuals, 75),\n",
    "            'skewness': self._calculate_skewness(),\n",
    "            'kurtosis': self._calculate_kurtosis()\n",
    "        }\n",
    "    \n",
    "    def _calculate_skewness(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate skewness of residuals\n",
    "        \n",
    "        Returns:\n",
    "            Skewness value\n",
    "        \"\"\"\n",
    "        mean_residual = np.mean(self.residuals)\n",
    "        std_residual = np.std(self.residuals)\n",
    "        \n",
    "        if std_residual == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        normalized_residuals = (self.residuals - mean_residual) / std_residual\n",
    "        skewness = np.mean(normalized_residuals ** 3)\n",
    "        \n",
    "        return skewness\n",
    "    \n",
    "    def _calculate_kurtosis(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate kurtosis of residuals\n",
    "        \n",
    "        Returns:\n",
    "            Kurtosis value (excess kurtosis)\n",
    "        \"\"\"\n",
    "        mean_residual = np.mean(self.residuals)\n",
    "        std_residual = np.std(self.residuals)\n",
    "        \n",
    "        if std_residual == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        normalized_residuals = (self.residuals - mean_residual) / std_residual\n",
    "        kurtosis = np.mean(normalized_residuals ** 4) - 3  # Excess kurtosis\n",
    "        \n",
    "        return kurtosis\n",
    "    \n",
    "    def regression_report(self, n_features: Optional[int] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate comprehensive regression report\n",
    "        \n",
    "        Args:\n",
    "            n_features: Number of features (for adjusted RÂ²)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing all regression metrics\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'n_samples': self.n_samples,\n",
    "            'mae': self.mean_absolute_error(),\n",
    "            'mse': self.mean_squared_error(),\n",
    "            'rmse': self.root_mean_squared_error(),\n",
    "            'r2_score': self.r2_score(),\n",
    "            'explained_variance': self.explained_variance_score(),\n",
    "            'max_error': self.max_error(),\n",
    "            'mape': self.mean_absolute_percentage_error(),\n",
    "            'smape': self.symmetric_mean_absolute_percentage_error(),\n",
    "            'residual_stats': self.residual_statistics()\n",
    "        }\n",
    "        \n",
    "        if n_features is not None:\n",
    "            report['adjusted_r2'] = self.adjusted_r2_score(n_features)\n",
    "        \n",
    "        # Interpretation helpers\n",
    "        r2 = report['r2_score']\n",
    "        if r2 >= 0.9:\n",
    "            r2_interpretation = \"Excellent fit\"\n",
    "        elif r2 >= 0.7:\n",
    "            r2_interpretation = \"Good fit\"\n",
    "        elif r2 >= 0.5:\n",
    "            r2_interpretation = \"Moderate fit\"\n",
    "        elif r2 >= 0:\n",
    "            r2_interpretation = \"Poor fit\"\n",
    "        else:\n",
    "            r2_interpretation = \"Worse than baseline\"\n",
    "        \n",
    "        report['r2_interpretation'] = r2_interpretation\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "print(\"â Regression metrics implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Cross-Validation Implementation\n",
    "\n",
    "Let's implement cross-validation techniques for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    Cross-validation implementation for model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cv_type: str = 'kfold', n_splits: int = 5, \n",
    "                 shuffle: bool = True, random_state: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cross-validator\n",
    "        \n",
    "        Args:\n",
    "            cv_type: Type of cross-validation ('kfold', 'stratified', 'loo')\n",
    "            n_splits: Number of splits\n",
    "            shuffle: Whether to shuffle data before splitting\n",
    "            random_state: Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.cv_type = cv_type\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def split(self, X: np.ndarray, y: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Generate train/validation splits\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix\n",
    "            y: Target vector\n",
    "        \n",
    "        Returns:\n",
    "            List of (train_indices, val_indices) tuples\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        if self.cv_type == 'kfold':\n",
    "            return self._kfold_split(n_samples)\n",
    "        elif self.cv_type == 'stratified':\n",
    "            return self._stratified_split(y)\n",
    "        elif self.cv_type == 'loo':\n",
    "            return self._loo_split(n_samples)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown cv_type: {self.cv_type}\")\n",
    "    \n",
    "    def _kfold_split(self, n_samples: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        K-Fold cross-validation split\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples\n",
    "        \n",
    "        Returns:\n",
    "            List of (train_indices, val_indices) tuples\n",
    "        \"\"\"\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        splits = []\n",
    "        fold_size = n_samples // self.n_splits\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            start_idx = i * fold_size\n",
    "            if i == self.n_splits - 1:  # Last fold gets remaining samples\n",
    "                end_idx = n_samples\n",
    "            else:\n",
    "                end_idx = (i + 1) * fold_size\n",
    "            \n",
    "            val_indices = indices[start_idx:end_idx]\n",
    "            train_indices = np.concatenate([indices[:start_idx], indices[end_idx:]])\n",
    "            \n",
    "            splits.append((train_indices, val_indices))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def _stratified_split(self, y: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Stratified K-Fold cross-validation split\n",
    "        \n",
    "        Args:\n",
    "            y: Target vector\n",
    "        \n",
    "        Returns:\n",
    "            List of (train_indices, val_indices) tuples\n",
    "        \"\"\"\n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Group indices by class\n",
    "        class_indices = {}\n",
    "        for class_label in classes:\n",
    "            class_idx = np.where(y == class_label)[0]\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(class_idx)\n",
    "            class_indices[class_label] = class_idx\n",
    "        \n",
    "        splits = []\n",
    "        \n",
    "        for fold in range(self.n_splits):\n",
    "            val_indices = []\n",
    "            train_indices = []\n",
    "            \n",
    "            for class_label in classes:\n",
    "                class_idx = class_indices[class_label]\n",
    "                n_class_samples = len(class_idx)\n",
    "                \n",
    "                # Calculate validation indices for this class\n",
    "                fold_size = n_class_samples // self.n_splits\n",
    "                start_idx = fold * fold_size\n",
    "                \n",
    "                if fold == self.n_splits - 1:\n",
    "                    end_idx = n_class_samples\n",
    "                else:\n",
    "                    end_idx = (fold + 1) * fold_size\n",
    "                \n",
    "                val_indices.extend(class_idx[start_idx:end_idx])\n",
    "                train_indices.extend(np.concatenate([\n",
    "                    class_idx[:start_idx], \n",
    "                    class_idx[end_idx:]\n",
    "                ]))\n",
    "            \n",
    "            splits.append((np.array(train_indices), np.array(val_indices)))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def _loo_split(self, n_samples: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Leave-One-Out cross-validation split\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples\n",
    "        \n",
    "        Returns:\n",
    "            List of (train_indices, val_indices) tuples\n",
    "        \"\"\"\n",
    "        splits = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            val_indices = np.array([i])\n",
    "            train_indices = np.concatenate([np.arange(i), np.arange(i+1, n_samples)])\n",
    "            splits.append((train_indices, val_indices))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def cross_validate(self, model_fn: Callable, X: np.ndarray, y: np.ndarray,\n",
    "                      scoring: Union[str, Callable] = 'accuracy',\n",
    "                      return_train_score: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform cross-validation\n",
    "        \n",
    "        Args:\n",
    "            model_fn: Function that creates and trains a model\n",
    "            X: Feature matrix\n",
    "            y: Target vector\n",
    "            scoring: Scoring metric ('accuracy', 'f1', 'r2', or callable)\n",
    "            return_train_score: Whether to return training scores\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with cross-validation results\n",
    "        \"\"\"\n",
    "        splits = self.split(X, y)\n",
    "        \n",
    "        val_scores = []\n",
    "        train_scores = [] if return_train_score else None\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "            print(f\"Fold {fold + 1}/{len(splits)}...\", end=' ')\n",
    "            \n",
    "            # Split data\n",
    "            X_train_fold = X[train_idx]\n",
    "            y_train_fold = y[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_val_fold = y[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = model_fn()\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Evaluate\n",
    "            val_pred = model.predict(X_val_fold)\n",
    "            val_score = self._calculate_score(y_val_fold, val_pred, scoring)\n",
    "            val_scores.append(val_score)\n",
    "            \n",
    "            if return_train_score:\n",
    "                train_pred = model.predict(X_train_fold)\n",
    "                train_score = self._calculate_score(y_train_fold, train_pred, scoring)\n",
    "                train_scores.append(train_score)\n",
    "            \n",
    "            print(f\"Score: {val_score:.4f}\")\n",
    "        \n",
    "        results = {\n",
    "            'val_scores': np.array(val_scores),\n",
    "            'val_mean': np.mean(val_scores),\n",
    "            'val_std': np.std(val_scores),\n",
    "            'val_min': np.min(val_scores),\n",
    "            'val_max': np.max(val_scores)\n",
    "        }\n",
    "        \n",
    "        if return_train_score:\n",
    "            results.update({\n",
    "                'train_scores': np.array(train_scores),\n",
    "                'train_mean': np.mean(train_scores),\n",
    "                'train_std': np.std(train_scores)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                        scoring: Union[str, Callable]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate score based on scoring metric\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels/values\n",
    "            y_pred: Predicted labels/values\n",
    "            scoring: Scoring metric\n",
    "        \n",
    "        Returns:\n",
    "            Score value\n",
    "        \"\"\"\n",
    "        if callable(scoring):\n",
    "            return scoring(y_true, y_pred)\n",
    "        \n",
    "        if scoring == 'accuracy':\n",
    "            return np.mean(y_true == y_pred)\n",
    "        elif scoring == 'f1':\n",
    "            metrics = ClassificationMetrics(y_true, y_pred)\n",
    "            _, _, f1 = metrics.precision_recall_f1('binary' if len(np.unique(y_true)) == 2 else 'weighted')\n",
    "            return f1\n",
    "        elif scoring == 'r2':\n",
    "            metrics = RegressionMetrics(y_true, y_pred)\n",
    "            return metrics.r2_score()\n",
    "        elif scoring == 'mae':\n",
    "            metrics = RegressionMetrics(y_true, y_pred)\n",
    "            return -metrics.mean_absolute_error()  # Negative for maximization\n",
    "        elif scoring == 'mse':\n",
    "            metrics = RegressionMetrics(y_true, y_pred)\n",
    "            return -metrics.mean_squared_error()  # Negative for maximization\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scoring metric: {scoring}\")\n",
    "\n",
    "\n",
    "print(\"â Cross-validation implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualization Tools for Model Evaluation\n",
    "\n",
    "Let's create comprehensive visualization tools for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization tools for model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, figsize: Tuple[int, int] = (12, 8)):\n",
    "        self.figsize = figsize\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str], \n",
    "                            normalize: bool = False, title: str = \"Confusion Matrix\"):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix\n",
    "        \n",
    "        Args:\n",
    "            cm: Confusion matrix\n",
    "            class_names: Names of classes\n",
    "            normalize: Whether to normalize the matrix\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm_display = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm_display = np.nan_to_num(cm_display)\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            cm_display = cm\n",
    "            fmt = 'd'\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_display, annot=True, fmt=fmt, cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        \n",
    "        plt.title(title + (\" (Normalized)\" if normalize else \"\"))\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray, auc: float,\n",
    "                      title: str = \"ROC Curve\"):\n",
    "        \"\"\"\n",
    "        Plot ROC curve\n",
    "        \n",
    "        Args:\n",
    "            fpr: False positive rates\n",
    "            tpr: True positive rates\n",
    "            auc: Area under the curve\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random Classifier')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axis('equal')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_precision_recall_curve(self, precision: np.ndarray, recall: np.ndarray,\n",
    "                                   title: str = \"Precision-Recall Curve\"):\n",
    "        \"\"\"\n",
    "        Plot Precision-Recall curve\n",
    "        \n",
    "        Args:\n",
    "            precision: Precision values\n",
    "            recall: Recall values\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        # Calculate AUC-PR\n",
    "        auc_pr = np.trapz(precision[::-1], recall[::-1])  # Reverse for correct integration\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, 'b-', linewidth=2, label=f'PR Curve (AUC = {auc_pr:.3f})')\n",
    "        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_residuals(self, y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                      title: str = \"Residual Analysis\"):\n",
    "        \"\"\"\n",
    "        Plot residual analysis for regression\n",
    "        \n",
    "        Args:\n",
    "            y_true: True values\n",
    "            y_pred: Predicted values\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        residuals = y_true - y_pred\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Residuals vs Predicted\n",
    "        axes[0, 0].scatter(y_pred, residuals, alpha=0.6)\n",
    "        axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[0, 0].set_xlabel('Predicted Values')\n",
    "        axes[0, 0].set_ylabel('Residuals')\n",
    "        axes[0, 0].set_title('Residuals vs Predicted')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # True vs Predicted\n",
    "        axes[0, 1].scatter(y_true, y_pred, alpha=0.6)\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        axes[0, 1].set_xlabel('True Values')\n",
    "        axes[0, 1].set_ylabel('Predicted Values')\n",
    "        axes[0, 1].set_title('True vs Predicted')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residual histogram\n",
    "        axes[1, 0].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "        axes[1, 0].set_xlabel('Residuals')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Residual Distribution')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot (simplified)\n",
    "        sorted_residuals = np.sort(residuals)\n",
    "        theoretical_quantiles = np.linspace(-3, 3, len(sorted_residuals))\n",
    "        axes[1, 1].scatter(theoretical_quantiles, sorted_residuals, alpha=0.6)\n",
    "        axes[1, 1].plot(theoretical_quantiles, theoretical_quantiles * np.std(residuals), 'r--')\n",
    "        axes[1, 1].set_xlabel('Theoretical Quantiles')\n",
    "        axes[1, 1].set_ylabel('Sample Quantiles')\n",
    "        axes[1, 1].set_title('Q-Q Plot')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_learning_curve(self, train_sizes: np.ndarray, train_scores: np.ndarray,\n",
    "                          val_scores: np.ndarray, title: str = \"Learning Curve\"):\n",
    "        \"\"\"\n",
    "        Plot learning curve\n",
    "        \n",
    "        Args:\n",
    "            train_sizes: Training set sizes\n",
    "            train_scores: Training scores\n",
    "            val_scores: Validation scores\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Calculate means and standard deviations\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # Plot training scores\n",
    "        plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                        alpha=0.2, color='blue')\n",
    "        \n",
    "        # Plot validation scores\n",
    "        plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
    "                        alpha=0.2, color='red')\n",
    "        \n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_model_comparison(self, model_names: List[str], scores: Dict[str, List[float]],\n",
    "                            title: str = \"Model Comparison\"):\n",
    "        \"\"\"\n",
    "        Plot model comparison\n",
    "        \n",
    "        Args:\n",
    "            model_names: Names of models\n",
    "            scores: Dictionary of metric_name -> list of scores for each model\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        n_metrics = len(scores)\n",
    "        fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 6))\n",
    "        \n",
    "        if n_metrics == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (metric_name, metric_scores) in enumerate(scores.items()):\n",
    "            axes[i].boxplot(metric_scores, labels=model_names)\n",
    "            axes[i].set_title(f'{metric_name.title()} Comparison')\n",
    "            axes[i].set_ylabel(metric_name.title())\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean values as text\n",
    "            for j, scores_list in enumerate(metric_scores):\n",
    "                mean_score = np.mean(scores_list)\n",
    "                axes[i].text(j + 1, mean_score, f'{mean_score:.3f}', \n",
    "                           ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"â Model evaluation visualizer implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Model Evaluation Framework\n",
    "\n",
    "Let's test our comprehensive evaluation framework with real datasets and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simplified neural network for testing (from previous labs)\n",
    "class SimpleModel:\n",
    "    \"\"\"\n",
    "    Simplified model for testing evaluation framework\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type: str = 'classification'):\n",
    "        self.problem_type = problem_type\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Simple linear model fitting\"\"\"\n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        if self.problem_type == 'classification':\n",
    "            # Logistic regression (simplified)\n",
    "            if y.ndim == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "            \n",
    "            # Initialize weights\n",
    "            self.weights = np.random.normal(0, 0.01, (X_with_bias.shape[1], 1))\n",
    "            \n",
    "            # Simple gradient descent\n",
    "            learning_rate = 0.01\n",
    "            for _ in range(100):\n",
    "                z = X_with_bias.dot(self.weights)\n",
    "                predictions = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "                \n",
    "                gradients = X_with_bias.T.dot(predictions - y) / len(y)\n",
    "                self.weights -= learning_rate * gradients\n",
    "        \n",
    "        else:  # regression\n",
    "            # Linear regression (closed form)\n",
    "            if y.ndim == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "            \n",
    "            # Solve normal equations\n",
    "            try:\n",
    "                self.weights = np.linalg.solve(X_with_bias.T.dot(X_with_bias), X_with_bias.T.dot(y))\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Use pseudo-inverse if singular\n",
    "                self.weights = np.linalg.pinv(X_with_bias).dot(y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        if self.problem_type == 'classification':\n",
    "            # Logistic regression predictions\n",
    "            z = X_with_bias.dot(self.weights)\n",
    "            probabilities = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "            return (probabilities > 0.5).astype(int)\n",
    "        else:\n",
    "            # Linear regression predictions\n",
    "            return X_with_bias.dot(self.weights)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict probabilities (classification only)\"\"\"\n",
    "        if self.problem_type != 'classification':\n",
    "            raise ValueError(\"predict_proba only available for classification\")\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        z = X_with_bias.dot(self.weights)\n",
    "        probabilities = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "\n",
    "print(\"â Simple model for testing created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Classification Evaluation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Test 1: Classification Evaluation\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Generate binary classification dataset\n",
    "X_cls, y_cls = make_classification(\n",
    "    n_samples=800,\n",
    "    n_features=5,\n",
    "    n_redundant=0,\n",
    "    n_informative=5,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42, stratify=y_cls\n",
    ")\n",
    "\n",
    "scaler_cls = StandardScaler()\n",
    "X_train_cls_scaled = scaler_cls.fit_transform(X_train_cls)\n",
    "X_test_cls_scaled = scaler_cls.transform(X_test_cls)\n",
    "\n",
    "print(f\"Classification dataset: {X_train_cls_scaled.shape[0]} train, {X_test_cls_scaled.shape[0]} test\")\n",
    "print(f\"Class distribution: {np.bincount(y_train_cls)}\")\n",
    "\n",
    "# Train model\n",
    "model_cls = SimpleModel('classification')\n",
    "model_cls.fit(X_train_cls_scaled, y_train_cls)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_cls = model_cls.predict(X_test_cls_scaled).flatten()\n",
    "y_pred_proba_cls = model_cls.predict_proba(X_test_cls_scaled).flatten()\n",
    "\n",
    "print(f\"\\nPredictions made: {len(y_pred_cls)} samples\")\n",
    "print(f\"Prediction distribution: {np.bincount(y_pred_cls)}\")\n",
    "\n",
    "# Comprehensive classification evaluation\n",
    "cls_metrics = ClassificationMetrics(\n",
    "    y_test_cls, y_pred_cls, y_pred_proba_cls, \n",
    "    class_names=['Class 0', 'Class 1']\n",
    ")\n",
    "\n",
    "# Generate classification report\n",
    "cls_report = cls_metrics.classification_report()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Accuracy: {cls_report['accuracy']:.4f}\")\n",
    "print(f\"AUC Score: {cls_report.get('auc_score', 'N/A')}\")\n",
    "print(f\"\\nMacro Average:\")\n",
    "print(f\"  Precision: {cls_report['macro_avg']['precision']:.4f}\")\n",
    "print(f\"  Recall: {cls_report['macro_avg']['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {cls_report['macro_avg']['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for class_name, metrics in cls_report['per_class'].items():\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"    Support: {metrics['support']}\")\n",
    "\n",
    "print(\"\\nâ Classification evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Regression Evaluation\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "print(\"\\nTest 2: Regression Evaluation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=600,\n",
    "    n_features=4,\n",
    "    noise=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "print(f\"Regression dataset: {X_train_reg_scaled.shape[0]} train, {X_test_reg_scaled.shape[0]} test\")\n",
    "print(f\"Target range: [{y_train_reg.min():.2f}, {y_train_reg.max():.2f}]\")\n",
    "\n",
    "# Train model\n",
    "model_reg = SimpleModel('regression')\n",
    "model_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = model_reg.predict(X_test_reg_scaled).flatten()\n",
    "\n",
    "print(f\"\\nPredictions made: {len(y_pred_reg)} samples\")\n",
    "print(f\"Prediction range: [{y_pred_reg.min():.2f}, {y_pred_reg.max():.2f}]\")\n",
    "\n",
    "# Comprehensive regression evaluation\n",
    "reg_metrics = RegressionMetrics(y_test_reg, y_pred_reg)\n",
    "reg_report = reg_metrics.regression_report(n_features=X_test_reg_scaled.shape[1])\n",
    "\n",
    "print(\"\\nRegression Report:\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"RÂ² Score: {reg_report['r2_score']:.4f} ({reg_report['r2_interpretation']})\")\n",
    "print(f\"Adjusted RÂ²: {reg_report.get('adjusted_r2', 'N/A')}\")\n",
    "print(f\"MAE: {reg_report['mae']:.4f}\")\n",
    "print(f\"MSE: {reg_report['mse']:.4f}\")\n",
    "print(f\"RMSE: {reg_report['rmse']:.4f}\")\n",
    "print(f\"Max Error: {reg_report['max_error']:.4f}\")\n",
    "print(f\"MAPE: {reg_report['mape']:.2f}%\")\n",
    "print(f\"Explained Variance: {reg_report['explained_variance']:.4f}\")\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "residual_stats = reg_report['residual_stats']\n",
    "print(f\"  Mean: {residual_stats['mean']:.6f}\")\n",
    "print(f\"  Std: {residual_stats['std']:.4f}\")\n",
    "print(f\"  Skewness: {residual_stats['skewness']:.4f}\")\n",
    "print(f\"  Kurtosis: {residual_stats['kurtosis']:.4f}\")\n",
    "\n",
    "print(\"\\nâ Regression evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualization Testing\n",
    "\n",
    "Let's test our visualization tools with the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visualization tools\n",
    "print(\"Testing Model Evaluation Visualizations\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "visualizer = ModelEvaluationVisualizer()\n",
    "\n",
    "# Classification visualizations\n",
    "print(\"\\n1. Confusion Matrix\")\n",
    "cm = cls_metrics.confusion_matrix()\n",
    "visualizer.plot_confusion_matrix(cm, ['Class 0', 'Class 1'], normalize=False)\n",
    "visualizer.plot_confusion_matrix(cm, ['Class 0', 'Class 1'], normalize=True)\n",
    "\n",
    "print(\"\\n2. ROC Curve\")\n",
    "try:\n",
    "    fpr, tpr, _ = cls_metrics.roc_curve()\n",
    "    auc_score = cls_metrics.auc_score()\n",
    "    visualizer.plot_roc_curve(fpr, tpr, auc_score)\n",
    "except Exception as e:\n",
    "    print(f\"ROC curve plotting failed: {e}\")\n",
    "\n",
    "print(\"\\n3. Precision-Recall Curve\")\n",
    "try:\n",
    "    precision, recall, _ = cls_metrics.precision_recall_curve()\n",
    "    visualizer.plot_precision_recall_curve(precision, recall)\n",
    "except Exception as e:\n",
    "    print(f\"PR curve plotting failed: {e}\")\n",
    "\n",
    "print(\"\\n4. Residual Analysis (Regression)\")\n",
    "visualizer.plot_residuals(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"\\nâ Visualization testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cross-validation\n",
    "print(\"\\nTesting Cross-Validation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Define model creation function\n",
    "def create_classification_model():\n",
    "    return SimpleModel('classification')\n",
    "\n",
    "def create_regression_model():\n",
    "    return SimpleModel('regression')\n",
    "\n",
    "# Test classification cross-validation\n",
    "print(\"\\nClassification Cross-Validation (K-Fold):\")\n",
    "cv_cls = CrossValidator(cv_type='kfold', n_splits=5, random_state=42)\n",
    "cv_results_cls = cv_cls.cross_validate(\n",
    "    create_classification_model,\n",
    "    X_train_cls_scaled, y_train_cls,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Validation Mean: {cv_results_cls['val_mean']:.4f} Â± {cv_results_cls['val_std']:.4f}\")\n",
    "print(f\"Training Mean: {cv_results_cls['train_mean']:.4f} Â± {cv_results_cls['train_std']:.4f}\")\n",
    "print(f\"Individual fold scores: {cv_results_cls['val_scores']}\")\n",
    "\n",
    "# Test regression cross-validation\n",
    "print(\"\\nRegression Cross-Validation (K-Fold):\")\n",
    "cv_reg = CrossValidator(cv_type='kfold', n_splits=5, random_state=42)\n",
    "cv_results_reg = cv_reg.cross_validate(\n",
    "    create_regression_model,\n",
    "    X_train_reg_scaled, y_train_reg,\n",
    "    scoring='r2',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Validation Mean: {cv_results_reg['val_mean']:.4f} Â± {cv_results_reg['val_std']:.4f}\")\n",
    "print(f\"Training Mean: {cv_results_reg['train_mean']:.4f} Â± {cv_results_reg['train_std']:.4f}\")\n",
    "print(f\"Individual fold scores: {cv_results_reg['val_scores']}\")\n",
    "\n",
    "# Test stratified cross-validation\n",
    "print(\"\\nStratified Cross-Validation:\")\n",
    "cv_stratified = CrossValidator(cv_type='stratified', n_splits=5, random_state=42)\n",
    "cv_results_stratified = cv_stratified.cross_validate(\n",
    "    create_classification_model,\n",
    "    X_train_cls_scaled, y_train_cls,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "print(f\"F1 Score Mean: {cv_results_stratified['val_mean']:.4f} Â± {cv_results_stratified['val_std']:.4f}\")\n",
    "\n",
    "print(\"\\nâ Cross-validation testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Checklist\n",
    "\n",
    "Mark each completed section:\n",
    "\n",
    "- [ ] â **Environment Setup**: Imported libraries and established evaluation framework\n",
    "- [ ] â **Classification Metrics**: Implemented comprehensive classification evaluation metrics\n",
    "- [ ] â **Regression Metrics**: Built complete regression evaluation system\n",
    "- [ ] â **Cross-Validation**: Created flexible cross-validation framework\n",
    "- [ ] â **Visualization Tools**: Implemented comprehensive evaluation visualization tools\n",
    "- [ ] â **Model Testing**: Created simple models for testing evaluation framework\n",
    "- [ ] â **Classification Testing**: Successfully tested classification evaluation\n",
    "- [ ] â **Regression Testing**: Validated regression evaluation metrics\n",
    "- [ ] â **Visualization Testing**: Tested and validated visualization tools\n",
    "- [ ] â **Cross-Validation Testing**: Verified cross-validation implementation\n",
    "\n",
    "## Key Concepts Summary\n",
    "\n",
    "**Classification Metrics:**\n",
    "1. **Basic Metrics**: Accuracy, Precision, Recall, F1-Score\n",
    "2. **Advanced Metrics**: ROC-AUC, Precision-Recall AUC, Specificity\n",
    "3. **Multi-class**: Macro, Micro, and Weighted averaging strategies\n",
    "4. **Confusion Matrix**: Complete breakdown of prediction performance\n",
    "\n",
    "**Regression Metrics:**\n",
    "1. **Error Metrics**: MAE, MSE, RMSE, Max Error\n",
    "2. **Percentage Errors**: MAPE, SMAPE for interpretable error rates\n",
    "3. **Variance Metrics**: RÂ², Adjusted RÂ², Explained Variance\n",
    "4. **Residual Analysis**: Statistical properties of prediction errors\n",
    "\n",
    "**Cross-Validation Techniques:**\n",
    "- **K-Fold**: Balanced splits for general use\n",
    "- **Stratified**: Maintains class proportions for classification\n",
    "- **Leave-One-Out**: Maximum data utilization for small datasets\n",
    "- **Custom Splits**: Flexible framework for domain-specific validation\n",
    "\n",
    "**Evaluation Best Practices:**\n",
    "- Use appropriate metrics for your problem type\n",
    "- Always validate on unseen data\n",
    "- Consider class imbalance in metric selection\n",
    "- Visualize results for better interpretation\n",
    "- Use cross-validation for robust assessment\n",
    "- Report confidence intervals when possible\n",
    "\n",
    "**Visualization Benefits:**\n",
    "- Confusion matrices reveal classification patterns\n",
    "- ROC curves show threshold trade-offs\n",
    "- Residual plots identify regression issues\n",
    "- Learning curves detect overfitting\n",
    "- Model comparisons guide selection\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Imbalanced Dataset Metrics**\n",
    "   - *Problem*: Accuracy misleading with class imbalance\n",
    "   - *Solutions*: Use F1-score, precision-recall curves, balanced accuracy\n",
    "\n",
    "2. **Division by Zero in Metrics**\n",
    "   - *Problem*: No true positives or false positives\n",
    "   - *Solutions*: Handle edge cases, use smoothed metrics, check data quality\n",
    "\n",
    "3. **Overfitting Detection**\n",
    "   - *Problem*: Large gap between train and validation scores\n",
    "   - *Solutions*: Use cross-validation, learning curves, regularization\n",
    "\n",
    "4. **Metric Selection**\n",
    "   - *Problem*: Choosing wrong metric for business objective\n",
    "   - *Solutions*: Understand domain requirements, use multiple complementary metrics\n",
    "\n",
    "5. **Cross-Validation Issues**\n",
    "   - *Problem*: High variance in CV scores\n",
    "   - *Solutions*: Increase number of folds, ensure proper stratification, check data quality\n",
    "\n",
    "6. **Visualization Problems**\n",
    "   - *Problem*: Plots not displaying correctly\n",
    "   - *Solutions*: Check data shapes, handle edge cases, validate input ranges\n",
    "\n",
    "## Validation Steps\n",
    "\n",
    "**Verify Your Implementation:**\n",
    "1. â Classification metrics match expected mathematical definitions\n",
    "2. â Regression metrics provide meaningful error measurements\n",
    "3. â Cross-validation produces consistent, reasonable results\n",
    "4. â Visualizations accurately represent the underlying data\n",
    "5. â Edge cases (perfect predictions, no variance) are handled properly\n",
    "6. â Multi-class and binary classification both work correctly\n",
    "\n",
    "## Cleanup Instructions\n",
    "\n",
    "**After Lab Completion:**\n",
    "1. Save your notebook with all evaluation results and visualizations\n",
    "2. Clear large variables if memory is limited:\n",
    "   ```python\n",
    "   # Uncomment if needed\n",
    "   # del X_train_cls_scaled, X_test_cls_scaled\n",
    "   # del X_train_reg_scaled, X_test_reg_scaled\n",
    "   # del cls_metrics, reg_metrics\n",
    "   ```\n",
    "3. Close plots: `plt.close('all')`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Your comprehensive model evaluation framework enables:\n",
    "- **Professional ML Projects**: Industry-standard evaluation practices\n",
    "- **Model Selection**: Systematic comparison of different approaches\n",
    "- **Performance Monitoring**: Continuous evaluation in production\n",
    "- **Research Applications**: Rigorous experimental evaluation\n",
    "- **Automated ML**: Integration with hyperparameter optimization\n",
    "\n",
    "The evaluation techniques you've learned are fundamental to successful machine learning practice and are used across all domains and applications.\n",
    "\n",
    "**Congratulations! You've completed Lab 2.6 - Model Evaluation and Metrics!** ð"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}