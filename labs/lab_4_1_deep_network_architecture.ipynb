{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.1: Deep Network Architecture with TensorFlow\n\n## Duration: 45 minutes\n\n## Learning Objectives\nBy the end of this lab, you will be able to:\n- Design and implement deep neural network architectures using TensorFlow/Keras\n- Understand different layer types and their applications\n- Configure weight initialization strategies in TensorFlow\n- Build flexible neural network architectures for various tasks\n- Compare traditional implementation with modern framework approaches\n\n## Prerequisites\n- Basic understanding of neural networks from previous labs\n- Knowledge of Python and NumPy\n- TensorFlow installed and working\n\n## Key Concepts\n- **TensorFlow/Keras**: High-level deep learning framework\n- **Sequential vs Functional API**: Different ways to build models\n- **Layer Types**: Dense, Dropout, BatchNormalization, etc.\n- **Weight Initializers**: Built-in initialization strategies\n- **Model Architecture**: Modern approaches to network design"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, initializers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Configure matplotlib for better plots\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Environment setup complete!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\n\n# Check if GPU is available\nif tf.config.list_physical_devices('GPU'):\n    print(\"✅ GPU is available\")\nelse:\n    print(\"💻 Using CPU (GPU not available)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Understanding TensorFlow/Keras Architecture Building\n\nTensorFlow/Keras provides two main approaches for building neural networks:\n- **Sequential API**: Linear stack of layers (simpler)\n- **Functional API**: More flexible, allows complex architectures\n\nLet's explore both approaches and understand modern deep learning architecture patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_sequential_model(layer_sizes, activation='relu', output_activation='linear'):\n    \"\"\"\n    Create a neural network using Sequential API\n    \n    Parameters:\n    layer_sizes: list of integers representing number of neurons in each layer\n    activation: activation function for hidden layers\n    output_activation: activation function for output layer\n    \n    Returns:\n    model: Keras Sequential model\n    \"\"\"\n    model = keras.Sequential(name='DeepNN_Sequential')\n    \n    # Add input layer (first hidden layer)\n    model.add(layers.Dense(layer_sizes[1], \n                          input_shape=(layer_sizes[0],),\n                          activation=activation,\n                          name=f'dense_1'))\n    \n    # Add hidden layers\n    for i in range(2, len(layer_sizes)-1):\n        model.add(layers.Dense(layer_sizes[i], \n                              activation=activation,\n                              name=f'dense_{i}'))\n    \n    # Add output layer\n    model.add(layers.Dense(layer_sizes[-1], \n                          activation=output_activation,\n                          name='output'))\n    \n    return model\n\ndef create_functional_model(layer_sizes, activation='relu', output_activation='linear'):\n    \"\"\"\n    Create a neural network using Functional API\n    \n    Parameters:\n    layer_sizes: list of integers representing number of neurons in each layer\n    activation: activation function for hidden layers\n    output_activation: activation function for output layer\n    \n    Returns:\n    model: Keras Functional model\n    \"\"\"\n    # Define input\n    inputs = layers.Input(shape=(layer_sizes[0],), name='input')\n    \n    # First hidden layer\n    x = layers.Dense(layer_sizes[1], activation=activation, name='dense_1')(inputs)\n    \n    # Additional hidden layers\n    for i in range(2, len(layer_sizes)-1):\n        x = layers.Dense(layer_sizes[i], activation=activation, name=f'dense_{i}')(x)\n    \n    # Output layer\n    outputs = layers.Dense(layer_sizes[-1], activation=output_activation, name='output')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='DeepNN_Functional')\n    \n    return model\n\ndef visualize_model_architecture(model, title=\"Neural Network Architecture\"):\n    \"\"\"\n    Visualize model architecture using TensorFlow's built-in plotting\n    \"\"\"\n    print(f\"\\n{title}\")\n    print(\"=\" * 60)\n    \n    # Print model summary\n    model.summary()\n    \n    # Plot model architecture\n    try:\n        keras.utils.plot_model(model, \n                              show_shapes=True, \n                              show_layer_names=True,\n                              rankdir='LR',\n                              show_dtype=False)\n        plt.title(title)\n        plt.show()\n    except ImportError:\n        print(\"Note: Install pydot and graphviz for visual model plots\")\n    \n    return model\n\n# Example architectures\narchitectures = {\n    'Shallow Network': [4, 8, 1],\n    'Deep Network': [784, 256, 128, 64, 10],\n    'Very Deep Network': [784, 512, 256, 128, 64, 32, 10]\n}\n\nprint(\"Creating Different Network Architectures with TensorFlow:\")\nprint()\n\nfor name, arch in architectures.items():\n    print(f\"\\n{name}: {arch}\")\n    \n    # Create using Sequential API\n    seq_model = create_sequential_model(arch)\n    print(f\"✅ Sequential model created - Total parameters: {seq_model.count_params():,}\")\n    \n    # Create using Functional API  \n    func_model = create_functional_model(arch)\n    print(f\"✅ Functional model created - Total parameters: {func_model.count_params():,}\")\n    \n    # Show detailed architecture for first model\n    if name == 'Deep Network':\n        visualize_model_architecture(seq_model, f\"{name} (Sequential API)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Weight Initialization in TensorFlow\n\nTensorFlow provides many built-in initialization strategies. Let's explore and compare them:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model_with_initializer(layer_sizes, kernel_initializer='he_normal', bias_initializer='zeros'):\n    \"\"\"\n    Create model with specific weight initialization\n    \n    Parameters:\n    layer_sizes: list of layer sizes\n    kernel_initializer: weight initialization strategy\n    bias_initializer: bias initialization strategy\n    \n    Returns:\n    model: Keras model with specified initialization\n    \"\"\"\n    model = keras.Sequential(name=f'Model_{kernel_initializer}')\n    \n    # Input layer\n    model.add(layers.Dense(layer_sizes[1], \n                          input_shape=(layer_sizes[0],),\n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          bias_initializer=bias_initializer,\n                          name=f'dense_1'))\n    \n    # Hidden layers\n    for i in range(2, len(layer_sizes)-1):\n        model.add(layers.Dense(layer_sizes[i], \n                              activation='relu',\n                              kernel_initializer=kernel_initializer,\n                              bias_initializer=bias_initializer,\n                              name=f'dense_{i}'))\n    \n    # Output layer\n    model.add(layers.Dense(layer_sizes[-1], \n                          activation='linear',\n                          kernel_initializer=kernel_initializer,\n                          bias_initializer=bias_initializer,\n                          name='output'))\n    \n    return model\n\ndef analyze_weight_distributions(models_dict, layer_index=0):\n    \"\"\"\n    Analyze and visualize weight distributions from different models\n    \n    Parameters:\n    models_dict: dictionary of models with their initializer names\n    layer_index: which layer to analyze (0-indexed)\n    \"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    for idx, (init_name, model) in enumerate(models_dict.items()):\n        if idx >= len(axes):\n            break\n            \n        # Get weights from specified layer\n        weights = model.layers[layer_index].get_weights()[0]  # [0] for kernel weights\n        \n        # Plot histogram\n        axes[idx].hist(weights.ravel(), bins=50, alpha=0.7, density=True)\n        axes[idx].set_title(f'{init_name}\\nMean: {weights.mean():.4f}, Std: {weights.std():.4f}')\n        axes[idx].set_xlabel('Weight Value')\n        axes[idx].set_ylabel('Density')\n        axes[idx].grid(True, alpha=0.3)\n        axes[idx].axvline(weights.mean(), color='red', linestyle='--', alpha=0.8)\n    \n    # Remove unused subplots\n    for idx in range(len(models_dict), len(axes)):\n        axes[idx].remove()\n    \n    plt.tight_layout()\n    plt.suptitle(f'Weight Distributions Comparison (Layer {layer_index + 1})', y=1.02, fontsize=16)\n    plt.show()\n\n# Test different TensorFlow initializers\nprint(\"Testing TensorFlow Weight Initializers:\")\nprint()\n\ntest_architecture = [100, 64, 32, 1]\n\n# Available initializers in TensorFlow\ntf_initializers = {\n    'glorot_uniform': 'glorot_uniform',     # Xavier uniform\n    'glorot_normal': 'glorot_normal',       # Xavier normal\n    'he_uniform': 'he_uniform',             # He uniform\n    'he_normal': 'he_normal',               # He normal\n    'random_uniform': 'random_uniform',     # Random uniform\n    'random_normal': 'random_normal'        # Random normal\n}\n\n# Create models with different initializers\nmodels = {}\nfor name, init in tf_initializers.items():\n    model = create_model_with_initializer(test_architecture, kernel_initializer=init)\n    models[name] = model\n    \n    # Get first layer weights for analysis\n    first_layer_weights = model.layers[0].get_weights()[0]\n    print(f\"{name:15}: Shape {first_layer_weights.shape}, \"\n          f\"Mean: {first_layer_weights.mean():.4f}, \"\n          f\"Std: {first_layer_weights.std():.4f}\")\n\nprint(\"\\nVisualizing weight distributions:\")\nanalyze_weight_distributions(models, layer_index=0)\n\n# Show detailed comparison between Xavier and He initialization\nprint(\"\\nDetailed Comparison: Xavier vs He Initialization\")\nprint(\"-\" * 60)\n\nxavier_model = create_model_with_initializer(test_architecture, kernel_initializer='glorot_normal')\nhe_model = create_model_with_initializer(test_architecture, kernel_initializer='he_normal')\n\nxavier_weights = xavier_model.layers[0].get_weights()[0]\nhe_weights = he_model.layers[0].get_weights()[0]\n\nprint(f\"Xavier Normal - Mean: {xavier_weights.mean():.6f}, Std: {xavier_weights.std():.6f}\")\nprint(f\"He Normal     - Mean: {he_weights.mean():.6f}, Std: {he_weights.std():.6f}\")\nprint()\nprint(\"Key Differences:\")\nprint(\"- Xavier initialization: Good for tanh, sigmoid activations\")\nprint(\"- He initialization: Optimized for ReLU and its variants\")\nprint(\"- He uses larger initial weights to compensate for ReLU's zero-killing property\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Advanced Layer Types and Modern Architectures\n\nTensorFlow/Keras provides many advanced layer types that improve deep network performance:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_modern_deep_network(input_dim, num_classes, architecture_type='standard'):\n    \"\"\"\n    Create modern deep network with advanced techniques\n    \n    Parameters:\n    input_dim: number of input features\n    num_classes: number of output classes\n    architecture_type: 'standard', 'dropout', 'batch_norm', or 'advanced'\n    \n    Returns:\n    model: Keras model with modern architecture\n    \"\"\"\n    \n    if architecture_type == 'standard':\n        # Basic deep network\n        model = keras.Sequential([\n            layers.Dense(512, activation='relu', input_shape=(input_dim,)),\n            layers.Dense(256, activation='relu'),\n            layers.Dense(128, activation='relu'),\n            layers.Dense(64, activation='relu'),\n            layers.Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid')\n        ], name='Standard_Deep_Network')\n    \n    elif architecture_type == 'dropout':\n        # Network with dropout for regularization\n        model = keras.Sequential([\n            layers.Dense(512, activation='relu', input_shape=(input_dim,)),\n            layers.Dropout(0.3),\n            layers.Dense(256, activation='relu'),\n            layers.Dropout(0.3),\n            layers.Dense(128, activation='relu'),\n            layers.Dropout(0.2),\n            layers.Dense(64, activation='relu'),\n            layers.Dropout(0.2),\n            layers.Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid')\n        ], name='Dropout_Deep_Network')\n    \n    elif architecture_type == 'batch_norm':\n        # Network with batch normalization\n        model = keras.Sequential([\n            layers.Dense(512, input_shape=(input_dim,)),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            \n            layers.Dense(256),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            \n            layers.Dense(128),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            \n            layers.Dense(64),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            \n            layers.Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid')\n        ], name='BatchNorm_Deep_Network')\n    \n    elif architecture_type == 'advanced':\n        # Advanced network combining multiple techniques\n        model = keras.Sequential([\n            # First block\n            layers.Dense(512, input_shape=(input_dim,), kernel_initializer='he_normal'),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            layers.Dropout(0.3),\n            \n            # Second block\n            layers.Dense(256, kernel_initializer='he_normal'),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            layers.Dropout(0.3),\n            \n            # Third block\n            layers.Dense(128, kernel_initializer='he_normal'),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            layers.Dropout(0.2),\n            \n            # Fourth block\n            layers.Dense(64, kernel_initializer='he_normal'),\n            layers.BatchNormalization(),\n            layers.Activation('relu'),\n            layers.Dropout(0.2),\n            \n            # Output\n            layers.Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid')\n        ], name='Advanced_Deep_Network')\n    \n    return model\n\ndef create_residual_block(x, units, dropout_rate=0.2):\n    \"\"\"\n    Create a residual block (simplified ResNet-style)\n    \n    Parameters:\n    x: input tensor\n    units: number of units in the block\n    dropout_rate: dropout rate\n    \n    Returns:\n    output: tensor after residual connection\n    \"\"\"\n    # Main path\n    shortcut = x\n    \n    # First layer\n    x = layers.Dense(units, kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    # Second layer\n    x = layers.Dense(units, kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Residual connection (if dimensions match)\n    if shortcut.shape[-1] == units:\n        x = layers.Add()([shortcut, x])\n    else:\n        # Project shortcut to match dimensions\n        shortcut = layers.Dense(units, kernel_initializer='he_normal')(shortcut)\n        x = layers.Add()([shortcut, x])\n    \n    x = layers.Activation('relu')(x)\n    return x\n\ndef create_residual_network(input_dim, num_classes):\n    \"\"\"\n    Create a ResNet-inspired deep network\n    \n    Parameters:\n    input_dim: number of input features\n    num_classes: number of output classes\n    \n    Returns:\n    model: Keras model with residual connections\n    \"\"\"\n    inputs = layers.Input(shape=(input_dim,))\n    \n    # Initial dense layer\n    x = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(inputs)\n    \n    # Residual blocks\n    x = create_residual_block(x, 256, 0.2)\n    x = create_residual_block(x, 256, 0.2)\n    x = create_residual_block(x, 128, 0.2)\n    x = create_residual_block(x, 128, 0.2)\n    x = create_residual_block(x, 64, 0.1)\n    \n    # Global average pooling and output\n    x = layers.GlobalAveragePooling1D()(tf.expand_dims(x, axis=1))\n    outputs = layers.Dense(num_classes, activation='softmax' if num_classes > 1 else 'sigmoid')(x)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs, name='Residual_Deep_Network')\n    return model\n\n# Create and compare different modern architectures\nprint(\"Creating Modern Deep Network Architectures:\")\nprint(\"=\" * 60)\n\ninput_dim = 784  # MNIST-like\nnum_classes = 10\n\narchitecture_types = ['standard', 'dropout', 'batch_norm', 'advanced']\n\nmodels = {}\nfor arch_type in architecture_types:\n    model = create_modern_deep_network(input_dim, num_classes, arch_type)\n    models[arch_type] = model\n    \n    print(f\"\\n{arch_type.upper()} Architecture:\")\n    print(f\"  Total parameters: {model.count_params():,}\")\n    print(f\"  Number of layers: {len(model.layers)}\")\n\n# Create residual network\nprint(f\"\\nRESIDUAL Architecture:\")\nresidual_model = create_residual_network(input_dim, num_classes)\nmodels['residual'] = residual_model\nprint(f\"  Total parameters: {residual_model.count_params():,}\")\nprint(f\"  Number of layers: {len(residual_model.layers)}\")\n\n# Show detailed summary for advanced model\nprint(f\"\\nDetailed Architecture - ADVANCED Model:\")\nprint(\"-\" * 40)\nmodels['advanced'].summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parameter Inspection and Analysis\n",
    "\n",
    "Let's analyze the initialized parameters and understand their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_parameters(network, layer_to_analyze=1):\n",
    "    \"\"\"\n",
    "    Analyze and visualize network parameters\n",
    "    \n",
    "    Parameters:\n",
    "    network: DeepNeuralNetwork instance\n",
    "    layer_to_analyze: which layer to analyze in detail\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing parameters for layer {layer_to_analyze}:\")\n",
    "    \n",
    "    W_key = f'W{layer_to_analyze}'\n",
    "    b_key = f'b{layer_to_analyze}'\n",
    "    \n",
    "    weights = network.parameters[W_key]\n",
    "    biases = network.parameters[b_key]\n",
    "    \n",
    "    print(f\"\\nWeight matrix {W_key}:\")\n",
    "    print(f\"  Shape: {weights.shape}\")\n",
    "    print(f\"  Mean: {weights.mean():.6f}\")\n",
    "    print(f\"  Std: {weights.std():.6f}\")\n",
    "    print(f\"  Min: {weights.min():.6f}\")\n",
    "    print(f\"  Max: {weights.max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nBias vector {b_key}:\")\n",
    "    print(f\"  Shape: {biases.shape}\")\n",
    "    print(f\"  Mean: {biases.mean():.6f}\")\n",
    "    print(f\"  Std: {biases.std():.6f}\")\n",
    "    \n",
    "    # Visualize weight distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Weight histogram\n",
    "    axes[0].hist(weights.ravel(), bins=50, alpha=0.7, color='blue', density=True)\n",
    "    axes[0].set_title(f'Weight Distribution - Layer {layer_to_analyze}')\n",
    "    axes[0].set_xlabel('Weight Value')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axvline(weights.mean(), color='red', linestyle='--', label=f'Mean: {weights.mean():.4f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Weight matrix heatmap\n",
    "    im = axes[1].imshow(weights, cmap='RdBu', aspect='auto')\n",
    "    axes[1].set_title(f'Weight Matrix Heatmap - Layer {layer_to_analyze}')\n",
    "    axes[1].set_xlabel('Input Neuron')\n",
    "    axes[1].set_ylabel('Output Neuron')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze parameters for the small network\n",
    "print(\"Parameter Analysis for Small Deep Network:\")\n",
    "analyze_network_parameters(small_network, layer_to_analyze=1)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Compare different initialization methods\n",
    "print(\"Comparing Initialization Methods:\")\n",
    "\n",
    "initialization_methods = ['random_normal', 'xavier_uniform', 'xavier_normal', 'he_uniform', 'he_normal']\n",
    "test_architecture = [100, 50, 25, 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, init_method in enumerate(initialization_methods):\n",
    "    test_network = DeepNeuralNetwork(test_architecture, initialization=init_method, random_seed=42)\n",
    "    \n",
    "    # Get first layer weights\n",
    "    weights = test_network.parameters['W1']\n",
    "    \n",
    "    axes[idx].hist(weights.ravel(), bins=40, alpha=0.7, density=True)\n",
    "    axes[idx].set_title(f'{init_method.replace(\"_\", \" \").title()}\\nMean: {weights.mean():.4f}, Std: {weights.std():.4f}')\n",
    "    axes[idx].set_xlabel('Weight Value')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison of Weight Initialization Methods', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Architecture Design Best Practices\n",
    "\n",
    "Let's explore best practices for designing deep network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitectureDesigner:\n",
    "    \"\"\"\n",
    "    Helper class for designing neural network architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def pyramid_architecture(input_size, output_size, num_hidden_layers, reduction_factor=2):\n",
    "        \"\"\"\n",
    "        Create a pyramid-style architecture that gradually reduces layer size\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        reduction_factor: factor by which each layer is reduced\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if num_hidden_layers == 0:\n",
    "            return [input_size, output_size]\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        \n",
    "        # Calculate intermediate sizes\n",
    "        current_size = input_size\n",
    "        for i in range(num_hidden_layers):\n",
    "            current_size = max(output_size, int(current_size / reduction_factor))\n",
    "            architecture.append(current_size)\n",
    "        \n",
    "        # Ensure last layer connects to output\n",
    "        if architecture[-1] != output_size:\n",
    "            architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    @staticmethod\n",
    "    def diamond_architecture(input_size, output_size, num_hidden_layers, expansion_factor=2):\n",
    "        \"\"\"\n",
    "        Create a diamond-style architecture that expands then contracts\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        expansion_factor: factor by which middle layers expand\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if num_hidden_layers == 0:\n",
    "            return [input_size, output_size]\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        \n",
    "        # Expansion phase\n",
    "        mid_point = num_hidden_layers // 2\n",
    "        max_size = input_size * expansion_factor\n",
    "        \n",
    "        for i in range(mid_point):\n",
    "            size = int(input_size + (max_size - input_size) * (i + 1) / mid_point)\n",
    "            architecture.append(size)\n",
    "        \n",
    "        # Contraction phase\n",
    "        remaining_layers = num_hidden_layers - mid_point\n",
    "        current_size = architecture[-1] if mid_point > 0 else input_size\n",
    "        \n",
    "        for i in range(remaining_layers):\n",
    "            size = int(current_size - (current_size - output_size) * (i + 1) / remaining_layers)\n",
    "            size = max(output_size, size)\n",
    "            architecture.append(size)\n",
    "        \n",
    "        # Ensure output layer\n",
    "        if architecture[-1] != output_size:\n",
    "            architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniform_architecture(input_size, output_size, num_hidden_layers, hidden_size=None):\n",
    "        \"\"\"\n",
    "        Create a uniform architecture with same-sized hidden layers\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        hidden_size: size of hidden layers (if None, use input_size)\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if hidden_size is None:\n",
    "            hidden_size = input_size\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        architecture.extend([hidden_size] * num_hidden_layers)\n",
    "        architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "\n",
    "# Test different architecture designs\n",
    "print(\"Architecture Design Examples:\")\n",
    "print()\n",
    "\n",
    "input_size, output_size = 784, 10  # MNIST-like problem\n",
    "num_hidden = 4\n",
    "\n",
    "architectures = {\n",
    "    'Pyramid (2x reduction)': ArchitectureDesigner.pyramid_architecture(input_size, output_size, num_hidden, 2),\n",
    "    'Pyramid (3x reduction)': ArchitectureDesigner.pyramid_architecture(input_size, output_size, num_hidden, 3),\n",
    "    'Diamond (2x expansion)': ArchitectureDesigner.diamond_architecture(input_size, output_size, num_hidden, 2),\n",
    "    'Uniform (512 units)': ArchitectureDesigner.uniform_architecture(input_size, output_size, num_hidden, 512),\n",
    "    'Uniform (256 units)': ArchitectureDesigner.uniform_architecture(input_size, output_size, num_hidden, 256)\n",
    "}\n",
    "\n",
    "for name, arch in architectures.items():\n",
    "    print(f\"{name:20}: {arch}\")\n",
    "    network = DeepNeuralNetwork(arch, initialization='he_normal')\n",
    "    print(f\"{'':20}  Total parameters: {network._count_parameters():,}\")\n",
    "    print()\n",
    "\n",
    "# Visualize different architectures\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, arch) in enumerate(architectures.items()):\n",
    "    # Simplified visualization for subplots\n",
    "    max_neurons = max(arch)\n",
    "    layer_positions = np.linspace(0, len(arch)-1, len(arch))\n",
    "    \n",
    "    for layer_idx, num_neurons in enumerate(arch):\n",
    "        # Normalize neuron positions\n",
    "        neuron_positions = np.linspace(\n",
    "            (max_neurons - num_neurons) / 2, \n",
    "            (max_neurons - num_neurons) / 2 + num_neurons - 1, \n",
    "            min(num_neurons, 10)  # Limit visualization to 10 neurons max\n",
    "        )\n",
    "        \n",
    "        # Draw neurons\n",
    "        for neuron_pos in neuron_positions:\n",
    "            circle = plt.Circle((layer_positions[layer_idx], neuron_pos), 0.05, \n",
    "                              color='lightblue', ec='darkblue', linewidth=1)\n",
    "            axes[idx].add_patch(circle)\n",
    "    \n",
    "    axes[idx].set_xlim(-0.5, len(arch) - 0.5)\n",
    "    axes[idx].set_ylim(-1, max_neurons)\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'{name}\\n{arch}', fontsize=10)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Different Neural Network Architectures', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Architecture Validation and Testing\n",
    "\n",
    "Let's create a validation framework to test our architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_architecture(architecture, max_params=1000000, min_layers=2):\n",
    "    \"\"\"\n",
    "    Validate neural network architecture\n",
    "    \n",
    "    Parameters:\n",
    "    architecture: list of layer sizes\n",
    "    max_params: maximum allowed parameters\n",
    "    min_layers: minimum number of layers\n",
    "    \n",
    "    Returns:\n",
    "    validation_result: dictionary with validation results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'valid': True,\n",
    "        'warnings': [],\n",
    "        'errors': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check minimum layers\n",
    "    if len(architecture) < min_layers:\n",
    "        result['errors'].append(f\"Architecture must have at least {min_layers} layers\")\n",
    "        result['valid'] = False\n",
    "    \n",
    "    # Check for zero or negative layer sizes\n",
    "    if any(size <= 0 for size in architecture):\n",
    "        result['errors'].append(\"All layer sizes must be positive\")\n",
    "        result['valid'] = False\n",
    "    \n",
    "    # Estimate parameter count\n",
    "    total_params = 0\n",
    "    for i in range(1, len(architecture)):\n",
    "        # Weights: current_layer_size × previous_layer_size\n",
    "        # Biases: current_layer_size\n",
    "        total_params += architecture[i] * architecture[i-1] + architecture[i]\n",
    "    \n",
    "    # Check parameter count\n",
    "    if total_params > max_params:\n",
    "        result['warnings'].append(f\"High parameter count: {total_params:,} (max recommended: {max_params:,})\")\n",
    "    \n",
    "    # Check for very large layers\n",
    "    max_layer_size = max(architecture)\n",
    "    if max_layer_size > 2048:\n",
    "        result['warnings'].append(f\"Very large layer detected: {max_layer_size} units\")\n",
    "    \n",
    "    # Check for dramatic size changes\n",
    "    for i in range(1, len(architecture)):\n",
    "        ratio = architecture[i-1] / architecture[i] if architecture[i] > 0 else float('inf')\n",
    "        if ratio > 10:\n",
    "            result['warnings'].append(f\"Large reduction from layer {i-1} to {i}: {architecture[i-1]} → {architecture[i]}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if len(architecture) > 6:\n",
    "        result['recommendations'].append(\"Consider using techniques like batch normalization for very deep networks\")\n",
    "    \n",
    "    if architecture[0] > 1000:\n",
    "        result['recommendations'].append(\"Consider dimensionality reduction for high-dimensional input\")\n",
    "    \n",
    "    result['total_parameters'] = total_params\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_validation_results(architecture, validation_result):\n",
    "    \"\"\"\n",
    "    Print validation results in a formatted way\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidation Results for Architecture: {architecture}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if validation_result['valid']:\n",
    "        print(\"✅ Architecture is VALID\")\n",
    "    else:\n",
    "        print(\"❌ Architecture is INVALID\")\n",
    "    \n",
    "    print(f\"Total Parameters: {validation_result['total_parameters']:,}\")\n",
    "    \n",
    "    if validation_result['errors']:\n",
    "        print(\"\\n🚨 ERRORS:\")\n",
    "        for error in validation_result['errors']:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        print(\"\\n⚠️  WARNINGS:\")\n",
    "        for warning in validation_result['warnings']:\n",
    "            print(f\"  - {warning}\")\n",
    "    \n",
    "    if validation_result['recommendations']:\n",
    "        print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "        for rec in validation_result['recommendations']:\n",
    "            print(f\"  - {rec}\")\n",
    "\n",
    "# Test validation on different architectures\n",
    "print(\"Architecture Validation Tests:\")\n",
    "\n",
    "test_architectures = [\n",
    "    [784, 256, 128, 64, 10],  # Good architecture\n",
    "    [784, 2048, 1024, 512, 10],  # Large architecture\n",
    "    [1000, 10],  # Too simple\n",
    "    [100, 1000, 5],  # Large jump down\n",
    "    [784, 512, 256, 128, 64, 32, 16, 8, 4, 1],  # Very deep\n",
    "    [784, 0, 10],  # Invalid (zero neurons)\n",
    "]\n",
    "\n",
    "for arch in test_architectures:\n",
    "    validation = validate_architecture(arch)\n",
    "    print_validation_results(arch, validation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Progress Tracking and Key Concepts\n",
    "\n",
    "Let's summarize what we've learned and check our progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Progress Tracking Checklist\nprogress_checklist = {\n    \"Understanding TensorFlow/Keras architecture building\": True,\n    \"Using Sequential and Functional APIs\": True,\n    \"Implementing weight initialization strategies\": True,\n    \"Creating modern deep networks with advanced layers\": True,\n    \"Understanding Dropout and BatchNormalization\": True,\n    \"Implementing residual connections\": True,\n    \"Comparing different architecture approaches\": True,\n    \"Understanding TensorFlow model summary and visualization\": True\n}\n\nprint(\"Progress Tracking Checklist:\")\nprint(\"=\" * 50)\nfor item, completed in progress_checklist.items():\n    status = \"✅\" if completed else \"❌\"\n    print(f\"{status} {item}\")\n\ncompleted_items = sum(progress_checklist.values())\ntotal_items = len(progress_checklist)\nprint(f\"\\nProgress: {completed_items}/{total_items} ({completed_items/total_items*100:.1f}%) Complete\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"KEY CONCEPTS SUMMARY\")\nprint(\"=\" * 60)\n\nkey_concepts = {\n    \"TensorFlow/Keras\": \"High-level deep learning framework with easy-to-use APIs\",\n    \"Sequential API\": \"Simple linear stack of layers for straightforward architectures\",\n    \"Functional API\": \"More flexible approach for complex architectures and multiple inputs/outputs\",\n    \"Weight Initialization\": \"Built-in strategies (He, Xavier/Glorot) for proper training\",\n    \"Dropout Layers\": \"Regularization technique to prevent overfitting during training\",\n    \"Batch Normalization\": \"Normalizes inputs to each layer, accelerates training\",\n    \"Residual Connections\": \"Skip connections that help train very deep networks\",\n    \"Model Summary\": \"TensorFlow's built-in tools for understanding model architecture\",\n    \"Modern Practices\": \"Combining techniques for robust, trainable deep networks\"\n}\n\nfor concept, description in key_concepts.items():\n    print(f\"\\n{concept}:\")\n    print(f\"  {description}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TENSORFLOW vs NUMPY COMPARISON\")\nprint(\"=\" * 60)\nprint(\"Advantages of TensorFlow/Keras:\")\nprint(\"✅ Automatic differentiation (no manual backpropagation)\")\nprint(\"✅ GPU acceleration support\")\nprint(\"✅ Built-in optimizers and loss functions\")\nprint(\"✅ Extensive library of layer types\")\nprint(\"✅ Model saving and loading capabilities\")\nprint(\"✅ Production deployment tools\")\nprint(\"✅ Large community and ecosystem\")\nprint()\nprint(\"NumPy Implementation Benefits:\")\nprint(\"📚 Better understanding of underlying mathematics\")\nprint(\"📚 Full control over every aspect of training\")\nprint(\"📚 Educational value for learning concepts\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"NEXT STEPS\")\nprint(\"=\" * 60)\nprint(\"1. Implement training loops with TensorFlow\")\nprint(\"2. Add advanced regularization techniques\")\nprint(\"3. Explore different optimizers (Adam, RMSprop, etc.)\")\nprint(\"4. Work with real datasets using tf.data\")\nprint(\"5. Learn about model compilation and metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Cleanup Instructions\n",
    "\n",
    "### Windows Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in the command prompt to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close command prompt\n",
    "\n",
    "### Mac Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in terminal to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close terminal\n",
    "\n",
    "### Save Your Work:\n",
    "- Your notebook is automatically saved\n",
    "- Consider saving a copy with your name: `lab_4_1_[your_name].ipynb`\n",
    "- Export as HTML for offline viewing: File → Download as → HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Import errors (NumPy, Matplotlib)**\n",
    "- **Solution**: Install missing packages: `pip install numpy matplotlib`\n",
    "- **Windows**: Use `conda install numpy matplotlib` if using Anaconda\n",
    "- **Mac**: Same as Windows, or use `pip3` instead of `pip`\n",
    "\n",
    "**Issue 2: Memory errors with large networks**\n",
    "- **Solution**: Reduce network size or batch size\n",
    "- **Alternative**: Use different initialization with smaller values\n",
    "\n",
    "**Issue 3: Slow execution**\n",
    "- **Solution**: Reduce network complexity or visualization details\n",
    "- **Check**: Available RAM and close other applications\n",
    "\n",
    "**Issue 4: Visualization not showing**\n",
    "- **Solution**: Run `%matplotlib inline` in a cell\n",
    "- **Alternative**: Try `plt.show()` after each plot\n",
    "\n",
    "**Issue 5: Random seed not working**\n",
    "- **Solution**: Run the seed setting cell before network creation\n",
    "- **Check**: Ensure consistent seed values across experiments\n",
    "\n",
    "### Getting Help:\n",
    "- Check the error message carefully\n",
    "- Try restarting the kernel: Kernel → Restart\n",
    "- Ask instructor or teaching assistant\n",
    "- Refer to NumPy documentation: https://numpy.org/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}