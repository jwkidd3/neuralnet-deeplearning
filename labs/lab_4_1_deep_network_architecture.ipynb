{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1: Deep Network Architecture Implementation\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Design and implement deep neural network architectures\n",
    "- Understand the structure of multi-layer neural networks\n",
    "- Initialize weights and biases for deep networks\n",
    "- Implement a flexible neural network class for various architectures\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of neural networks\n",
    "- Knowledge of Python and NumPy\n",
    "- Understanding of matrix operations\n",
    "\n",
    "## Key Concepts\n",
    "- **Deep Neural Networks**: Networks with multiple hidden layers\n",
    "- **Weight Initialization**: Proper initialization strategies for deep networks\n",
    "- **Network Architecture**: Layer sizes and connectivity patterns\n",
    "- **Parameter Management**: Organizing weights and biases across layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Deep Network Architecture\n",
    "\n",
    "A deep neural network consists of:\n",
    "- **Input layer**: Receives the input data\n",
    "- **Hidden layers**: Multiple layers that process information\n",
    "- **Output layer**: Produces the final predictions\n",
    "\n",
    "Let's visualize different network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_architecture(layer_sizes, title=\"Neural Network Architecture\"):\n",
    "    \"\"\"\n",
    "    Visualize neural network architecture\n",
    "    \n",
    "    Parameters:\n",
    "    layer_sizes: list of integers representing number of neurons in each layer\n",
    "    title: title for the plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate positions\n",
    "    max_neurons = max(layer_sizes)\n",
    "    layer_positions = np.linspace(0, len(layer_sizes)-1, len(layer_sizes))\n",
    "    \n",
    "    # Draw neurons\n",
    "    for layer_idx, num_neurons in enumerate(layer_sizes):\n",
    "        # Center neurons vertically\n",
    "        neuron_positions = np.linspace(\n",
    "            (max_neurons - num_neurons) / 2, \n",
    "            (max_neurons - num_neurons) / 2 + num_neurons - 1, \n",
    "            num_neurons\n",
    "        )\n",
    "        \n",
    "        # Draw neurons as circles\n",
    "        for neuron_pos in neuron_positions:\n",
    "            circle = plt.Circle((layer_positions[layer_idx], neuron_pos), 0.1, \n",
    "                              color='lightblue', ec='darkblue', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Draw connections to next layer\n",
    "        if layer_idx < len(layer_sizes) - 1:\n",
    "            next_neurons = layer_sizes[layer_idx + 1]\n",
    "            next_positions = np.linspace(\n",
    "                (max_neurons - next_neurons) / 2, \n",
    "                (max_neurons - next_neurons) / 2 + next_neurons - 1, \n",
    "                next_neurons\n",
    "            )\n",
    "            \n",
    "            # Draw connections\n",
    "            for curr_pos in neuron_positions:\n",
    "                for next_pos in next_positions:\n",
    "                    ax.plot([layer_positions[layer_idx], layer_positions[layer_idx + 1]], \n",
    "                           [curr_pos, next_pos], 'gray', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # Add layer labels\n",
    "    layer_names = ['Input'] + [f'Hidden {i}' for i in range(1, len(layer_sizes)-1)] + ['Output']\n",
    "    for i, (pos, name, size) in enumerate(zip(layer_positions, layer_names, layer_sizes)):\n",
    "        ax.text(pos, max_neurons + 0.5, f'{name}\\n({size} units)', \n",
    "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-0.5, len(layer_sizes) - 0.5)\n",
    "    ax.set_ylim(-1, max_neurons + 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize different architectures\n",
    "print(\"Example Network Architectures:\")\n",
    "print(\"\\n1. Shallow Network:\")\n",
    "visualize_network_architecture([4, 3, 1], \"Shallow Network (4-3-1)\")\n",
    "\n",
    "print(\"\\n2. Deep Network:\")\n",
    "visualize_network_architecture([4, 7, 5, 3, 1], \"Deep Network (4-7-5-3-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Weight Initialization Strategies\n",
    "\n",
    "Proper weight initialization is crucial for deep networks to train effectively. Let's implement different initialization methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    \"\"\"\n",
    "    Collection of weight initialization methods for deep networks\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        \"\"\"Initialize weights to zeros (not recommended for hidden layers)\"\"\"\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_normal(shape, mean=0, std=0.01):\n",
    "        \"\"\"Initialize weights from normal distribution\"\"\"\n",
    "        return np.random.normal(mean, std, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_uniform(shape, low=-0.1, high=0.1):\n",
    "        \"\"\"Initialize weights from uniform distribution\"\"\"\n",
    "        return np.random.uniform(low, high, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_uniform(shape):\n",
    "        \"\"\"Xavier/Glorot uniform initialization\"\"\"\n",
    "        fan_in, fan_out = shape[1], shape[0]\n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_normal(shape):\n",
    "        \"\"\"Xavier/Glorot normal initialization\"\"\"\n",
    "        fan_in, fan_out = shape[1], shape[0]\n",
    "        std = np.sqrt(2 / (fan_in + fan_out))\n",
    "        return np.random.normal(0, std, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_uniform(shape):\n",
    "        \"\"\"He uniform initialization (good for ReLU)\"\"\"\n",
    "        fan_in = shape[1]\n",
    "        limit = np.sqrt(6 / fan_in)\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_normal(shape):\n",
    "        \"\"\"He normal initialization (good for ReLU)\"\"\"\n",
    "        fan_in = shape[1]\n",
    "        std = np.sqrt(2 / fan_in)\n",
    "        return np.random.normal(0, std, shape)\n",
    "\n",
    "# Test different initialization methods\n",
    "print(\"Weight Initialization Methods:\")\n",
    "test_shape = (10, 5)  # 10 output neurons, 5 input neurons\n",
    "\n",
    "initializers = {\n",
    "    'Random Normal': WeightInitializer.random_normal,\n",
    "    'Xavier Uniform': WeightInitializer.xavier_uniform,\n",
    "    'Xavier Normal': WeightInitializer.xavier_normal,\n",
    "    'He Uniform': WeightInitializer.he_uniform,\n",
    "    'He Normal': WeightInitializer.he_normal\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, init_func) in enumerate(initializers.items()):\n",
    "    weights = init_func(test_shape)\n",
    "    \n",
    "    axes[idx].hist(weights.ravel(), bins=30, alpha=0.7, density=True)\n",
    "    axes[idx].set_title(f'{name}\\nMean: {weights.mean():.4f}, Std: {weights.std():.4f}')\n",
    "    axes[idx].set_xlabel('Weight Value')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Weight Initialization Distributions', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"- Xavier initialization works well with tanh and sigmoid activations\")\n",
    "print(\"- He initialization works well with ReLU and its variants\")\n",
    "print(\"- Proper initialization prevents vanishing/exploding gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deep Neural Network Class Implementation\n",
    "\n",
    "Now let's implement a flexible deep neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Deep Neural Network implementation with configurable architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, initialization='he_normal', random_seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the deep neural network\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes: list of integers, number of units in each layer\n",
    "        initialization: string, weight initialization method\n",
    "        random_seed: int, random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "            \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.initialization = initialization\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.parameters = self._initialize_parameters()\n",
    "        \n",
    "        # Store gradients\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Store cache for backpropagation\n",
    "        self.cache = {}\n",
    "        \n",
    "        print(f\"Deep Neural Network initialized with architecture: {layer_sizes}\")\n",
    "        print(f\"Total parameters: {self._count_parameters()}\")\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for all layers\n",
    "        \n",
    "        Returns:\n",
    "        parameters: dictionary containing weights and biases\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        # Get initialization function\n",
    "        init_functions = {\n",
    "            'random_normal': WeightInitializer.random_normal,\n",
    "            'xavier_uniform': WeightInitializer.xavier_uniform,\n",
    "            'xavier_normal': WeightInitializer.xavier_normal,\n",
    "            'he_uniform': WeightInitializer.he_uniform,\n",
    "            'he_normal': WeightInitializer.he_normal\n",
    "        }\n",
    "        \n",
    "        init_func = init_functions.get(self.initialization, WeightInitializer.he_normal)\n",
    "        \n",
    "        for layer in range(1, self.num_layers):\n",
    "            # Weight matrix shape: (current_layer_size, previous_layer_size)\n",
    "            weight_shape = (self.layer_sizes[layer], self.layer_sizes[layer-1])\n",
    "            \n",
    "            # Initialize weights\n",
    "            parameters[f'W{layer}'] = init_func(weight_shape)\n",
    "            \n",
    "            # Initialize biases to zero\n",
    "            parameters[f'b{layer}'] = np.zeros((self.layer_sizes[layer], 1))\n",
    "            \n",
    "            print(f\"Layer {layer}: W{layer} shape = {weight_shape}, b{layer} shape = {parameters[f'b{layer}'].shape}\")\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count total number of parameters in the network\n",
    "        \n",
    "        Returns:\n",
    "        total_params: int, total number of parameters\n",
    "        \"\"\"\n",
    "        total_params = 0\n",
    "        for key, param in self.parameters.items():\n",
    "            total_params += param.size\n",
    "        return total_params\n",
    "    \n",
    "    def get_parameter_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary of network parameters\n",
    "        \n",
    "        Returns:\n",
    "        summary: dictionary with parameter information\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'architecture': self.layer_sizes,\n",
    "            'num_layers': self.num_layers,\n",
    "            'total_parameters': self._count_parameters(),\n",
    "            'initialization': self.initialization,\n",
    "            'parameter_details': {}\n",
    "        }\n",
    "        \n",
    "        for layer in range(1, self.num_layers):\n",
    "            W_key, b_key = f'W{layer}', f'b{layer}'\n",
    "            summary['parameter_details'][f'Layer_{layer}'] = {\n",
    "                'weights_shape': self.parameters[W_key].shape,\n",
    "                'biases_shape': self.parameters[b_key].shape,\n",
    "                'weights_params': self.parameters[W_key].size,\n",
    "                'biases_params': self.parameters[b_key].size\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_architecture(self):\n",
    "        \"\"\"\n",
    "        Print detailed architecture information\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"DEEP NEURAL NETWORK ARCHITECTURE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Architecture: {self.layer_sizes}\")\n",
    "        print(f\"Number of layers: {self.num_layers}\")\n",
    "        print(f\"Initialization method: {self.initialization}\")\n",
    "        print(f\"Total parameters: {self._count_parameters():,}\")\n",
    "        print()\n",
    "        \n",
    "        layer_names = ['Input'] + [f'Hidden {i}' for i in range(1, self.num_layers-1)] + ['Output']\n",
    "        \n",
    "        for i, (name, size) in enumerate(zip(layer_names, self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                print(f\"Layer {i} ({name:>8}): {size:>4} units\")\n",
    "            else:\n",
    "                W_key, b_key = f'W{i}', f'b{i}'\n",
    "                W_params = self.parameters[W_key].size\n",
    "                b_params = self.parameters[b_key].size\n",
    "                total_params = W_params + b_params\n",
    "                print(f\"Layer {i} ({name:>8}): {size:>4} units, {total_params:>6} parameters\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Test the Deep Neural Network class\n",
    "print(\"Testing Deep Neural Network Implementation:\")\n",
    "print()\n",
    "\n",
    "# Example 1: Small deep network\n",
    "print(\"Example 1: Small Deep Network\")\n",
    "small_network = DeepNeuralNetwork([4, 8, 6, 3, 1], initialization='he_normal', random_seed=42)\n",
    "small_network.print_architecture()\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Example 2: Larger deep network\n",
    "print(\"Example 2: Larger Deep Network\")\n",
    "large_network = DeepNeuralNetwork([784, 256, 128, 64, 32, 10], initialization='xavier_normal', random_seed=42)\n",
    "large_network.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parameter Inspection and Analysis\n",
    "\n",
    "Let's analyze the initialized parameters and understand their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_parameters(network, layer_to_analyze=1):\n",
    "    \"\"\"\n",
    "    Analyze and visualize network parameters\n",
    "    \n",
    "    Parameters:\n",
    "    network: DeepNeuralNetwork instance\n",
    "    layer_to_analyze: which layer to analyze in detail\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing parameters for layer {layer_to_analyze}:\")\n",
    "    \n",
    "    W_key = f'W{layer_to_analyze}'\n",
    "    b_key = f'b{layer_to_analyze}'\n",
    "    \n",
    "    weights = network.parameters[W_key]\n",
    "    biases = network.parameters[b_key]\n",
    "    \n",
    "    print(f\"\\nWeight matrix {W_key}:\")\n",
    "    print(f\"  Shape: {weights.shape}\")\n",
    "    print(f\"  Mean: {weights.mean():.6f}\")\n",
    "    print(f\"  Std: {weights.std():.6f}\")\n",
    "    print(f\"  Min: {weights.min():.6f}\")\n",
    "    print(f\"  Max: {weights.max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nBias vector {b_key}:\")\n",
    "    print(f\"  Shape: {biases.shape}\")\n",
    "    print(f\"  Mean: {biases.mean():.6f}\")\n",
    "    print(f\"  Std: {biases.std():.6f}\")\n",
    "    \n",
    "    # Visualize weight distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Weight histogram\n",
    "    axes[0].hist(weights.ravel(), bins=50, alpha=0.7, color='blue', density=True)\n",
    "    axes[0].set_title(f'Weight Distribution - Layer {layer_to_analyze}')\n",
    "    axes[0].set_xlabel('Weight Value')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axvline(weights.mean(), color='red', linestyle='--', label=f'Mean: {weights.mean():.4f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Weight matrix heatmap\n",
    "    im = axes[1].imshow(weights, cmap='RdBu', aspect='auto')\n",
    "    axes[1].set_title(f'Weight Matrix Heatmap - Layer {layer_to_analyze}')\n",
    "    axes[1].set_xlabel('Input Neuron')\n",
    "    axes[1].set_ylabel('Output Neuron')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze parameters for the small network\n",
    "print(\"Parameter Analysis for Small Deep Network:\")\n",
    "analyze_network_parameters(small_network, layer_to_analyze=1)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Compare different initialization methods\n",
    "print(\"Comparing Initialization Methods:\")\n",
    "\n",
    "initialization_methods = ['random_normal', 'xavier_uniform', 'xavier_normal', 'he_uniform', 'he_normal']\n",
    "test_architecture = [100, 50, 25, 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, init_method in enumerate(initialization_methods):\n",
    "    test_network = DeepNeuralNetwork(test_architecture, initialization=init_method, random_seed=42)\n",
    "    \n",
    "    # Get first layer weights\n",
    "    weights = test_network.parameters['W1']\n",
    "    \n",
    "    axes[idx].hist(weights.ravel(), bins=40, alpha=0.7, density=True)\n",
    "    axes[idx].set_title(f'{init_method.replace(\"_\", \" \").title()}\\nMean: {weights.mean():.4f}, Std: {weights.std():.4f}')\n",
    "    axes[idx].set_xlabel('Weight Value')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison of Weight Initialization Methods', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Architecture Design Best Practices\n",
    "\n",
    "Let's explore best practices for designing deep network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitectureDesigner:\n",
    "    \"\"\"\n",
    "    Helper class for designing neural network architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def pyramid_architecture(input_size, output_size, num_hidden_layers, reduction_factor=2):\n",
    "        \"\"\"\n",
    "        Create a pyramid-style architecture that gradually reduces layer size\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        reduction_factor: factor by which each layer is reduced\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if num_hidden_layers == 0:\n",
    "            return [input_size, output_size]\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        \n",
    "        # Calculate intermediate sizes\n",
    "        current_size = input_size\n",
    "        for i in range(num_hidden_layers):\n",
    "            current_size = max(output_size, int(current_size / reduction_factor))\n",
    "            architecture.append(current_size)\n",
    "        \n",
    "        # Ensure last layer connects to output\n",
    "        if architecture[-1] != output_size:\n",
    "            architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    @staticmethod\n",
    "    def diamond_architecture(input_size, output_size, num_hidden_layers, expansion_factor=2):\n",
    "        \"\"\"\n",
    "        Create a diamond-style architecture that expands then contracts\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        expansion_factor: factor by which middle layers expand\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if num_hidden_layers == 0:\n",
    "            return [input_size, output_size]\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        \n",
    "        # Expansion phase\n",
    "        mid_point = num_hidden_layers // 2\n",
    "        max_size = input_size * expansion_factor\n",
    "        \n",
    "        for i in range(mid_point):\n",
    "            size = int(input_size + (max_size - input_size) * (i + 1) / mid_point)\n",
    "            architecture.append(size)\n",
    "        \n",
    "        # Contraction phase\n",
    "        remaining_layers = num_hidden_layers - mid_point\n",
    "        current_size = architecture[-1] if mid_point > 0 else input_size\n",
    "        \n",
    "        for i in range(remaining_layers):\n",
    "            size = int(current_size - (current_size - output_size) * (i + 1) / remaining_layers)\n",
    "            size = max(output_size, size)\n",
    "            architecture.append(size)\n",
    "        \n",
    "        # Ensure output layer\n",
    "        if architecture[-1] != output_size:\n",
    "            architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniform_architecture(input_size, output_size, num_hidden_layers, hidden_size=None):\n",
    "        \"\"\"\n",
    "        Create a uniform architecture with same-sized hidden layers\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: number of input features\n",
    "        output_size: number of output units\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        hidden_size: size of hidden layers (if None, use input_size)\n",
    "        \n",
    "        Returns:\n",
    "        architecture: list of layer sizes\n",
    "        \"\"\"\n",
    "        if hidden_size is None:\n",
    "            hidden_size = input_size\n",
    "        \n",
    "        architecture = [input_size]\n",
    "        architecture.extend([hidden_size] * num_hidden_layers)\n",
    "        architecture.append(output_size)\n",
    "        \n",
    "        return architecture\n",
    "\n",
    "# Test different architecture designs\n",
    "print(\"Architecture Design Examples:\")\n",
    "print()\n",
    "\n",
    "input_size, output_size = 784, 10  # MNIST-like problem\n",
    "num_hidden = 4\n",
    "\n",
    "architectures = {\n",
    "    'Pyramid (2x reduction)': ArchitectureDesigner.pyramid_architecture(input_size, output_size, num_hidden, 2),\n",
    "    'Pyramid (3x reduction)': ArchitectureDesigner.pyramid_architecture(input_size, output_size, num_hidden, 3),\n",
    "    'Diamond (2x expansion)': ArchitectureDesigner.diamond_architecture(input_size, output_size, num_hidden, 2),\n",
    "    'Uniform (512 units)': ArchitectureDesigner.uniform_architecture(input_size, output_size, num_hidden, 512),\n",
    "    'Uniform (256 units)': ArchitectureDesigner.uniform_architecture(input_size, output_size, num_hidden, 256)\n",
    "}\n",
    "\n",
    "for name, arch in architectures.items():\n",
    "    print(f\"{name:20}: {arch}\")\n",
    "    network = DeepNeuralNetwork(arch, initialization='he_normal')\n",
    "    print(f\"{'':20}  Total parameters: {network._count_parameters():,}\")\n",
    "    print()\n",
    "\n",
    "# Visualize different architectures\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, arch) in enumerate(architectures.items()):\n",
    "    # Simplified visualization for subplots\n",
    "    max_neurons = max(arch)\n",
    "    layer_positions = np.linspace(0, len(arch)-1, len(arch))\n",
    "    \n",
    "    for layer_idx, num_neurons in enumerate(arch):\n",
    "        # Normalize neuron positions\n",
    "        neuron_positions = np.linspace(\n",
    "            (max_neurons - num_neurons) / 2, \n",
    "            (max_neurons - num_neurons) / 2 + num_neurons - 1, \n",
    "            min(num_neurons, 10)  # Limit visualization to 10 neurons max\n",
    "        )\n",
    "        \n",
    "        # Draw neurons\n",
    "        for neuron_pos in neuron_positions:\n",
    "            circle = plt.Circle((layer_positions[layer_idx], neuron_pos), 0.05, \n",
    "                              color='lightblue', ec='darkblue', linewidth=1)\n",
    "            axes[idx].add_patch(circle)\n",
    "    \n",
    "    axes[idx].set_xlim(-0.5, len(arch) - 0.5)\n",
    "    axes[idx].set_ylim(-1, max_neurons)\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'{name}\\n{arch}', fontsize=10)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Different Neural Network Architectures', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Architecture Validation and Testing\n",
    "\n",
    "Let's create a validation framework to test our architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_architecture(architecture, max_params=1000000, min_layers=2):\n",
    "    \"\"\"\n",
    "    Validate neural network architecture\n",
    "    \n",
    "    Parameters:\n",
    "    architecture: list of layer sizes\n",
    "    max_params: maximum allowed parameters\n",
    "    min_layers: minimum number of layers\n",
    "    \n",
    "    Returns:\n",
    "    validation_result: dictionary with validation results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'valid': True,\n",
    "        'warnings': [],\n",
    "        'errors': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check minimum layers\n",
    "    if len(architecture) < min_layers:\n",
    "        result['errors'].append(f\"Architecture must have at least {min_layers} layers\")\n",
    "        result['valid'] = False\n",
    "    \n",
    "    # Check for zero or negative layer sizes\n",
    "    if any(size <= 0 for size in architecture):\n",
    "        result['errors'].append(\"All layer sizes must be positive\")\n",
    "        result['valid'] = False\n",
    "    \n",
    "    # Estimate parameter count\n",
    "    total_params = 0\n",
    "    for i in range(1, len(architecture)):\n",
    "        # Weights: current_layer_size × previous_layer_size\n",
    "        # Biases: current_layer_size\n",
    "        total_params += architecture[i] * architecture[i-1] + architecture[i]\n",
    "    \n",
    "    # Check parameter count\n",
    "    if total_params > max_params:\n",
    "        result['warnings'].append(f\"High parameter count: {total_params:,} (max recommended: {max_params:,})\")\n",
    "    \n",
    "    # Check for very large layers\n",
    "    max_layer_size = max(architecture)\n",
    "    if max_layer_size > 2048:\n",
    "        result['warnings'].append(f\"Very large layer detected: {max_layer_size} units\")\n",
    "    \n",
    "    # Check for dramatic size changes\n",
    "    for i in range(1, len(architecture)):\n",
    "        ratio = architecture[i-1] / architecture[i] if architecture[i] > 0 else float('inf')\n",
    "        if ratio > 10:\n",
    "            result['warnings'].append(f\"Large reduction from layer {i-1} to {i}: {architecture[i-1]} → {architecture[i]}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if len(architecture) > 6:\n",
    "        result['recommendations'].append(\"Consider using techniques like batch normalization for very deep networks\")\n",
    "    \n",
    "    if architecture[0] > 1000:\n",
    "        result['recommendations'].append(\"Consider dimensionality reduction for high-dimensional input\")\n",
    "    \n",
    "    result['total_parameters'] = total_params\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_validation_results(architecture, validation_result):\n",
    "    \"\"\"\n",
    "    Print validation results in a formatted way\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidation Results for Architecture: {architecture}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if validation_result['valid']:\n",
    "        print(\"✅ Architecture is VALID\")\n",
    "    else:\n",
    "        print(\"❌ Architecture is INVALID\")\n",
    "    \n",
    "    print(f\"Total Parameters: {validation_result['total_parameters']:,}\")\n",
    "    \n",
    "    if validation_result['errors']:\n",
    "        print(\"\\n🚨 ERRORS:\")\n",
    "        for error in validation_result['errors']:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        print(\"\\n⚠️  WARNINGS:\")\n",
    "        for warning in validation_result['warnings']:\n",
    "            print(f\"  - {warning}\")\n",
    "    \n",
    "    if validation_result['recommendations']:\n",
    "        print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "        for rec in validation_result['recommendations']:\n",
    "            print(f\"  - {rec}\")\n",
    "\n",
    "# Test validation on different architectures\n",
    "print(\"Architecture Validation Tests:\")\n",
    "\n",
    "test_architectures = [\n",
    "    [784, 256, 128, 64, 10],  # Good architecture\n",
    "    [784, 2048, 1024, 512, 10],  # Large architecture\n",
    "    [1000, 10],  # Too simple\n",
    "    [100, 1000, 5],  # Large jump down\n",
    "    [784, 512, 256, 128, 64, 32, 16, 8, 4, 1],  # Very deep\n",
    "    [784, 0, 10],  # Invalid (zero neurons)\n",
    "]\n",
    "\n",
    "for arch in test_architectures:\n",
    "    validation = validate_architecture(arch)\n",
    "    print_validation_results(arch, validation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Progress Tracking and Key Concepts\n",
    "\n",
    "Let's summarize what we've learned and check our progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Tracking Checklist\n",
    "progress_checklist = {\n",
    "    \"Understanding deep network architecture concepts\": True,\n",
    "    \"Implementing weight initialization strategies\": True,\n",
    "    \"Creating a flexible DeepNeuralNetwork class\": True,\n",
    "    \"Analyzing network parameters and properties\": True,\n",
    "    \"Exploring architecture design patterns\": True,\n",
    "    \"Validating architectures for best practices\": True,\n",
    "    \"Understanding parameter count implications\": True\n",
    "}\n",
    "\n",
    "print(\"Progress Tracking Checklist:\")\n",
    "print(\"=\" * 50)\n",
    "for item, completed in progress_checklist.items():\n",
    "    status = \"✅\" if completed else \"❌\"\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "completed_items = sum(progress_checklist.values())\n",
    "total_items = len(progress_checklist)\n",
    "print(f\"\\nProgress: {completed_items}/{total_items} ({completed_items/total_items*100:.1f}%) Complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY CONCEPTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "key_concepts = {\n",
    "    \"Deep Neural Networks\": \"Networks with multiple hidden layers for complex pattern recognition\",\n",
    "    \"Weight Initialization\": \"Critical for training success; He for ReLU, Xavier for tanh/sigmoid\",\n",
    "    \"Architecture Design\": \"Layer sizes and connectivity patterns affect model capacity\",\n",
    "    \"Parameter Management\": \"Organizing weights and biases across all network layers\",\n",
    "    \"Pyramid Architecture\": \"Gradually reducing layer sizes from input to output\",\n",
    "    \"Diamond Architecture\": \"Expanding then contracting layer sizes\",\n",
    "    \"Uniform Architecture\": \"Same-sized hidden layers throughout the network\",\n",
    "    \"Architecture Validation\": \"Checking for best practices and potential issues\"\n",
    "}\n",
    "\n",
    "for concept, description in key_concepts.items():\n",
    "    print(f\"\\n{concept}:\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Implement forward propagation for deep networks\")\n",
    "print(\"2. Implement backward propagation with chain rule\")\n",
    "print(\"3. Add regularization techniques\")\n",
    "print(\"4. Implement training loops and optimization\")\n",
    "print(\"5. Test on real datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Cleanup Instructions\n",
    "\n",
    "### Windows Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in the command prompt to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close command prompt\n",
    "\n",
    "### Mac Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in terminal to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close terminal\n",
    "\n",
    "### Save Your Work:\n",
    "- Your notebook is automatically saved\n",
    "- Consider saving a copy with your name: `lab_4_1_[your_name].ipynb`\n",
    "- Export as HTML for offline viewing: File → Download as → HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Import errors (NumPy, Matplotlib)**\n",
    "- **Solution**: Install missing packages: `pip install numpy matplotlib`\n",
    "- **Windows**: Use `conda install numpy matplotlib` if using Anaconda\n",
    "- **Mac**: Same as Windows, or use `pip3` instead of `pip`\n",
    "\n",
    "**Issue 2: Memory errors with large networks**\n",
    "- **Solution**: Reduce network size or batch size\n",
    "- **Alternative**: Use different initialization with smaller values\n",
    "\n",
    "**Issue 3: Slow execution**\n",
    "- **Solution**: Reduce network complexity or visualization details\n",
    "- **Check**: Available RAM and close other applications\n",
    "\n",
    "**Issue 4: Visualization not showing**\n",
    "- **Solution**: Run `%matplotlib inline` in a cell\n",
    "- **Alternative**: Try `plt.show()` after each plot\n",
    "\n",
    "**Issue 5: Random seed not working**\n",
    "- **Solution**: Run the seed setting cell before network creation\n",
    "- **Check**: Ensure consistent seed values across experiments\n",
    "\n",
    "### Getting Help:\n",
    "- Check the error message carefully\n",
    "- Try restarting the kernel: Kernel → Restart\n",
    "- Ask instructor or teaching assistant\n",
    "- Refer to NumPy documentation: https://numpy.org/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}