{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Lab 4.1: Introduction to TensorFlow - Why Frameworks Matter\n","\n","## Duration: 45 minutes\n","\n","## Learning Objectives\n","By the end of this lab, you will be able to:\n","- Understand the key benefits of using TensorFlow/Keras over NumPy\n","- Build your first neural network with TensorFlow in just a few lines\n","- Compare manual NumPy implementation with TensorFlow's automatic features\n","- Appreciate automatic differentiation and GPU acceleration\n","- Use built-in optimizers and loss functions\n","\n","## Prerequisites\n","- Completion of Labs 1-3 (NumPy neural network implementation)\n","- Basic understanding of neural networks\n","- Python and NumPy knowledge\n","\n","## Key Concepts\n","- **TensorFlow/Keras**: Google's deep learning framework with high-level API\n","- **Automatic Differentiation**: No manual backpropagation needed!\n","- **Built-in Functions**: Optimizers, losses, metrics, and more\n","- **GPU Acceleration**: Automatic hardware optimization\n","- **Model Management**: Easy saving, loading, and deployment"]},{"cell_type":"markdown","metadata":{},"source":["## Setup and Imports\n","\n","First, let's import our libraries and see what we're working with:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","print(\"Welcome to TensorFlow!\")\n","print(f\"TensorFlow version: {tf.__version__}\")\n","print(f\"Keras version: {keras.__version__}\")\n","print()\n","\n","# Check hardware availability\n","if tf.config.list_physical_devices('GPU'):\n","    print(\"GPU is available - your models will run faster!\")\n","    gpu_devices = tf.config.list_physical_devices('GPU')\n","    for device in gpu_devices:\n","        print(f\"   Found GPU: {device.name}\")\n","else:\n","    print(\"Using CPU (still much easier than NumPy!)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Ready to see why TensorFlow makes deep learning easier?\")\n","print(\"=\"*60)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: The Pain of NumPy - Manual Implementation\n","\n","Remember all the code we wrote in Labs 1-3? Let's remind ourselves of the complexity:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This is what we had to write in NumPy for a simple neural network!\n","class SimpleNumpyNetwork:\n","    \"\"\"A reminder of our NumPy implementation from earlier labs\"\"\"\n","    \n","    def __init__(self, layer_sizes):\n","        self.layer_sizes = layer_sizes\n","        self.parameters = {}\n","        self.gradients = {}\n","        \n","        # Initialize weights and biases manually\n","        for i in range(1, len(layer_sizes)):\n","            self.parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * 0.01\n","            self.parameters[f'b{i}'] = np.zeros((layer_sizes[i], 1))\n","    \n","    def sigmoid(self, z):\n","        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n","    \n","    def sigmoid_derivative(self, a):\n","        return a * (1 - a)\n","    \n","    def forward(self, X):\n","        # Manual forward propagation\n","        self.cache = {'A0': X}\n","        A = X\n","        \n","        for i in range(1, len(self.layer_sizes)):\n","            Z = self.parameters[f'W{i}'] @ A + self.parameters[f'b{i}']\n","            A = self.sigmoid(Z)\n","            self.cache[f'Z{i}'] = Z\n","            self.cache[f'A{i}'] = A\n","        \n","        return A\n","    \n","    def backward(self, X, Y, output):\n","        # Manual backpropagation - so much code!\n","        m = X.shape[1]\n","        L = len(self.layer_sizes) - 1\n","        \n","        # Output layer gradient\n","        dA = output - Y\n","        \n","        for i in reversed(range(1, L + 1)):\n","            dZ = dA * self.sigmoid_derivative(self.cache[f'A{i}'])\n","            self.gradients[f'W{i}'] = (1/m) * dZ @ self.cache[f'A{i-1}'].T\n","            self.gradients[f'b{i}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n","            if i > 1:\n","                dA = self.parameters[f'W{i}'].T @ dZ\n","    \n","    def update_parameters(self, learning_rate):\n","        # Manual parameter updates\n","        for i in range(1, len(self.layer_sizes)):\n","            self.parameters[f'W{i}'] -= learning_rate * self.gradients[f'W{i}']\n","            self.parameters[f'b{i}'] -= learning_rate * self.gradients[f'b{i}']\n","    \n","    def train_step(self, X, Y, learning_rate=0.01):\n","        # One training step\n","        output = self.forward(X)\n","        loss = -np.mean(Y * np.log(output + 1e-8) + (1 - Y) * np.log(1 - output + 1e-8))\n","        self.backward(X, Y, output)\n","        self.update_parameters(learning_rate)\n","        return loss\n","\n","# Create a simple dataset\n","np.random.seed(42)\n","X_train = np.random.randn(2, 100)\n","Y_train = (X_train[0] + X_train[1] > 0).astype(int).reshape(1, -1)\n","\n","print(\"NumPy Implementation:\")\n","print(\"=\"*40)\n","print(\"Lines of code for basic neural network: ~70\")\n","print(\"Features we had to implement manually:\")\n","print(\"  X Forward propagation\")\n","print(\"  X Backpropagation\")\n","print(\"  X Activation functions\")\n","print(\"  X Loss functions\")\n","print(\"  X Parameter updates\")\n","print(\"  X Weight initialization\")\n","print(\"\\nLet's train it briefly...\")\n","\n","# Train the NumPy network\n","numpy_net = SimpleNumpyNetwork([2, 4, 1])\n","numpy_losses = []\n","\n","start_time = time.time()\n","for epoch in range(100):\n","    loss = numpy_net.train_step(X_train, Y_train, learning_rate=0.1)\n","    numpy_losses.append(loss)\n","    if epoch % 20 == 0:\n","        print(f\"  Epoch {epoch}: Loss = {loss:.4f}\")\n","numpy_time = time.time() - start_time\n","\n","print(f\"\\nTraining time: {numpy_time:.3f} seconds\")\n","print(f\"Final loss: {numpy_losses[-1]:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: The Magic of TensorFlow - Same Network, 10x Easier!\n","\n","Now let's build the EXACT same network with TensorFlow/Keras:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"TensorFlow Implementation:\")\n","print(\"=\"*40)\n","\n","# Build the same network in TensorFlow - SO MUCH SIMPLER!\n","tf_model = keras.Sequential([\n","    layers.Dense(4, activation='sigmoid', input_shape=(2,)),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile with optimizer and loss - one line!\n","tf_model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","print(\"Neural network created in just 6 lines!\")\n","print()\n","print(\"What TensorFlow handles for us:\")\n","print(\"  ✓ Forward propagation (automatic)\")\n","print(\"  ✓ Backpropagation (automatic)\")\n","print(\"  ✓ Activation functions (30+ built-in)\")\n","print(\"  ✓ Loss functions (20+ built-in)\")\n","print(\"  ✓ Optimizers (10+ built-in, including Adam!)\")\n","print(\"  ✓ Weight initialization (smart defaults)\")\n","print(\"  ✓ GPU acceleration (automatic)\")\n","print(\"  ✓ Batch processing (automatic)\")\n","\n","print(\"\\nLet's train it...\")\n","\n","# Prepare data (TensorFlow format)\n","X_train_tf = X_train.T  # TensorFlow expects (samples, features)\n","Y_train_tf = Y_train.T\n","\n","# Train the model - one line!\n","start_time = time.time()\n","history = tf_model.fit(\n","    X_train_tf, Y_train_tf,\n","    epochs=100,\n","    batch_size=32,\n","    verbose=0  # Suppress output for cleaner display\n",")\n","tf_time = time.time() - start_time\n","\n","print(f\"\\nTraining time: {tf_time:.3f} seconds\")\n","print(f\"Final loss: {history.history['loss'][-1]:.4f}\")\n","print(f\"Final accuracy: {history.history['accuracy'][-1]:.2%}\")\n","\n","# Visualize the comparison\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Loss comparison\n","ax1.plot(numpy_losses, label='NumPy Implementation', linewidth=2, color='blue')\n","ax1.plot(history.history['loss'], label='TensorFlow', linewidth=2, color='red')\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('Loss')\n","ax1.set_title('Training Loss Comparison')\n","ax1.legend()\n","ax1.grid(True, alpha=0.3)\n","\n","# Implementation comparison\n","categories = ['Lines of\\nCode', 'Training\\nSpeed', 'Features', 'GPU\\nSupport']\n","numpy_scores = [70, 30, 20, 0]  # Relative scores\n","tf_scores = [6, 90, 100, 100]   # Relative scores\n","\n","x = np.arange(len(categories))\n","width = 0.35\n","\n","ax2.bar(x - width/2, numpy_scores, width, label='NumPy', color='blue', alpha=0.7)\n","ax2.bar(x + width/2, tf_scores, width, label='TensorFlow', color='red', alpha=0.7)\n","ax2.set_ylabel('Score (Higher is Better)')\n","ax2.set_title('Implementation Comparison')\n","ax2.set_xticks(x)\n","ax2.set_xticklabels(categories)\n","ax2.legend()\n","ax2.grid(True, alpha=0.3, axis='y')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nKey Insight: TensorFlow gives us the same (or better) results\")\n","print(\"   with 10x less code and many more features!\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Putting It All Together - A Complete Example\n","\n","Let's build a complete classification model with all of TensorFlow's benefits:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Complete TensorFlow Workflow:\")\n","print(\"=\"*50)\n","\n","# Generate a multi-class classification dataset\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create dataset\n","X, y = make_classification(\n","    n_samples=1000,\n","    n_features=20,\n","    n_informative=15,\n","    n_redundant=5,\n","    n_classes=3,\n","    random_state=42\n",")\n","\n","# Preprocess\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(f\"Dataset: {X_train.shape[0]} training, {X_test.shape[0]} test samples\")\n","print(f\"Features: {X_train.shape[1]}\")\n","print(f\"Classes: {len(np.unique(y))}\")\n","\n","# Build model - notice how easy it is!\n","model = keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(20,)),\n","    layers.BatchNormalization(),\n","    layers.Dropout(0.3),\n","    \n","    layers.Dense(32, activation='relu'),\n","    layers.BatchNormalization(),\n","    layers.Dropout(0.2),\n","    \n","    layers.Dense(16, activation='relu'),\n","    \n","    layers.Dense(3, activation='softmax')\n","])\n","\n","# Compile with advanced features\n","model.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","print(\"\\nModel compiled with:\")\n","print(\"  • Adam optimizer (adaptive learning rate)\")\n","print(\"  • Categorical crossentropy loss\")\n","print(\"  • Multiple metrics tracking\")\n","print(\"  • Batch normalization\")\n","print(\"  • Dropout regularization\")\n","\n","# Train with callbacks\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=10,\n","    restore_best_weights=True\n",")\n","\n","print(\"\\nTraining with early stopping...\")\n","history = model.fit(\n","    X_train, y_train,\n","    validation_split=0.2,\n","    epochs=50,\n","    batch_size=32,\n","    callbacks=[early_stopping],\n","    verbose=0\n",")\n","\n","print(f\"Training stopped at epoch {len(history.history['loss'])}\")\n","\n","# Evaluate\n","test_loss, test_acc = model.evaluate(\n","    X_test, y_test, verbose=0\n",")\n","\n","print(f\"\\nTest Results:\")\n","print(f\"  Loss: {test_loss:.4f}\")\n","print(f\"  Accuracy: {test_acc:.2%}\")\n","\n","print(\"\\nComplete deep learning pipeline in < 30 lines of code!\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}