{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2: Deep Network Propagation Algorithms\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement forward propagation for deep neural networks\n",
    "- Understand and implement backward propagation with the chain rule\n",
    "- Cache intermediate values for efficient backpropagation\n",
    "- Debug propagation algorithms and verify gradient computations\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 4.1: Deep Network Architecture Implementation\n",
    "- Understanding of matrix operations and the chain rule\n",
    "- Knowledge of activation functions and their derivatives\n",
    "\n",
    "## Key Concepts\n",
    "- **Forward Propagation**: Computing outputs by passing data through the network\n",
    "- **Backward Propagation**: Computing gradients using the chain rule\n",
    "- **Caching**: Storing intermediate values for efficient gradient computation\n",
    "- **Chain Rule**: Mathematical foundation for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Activation Functions and Their Derivatives\n",
    "\n",
    "Before implementing propagation algorithms, let's define activation functions and their derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"\n",
    "    Collection of activation functions and their derivatives\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Tanh activation function\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh\"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        \"\"\"Linear activation function\"\"\"\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        \"\"\"Derivative of linear\"\"\"\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax activation function\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "# Test activation functions\n",
    "print(\"Testing Activation Functions:\")\n",
    "\n",
    "# Test data\n",
    "z = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Plot activation functions and their derivatives\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "activations = [\n",
    "    ('ReLU', ActivationFunctions.relu, ActivationFunctions.relu_derivative),\n",
    "    ('Sigmoid', ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n",
    "    ('Tanh', ActivationFunctions.tanh, ActivationFunctions.tanh_derivative)\n",
    "]\n",
    "\n",
    "for idx, (name, func, deriv_func) in enumerate(activations):\n",
    "    # Activation function\n",
    "    y = func(z)\n",
    "    axes[0, idx].plot(z, y, 'b-', linewidth=2, label=name)\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    axes[0, idx].set_title(f'{name} Function')\n",
    "    axes[0, idx].set_xlabel('z')\n",
    "    axes[0, idx].set_ylabel(f'{name.lower()}(z)')\n",
    "    \n",
    "    # Derivative\n",
    "    dy = deriv_func(z)\n",
    "    axes[1, idx].plot(z, dy, 'r-', linewidth=2, label=f\"{name}' derivative\")\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "    axes[1, idx].set_title(f'{name} Derivative')\n",
    "    axes[1, idx].set_xlabel('z')\n",
    "    axes[1, idx].set_ylabel(f\"d{name.lower()}/dz\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nActivation functions and derivatives are working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deep Neural Network with Propagation\n",
    "\n",
    "Let's extend our neural network class to include forward and backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetworkWithPropagation:\n",
    "    \"\"\"\n",
    "    Deep Neural Network with forward and backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, hidden_activation='relu', output_activation='sigmoid', \n",
    "                 initialization='he_normal', random_seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the deep neural network with propagation capabilities\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes: list of integers, number of units in each layer\n",
    "        hidden_activation: activation function for hidden layers\n",
    "        output_activation: activation function for output layer\n",
    "        initialization: weight initialization method\n",
    "        random_seed: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "            \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.parameters = self._initialize_parameters(initialization)\n",
    "        \n",
    "        # Cache for forward propagation\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Gradients\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Activation function mappings\n",
    "        self.activation_functions = {\n",
    "            'relu': (ActivationFunctions.relu, ActivationFunctions.relu_derivative),\n",
    "            'sigmoid': (ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n",
    "            'tanh': (ActivationFunctions.tanh, ActivationFunctions.tanh_derivative),\n",
    "            'linear': (ActivationFunctions.linear, ActivationFunctions.linear_derivative),\n",
    "            'softmax': (ActivationFunctions.softmax, None)  # Softmax derivative handled separately\n",
    "        }\n",
    "        \n",
    "        print(f\"Deep Neural Network initialized:\")\n",
    "        print(f\"Architecture: {layer_sizes}\")\n",
    "        print(f\"Hidden activation: {hidden_activation}\")\n",
    "        print(f\"Output activation: {output_activation}\")\n",
    "        print(f\"Total parameters: {self._count_parameters():,}\")\n",
    "    \n",
    "    def _initialize_parameters(self, initialization='he_normal'):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for layer in range(1, self.num_layers):\n",
    "            # Weight initialization\n",
    "            if initialization == 'he_normal':\n",
    "                parameters[f'W{layer}'] = np.random.randn(\n",
    "                    self.layer_sizes[layer], self.layer_sizes[layer-1]\n",
    "                ) * np.sqrt(2 / self.layer_sizes[layer-1])\n",
    "            elif initialization == 'xavier_normal':\n",
    "                parameters[f'W{layer}'] = np.random.randn(\n",
    "                    self.layer_sizes[layer], self.layer_sizes[layer-1]\n",
    "                ) * np.sqrt(2 / (self.layer_sizes[layer-1] + self.layer_sizes[layer]))\n",
    "            else:\n",
    "                parameters[f'W{layer}'] = np.random.randn(\n",
    "                    self.layer_sizes[layer], self.layer_sizes[layer-1]\n",
    "                ) * 0.01\n",
    "            \n",
    "            # Bias initialization\n",
    "            parameters[f'b{layer}'] = np.zeros((self.layer_sizes[layer], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        \"\"\"Count total parameters\"\"\"\n",
    "        return sum(param.size for param in self.parameters.values())\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "        \n",
    "        Parameters:\n",
    "        X: input data of shape (input_size, m) where m is number of examples\n",
    "        \n",
    "        Returns:\n",
    "        AL: output of the last layer\n",
    "        \"\"\"\n",
    "        # Clear previous cache\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Input layer\n",
    "        A = X\n",
    "        self.cache['A0'] = A\n",
    "        \n",
    "        # Forward through all layers\n",
    "        for layer in range(1, self.num_layers):\n",
    "            A_prev = A\n",
    "            \n",
    "            # Linear transformation\n",
    "            Z = np.dot(self.parameters[f'W{layer}'], A_prev) + self.parameters[f'b{layer}']\n",
    "            \n",
    "            # Store linear cache\n",
    "            self.cache[f'Z{layer}'] = Z\n",
    "            self.cache[f'A{layer-1}'] = A_prev\n",
    "            \n",
    "            # Apply activation function\n",
    "            if layer == self.num_layers - 1:  # Output layer\n",
    "                activation_name = self.output_activation\n",
    "            else:  # Hidden layers\n",
    "                activation_name = self.hidden_activation\n",
    "            \n",
    "            activation_func, _ = self.activation_functions[activation_name]\n",
    "            A = activation_func(Z)\n",
    "            \n",
    "            # Store activation cache\n",
    "            self.cache[f'A{layer}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward_propagation(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients\n",
    "        \n",
    "        Parameters:\n",
    "        AL: output of forward propagation\n",
    "        Y: true labels\n",
    "        \n",
    "        Returns:\n",
    "        gradients: dictionary containing gradients\n",
    "        \"\"\"\n",
    "        m = AL.shape[1]  # Number of examples\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Initialize backward propagation\n",
    "        # For binary classification with sigmoid output\n",
    "        if self.output_activation == 'sigmoid':\n",
    "            dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        elif self.output_activation == 'linear':\n",
    "            dAL = AL - Y\n",
    "        else:\n",
    "            # General case - derivative of cost with respect to output\n",
    "            dAL = AL - Y\n",
    "        \n",
    "        # Backward propagation through all layers\n",
    "        for layer in range(self.num_layers - 1, 0, -1):\n",
    "            # Get cached values\n",
    "            Z = self.cache[f'Z{layer}']\n",
    "            A_prev = self.cache[f'A{layer-1}']\n",
    "            \n",
    "            # Get activation derivative\n",
    "            if layer == self.num_layers - 1:  # Output layer\n",
    "                activation_name = self.output_activation\n",
    "            else:  # Hidden layers\n",
    "                activation_name = self.hidden_activation\n",
    "            \n",
    "            _, activation_derivative = self.activation_functions[activation_name]\n",
    "            \n",
    "            # Compute dZ\n",
    "            if activation_name == 'softmax':\n",
    "                # For softmax, we assume dAL already includes the derivative\n",
    "                dZ = dAL\n",
    "            else:\n",
    "                dZ = dAL * activation_derivative(Z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            self.gradients[f'dW{layer}'] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            self.gradients[f'db{layer}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            # Compute dA_prev for next iteration\n",
    "            if layer > 1:  # Not the first layer\n",
    "                dAL = np.dot(self.parameters[f'W{layer}'].T, dZ)\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y, cost_function='binary_crossentropy'):\n",
    "        \"\"\"\n",
    "        Compute the cost/loss\n",
    "        \n",
    "        Parameters:\n",
    "        AL: output of forward propagation\n",
    "        Y: true labels\n",
    "        cost_function: type of cost function to use\n",
    "        \n",
    "        Returns:\n",
    "        cost: scalar cost value\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        if cost_function == 'binary_crossentropy':\n",
    "            # Binary cross-entropy\n",
    "            cost = -(1/m) * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        elif cost_function == 'mean_squared_error':\n",
    "            # Mean squared error\n",
    "            cost = (1/(2*m)) * np.sum(np.square(AL - Y))\n",
    "        else:\n",
    "            # Default to MSE\n",
    "            cost = (1/(2*m)) * np.sum(np.square(AL - Y))\n",
    "        \n",
    "        return cost\n",
    "\n",
    "# Test the extended neural network\n",
    "print(\"Testing Deep Neural Network with Propagation:\")\n",
    "print()\n",
    "\n",
    "# Create test network\n",
    "test_network = DeepNeuralNetworkWithPropagation(\n",
    "    layer_sizes=[4, 8, 6, 3, 1],\n",
    "    hidden_activation='relu',\n",
    "    output_activation='sigmoid',\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Create test data\n",
    "m = 100  # number of examples\n",
    "X = np.random.randn(4, m)  # 4 features, 100 examples\n",
    "Y = np.random.randint(0, 2, (1, m))  # Binary labels\n",
    "\n",
    "print(f\"\\nTest data shapes:\")\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"Y: {Y.shape}\")\n",
    "\n",
    "# Test forward propagation\n",
    "print(\"\\nTesting forward propagation...\")\n",
    "AL = test_network.forward_propagation(X)\n",
    "print(f\"Output shape: {AL.shape}\")\n",
    "print(f\"Output range: [{AL.min():.4f}, {AL.max():.4f}]\")\n",
    "\n",
    "# Test cost computation\n",
    "cost = test_network.compute_cost(AL, Y)\n",
    "print(f\"\\nCost: {cost:.6f}\")\n",
    "\n",
    "# Test backward propagation\n",
    "print(\"\\nTesting backward propagation...\")\n",
    "gradients = test_network.backward_propagation(AL, Y)\n",
    "\n",
    "print(f\"\\nGradient shapes:\")\n",
    "for key, grad in gradients.items():\n",
    "    print(f\"{key}: {grad.shape}\")\n",
    "\n",
    "print(\"\\nâœ… All propagation tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Gradient Checking\n",
    "\n",
    "Let's implement gradient checking to verify our backpropagation implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(network, X, Y, epsilon=1e-7, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Perform gradient checking to verify backpropagation implementation\n",
    "    \n",
    "    Parameters:\n",
    "    network: neural network instance\n",
    "    X: input data\n",
    "    Y: labels\n",
    "    epsilon: small value for numerical gradient computation\n",
    "    threshold: threshold for considering gradients as matching\n",
    "    \n",
    "    Returns:\n",
    "    difference: relative difference between analytical and numerical gradients\n",
    "    \"\"\"\n",
    "    print(\"Performing gradient checking...\")\n",
    "    \n",
    "    # Forward propagation and backward propagation\n",
    "    AL = network.forward_propagation(X)\n",
    "    analytical_gradients = network.backward_propagation(AL, Y)\n",
    "    \n",
    "    # Convert parameters and gradients to vectors\n",
    "    parameters_vector = []\n",
    "    gradients_vector = []\n",
    "    keys = []\n",
    "    \n",
    "    for key in network.parameters.keys():\n",
    "        parameters_vector.extend(network.parameters[key].flatten())\n",
    "        if key.replace('W', 'dW').replace('b', 'db') in analytical_gradients:\n",
    "            grad_key = key.replace('W', 'dW').replace('b', 'db')\n",
    "            gradients_vector.extend(analytical_gradients[grad_key].flatten())\n",
    "            keys.extend([f\"{key}_{i}\" for i in range(network.parameters[key].size)])\n",
    "    \n",
    "    parameters_vector = np.array(parameters_vector)\n",
    "    gradients_vector = np.array(gradients_vector)\n",
    "    \n",
    "    # Compute numerical gradients\n",
    "    numerical_gradients = np.zeros_like(parameters_vector)\n",
    "    \n",
    "    print(f\"Computing numerical gradients for {len(parameters_vector)} parameters...\")\n",
    "    \n",
    "    # Sample a subset of parameters for efficiency (checking first 50)\n",
    "    check_indices = np.random.choice(len(parameters_vector), min(50, len(parameters_vector)), replace=False)\n",
    "    \n",
    "    for idx in check_indices:\n",
    "        # Create theta_plus\n",
    "        theta_plus = parameters_vector.copy()\n",
    "        theta_plus[idx] += epsilon\n",
    "        \n",
    "        # Create theta_minus\n",
    "        theta_minus = parameters_vector.copy()\n",
    "        theta_minus[idx] -= epsilon\n",
    "        \n",
    "        # Compute J_plus and J_minus\n",
    "        J_plus = _compute_cost_with_parameters(network, theta_plus, X, Y)\n",
    "        J_minus = _compute_cost_with_parameters(network, theta_minus, X, Y)\n",
    "        \n",
    "        # Compute numerical gradient\n",
    "        numerical_gradients[idx] = (J_plus - J_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Compare gradients for sampled indices\n",
    "    analytical_sample = gradients_vector[check_indices]\n",
    "    numerical_sample = numerical_gradients[check_indices]\n",
    "    \n",
    "    # Compute relative difference\n",
    "    numerator = np.linalg.norm(analytical_sample - numerical_sample)\n",
    "    denominator = np.linalg.norm(analytical_sample) + np.linalg.norm(numerical_sample)\n",
    "    difference = numerator / (denominator + 1e-8)\n",
    "    \n",
    "    print(f\"\\nGradient Check Results:\")\n",
    "    print(f\"Relative difference: {difference:.2e}\")\n",
    "    print(f\"Threshold: {threshold:.2e}\")\n",
    "    \n",
    "    if difference < threshold:\n",
    "        print(\"âœ… Gradient check PASSED! Backpropagation is correct.\")\n",
    "    else:\n",
    "        print(\"âŒ Gradient check FAILED! There may be a bug in backpropagation.\")\n",
    "        \n",
    "        # Show detailed comparison for debugging\n",
    "        print(\"\\nDetailed comparison (first 10 parameters):\")\n",
    "        for i in range(min(10, len(check_indices))):\n",
    "            idx = check_indices[i]\n",
    "            print(f\"Parameter {idx}: Analytical = {analytical_sample[i]:.6e}, \"\n",
    "                  f\"Numerical = {numerical_sample[i]:.6e}, \"\n",
    "                  f\"Diff = {abs(analytical_sample[i] - numerical_sample[i]):.6e}\")\n",
    "    \n",
    "    return difference\n",
    "\n",
    "def _compute_cost_with_parameters(network, parameters_vector, X, Y):\n",
    "    \"\"\"\n",
    "    Helper function to compute cost with given parameters vector\n",
    "    \"\"\"\n",
    "    # Convert parameters vector back to parameter dictionary\n",
    "    idx = 0\n",
    "    for key in network.parameters.keys():\n",
    "        param_size = network.parameters[key].size\n",
    "        param_shape = network.parameters[key].shape\n",
    "        network.parameters[key] = parameters_vector[idx:idx+param_size].reshape(param_shape)\n",
    "        idx += param_size\n",
    "    \n",
    "    # Forward propagation and cost computation\n",
    "    AL = network.forward_propagation(X)\n",
    "    cost = network.compute_cost(AL, Y)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Test gradient checking\n",
    "print(\"Testing Gradient Checking:\")\n",
    "print()\n",
    "\n",
    "# Create a smaller network for faster gradient checking\n",
    "small_network = DeepNeuralNetworkWithPropagation(\n",
    "    layer_sizes=[3, 4, 2, 1],\n",
    "    hidden_activation='relu',\n",
    "    output_activation='sigmoid',\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Create small test dataset\n",
    "X_test = np.random.randn(3, 10)\n",
    "Y_test = np.random.randint(0, 2, (1, 10))\n",
    "\n",
    "# Perform gradient check\n",
    "grad_diff = gradient_check(small_network, X_test, Y_test)\n",
    "\n",
    "print(\"\\nGradient checking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing Propagation Flow\n",
    "\n",
    "Let's visualize how information flows through the network during forward and backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_propagation_flow(network, X, Y, layer_to_analyze=2):\n",
    "    \"\"\"\n",
    "    Visualize the flow of information during propagation\n",
    "    \n",
    "    Parameters:\n",
    "    network: neural network instance\n",
    "    X: input data\n",
    "    Y: labels\n",
    "    layer_to_analyze: which layer to analyze in detail\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing propagation flow for layer {layer_to_analyze}:\")\n",
    "    \n",
    "    # Forward propagation\n",
    "    AL = network.forward_propagation(X)\n",
    "    \n",
    "    # Backward propagation\n",
    "    gradients = network.backward_propagation(AL, Y)\n",
    "    \n",
    "    # Extract information for visualization\n",
    "    Z = network.cache[f'Z{layer_to_analyze}']\n",
    "    A = network.cache[f'A{layer_to_analyze}']\n",
    "    A_prev = network.cache[f'A{layer_to_analyze-1}']\n",
    "    \n",
    "    W = network.parameters[f'W{layer_to_analyze}']\n",
    "    b = network.parameters[f'b{layer_to_analyze}']\n",
    "    dW = gradients[f'dW{layer_to_analyze}']\n",
    "    db = gradients[f'db{layer_to_analyze}']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Row 1: Forward propagation\n",
    "    # Input activations\n",
    "    im1 = axes[0, 0].imshow(A_prev[:10, :10], cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title(f'Input Activations A{layer_to_analyze-1}\\n(first 10x10)')\n",
    "    axes[0, 0].set_xlabel('Examples')\n",
    "    axes[0, 0].set_ylabel('Neurons')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Weights\n",
    "    im2 = axes[0, 1].imshow(W, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title(f'Weights W{layer_to_analyze}')\n",
    "    axes[0, 1].set_xlabel('Input Neurons')\n",
    "    axes[0, 1].set_ylabel('Output Neurons')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Linear outputs\n",
    "    im3 = axes[0, 2].imshow(Z[:10, :10], cmap='RdBu', aspect='auto')\n",
    "    axes[0, 2].set_title(f'Linear Outputs Z{layer_to_analyze}\\n(first 10x10)')\n",
    "    axes[0, 2].set_xlabel('Examples')\n",
    "    axes[0, 2].set_ylabel('Neurons')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Activations\n",
    "    im4 = axes[0, 3].imshow(A[:10, :10], cmap='RdBu', aspect='auto')\n",
    "    axes[0, 3].set_title(f'Activations A{layer_to_analyze}\\n(first 10x10)')\n",
    "    axes[0, 3].set_xlabel('Examples')\n",
    "    axes[0, 3].set_ylabel('Neurons')\n",
    "    plt.colorbar(im4, ax=axes[0, 3])\n",
    "    \n",
    "    # Row 2: Backward propagation (gradients)\n",
    "    # Weight gradients\n",
    "    im5 = axes[1, 0].imshow(dW, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title(f'Weight Gradients dW{layer_to_analyze}')\n",
    "    axes[1, 0].set_xlabel('Input Neurons')\n",
    "    axes[1, 0].set_ylabel('Output Neurons')\n",
    "    plt.colorbar(im5, ax=axes[1, 0])\n",
    "    \n",
    "    # Bias gradients\n",
    "    axes[1, 1].bar(range(len(db.flatten())), db.flatten())\n",
    "    axes[1, 1].set_title(f'Bias Gradients db{layer_to_analyze}')\n",
    "    axes[1, 1].set_xlabel('Neuron Index')\n",
    "    axes[1, 1].set_ylabel('Gradient Value')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient statistics\n",
    "    grad_stats = [\n",
    "        ('W grad mean', dW.mean()),\n",
    "        ('W grad std', dW.std()),\n",
    "        ('b grad mean', db.mean()),\n",
    "        ('b grad std', db.std())\n",
    "    ]\n",
    "    \n",
    "    stats_text = '\\n'.join([f'{stat}: {val:.6f}' for stat, val in grad_stats])\n",
    "    axes[1, 2].text(0.1, 0.5, stats_text, transform=axes[1, 2].transAxes,\n",
    "                    fontsize=12, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    axes[1, 2].set_title(f'Gradient Statistics\\nLayer {layer_to_analyze}')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Activation distribution\n",
    "    axes[1, 3].hist(A.flatten(), bins=30, alpha=0.7, density=True)\n",
    "    axes[1, 3].set_title(f'Activation Distribution\\nLayer {layer_to_analyze}')\n",
    "    axes[1, 3].set_xlabel('Activation Value')\n",
    "    axes[1, 3].set_ylabel('Density')\n",
    "    axes[1, 3].grid(True, alpha=0.3)\n",
    "    axes[1, 3].axvline(A.mean(), color='red', linestyle='--', label=f'Mean: {A.mean():.3f}')\n",
    "    axes[1, 3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nLayer {layer_to_analyze} Summary:\")\n",
    "    print(f\"Input shape: {A_prev.shape}\")\n",
    "    print(f\"Output shape: {A.shape}\")\n",
    "    print(f\"Weight shape: {W.shape}\")\n",
    "    print(f\"Bias shape: {b.shape}\")\n",
    "    print(f\"Activation range: [{A.min():.4f}, {A.max():.4f}]\")\n",
    "    print(f\"Gradient magnitudes: dW={np.linalg.norm(dW):.6f}, db={np.linalg.norm(db):.6f}\")\n",
    "\n",
    "# Visualize propagation for our test network\n",
    "print(\"Propagation Flow Visualization:\")\n",
    "print()\n",
    "\n",
    "# Use the test network with more data for better visualization\n",
    "X_vis = np.random.randn(4, 50)\n",
    "Y_vis = np.random.randint(0, 2, (1, 50))\n",
    "\n",
    "visualize_propagation_flow(test_network, X_vis, Y_vis, layer_to_analyze=2)\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Debugging Propagation Issues\n",
    "\n",
    "Let's create tools to debug common propagation problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagationDebugger:\n",
    "    \"\"\"\n",
    "    Tools for debugging propagation issues\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_vanishing_exploding_gradients(network, threshold_vanishing=1e-6, threshold_exploding=10):\n",
    "        \"\"\"\n",
    "        Check for vanishing or exploding gradients\n",
    "        \n",
    "        Parameters:\n",
    "        network: neural network instance\n",
    "        threshold_vanishing: threshold below which gradients are considered vanishing\n",
    "        threshold_exploding: threshold above which gradients are considered exploding\n",
    "        \n",
    "        Returns:\n",
    "        analysis: dictionary with gradient analysis\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'vanishing_layers': [],\n",
    "            'exploding_layers': [],\n",
    "            'gradient_norms': {},\n",
    "            'status': 'healthy'\n",
    "        }\n",
    "        \n",
    "        print(\"Checking for vanishing/exploding gradients:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for key, grad in network.gradients.items():\n",
    "            if 'dW' in key:\n",
    "                layer_num = int(key.replace('dW', ''))\n",
    "                grad_norm = np.linalg.norm(grad)\n",
    "                analysis['gradient_norms'][f'Layer_{layer_num}'] = grad_norm\n",
    "                \n",
    "                print(f\"Layer {layer_num}: ||dW|| = {grad_norm:.6e}\")\n",
    "                \n",
    "                if grad_norm < threshold_vanishing:\n",
    "                    analysis['vanishing_layers'].append(layer_num)\n",
    "                    print(f\"  âš ï¸  Potential vanishing gradient!\")\n",
    "                    \n",
    "                elif grad_norm > threshold_exploding:\n",
    "                    analysis['exploding_layers'].append(layer_num)\n",
    "                    print(f\"  âš ï¸  Potential exploding gradient!\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  âœ… Gradient magnitude is healthy\")\n",
    "        \n",
    "        # Set overall status\n",
    "        if analysis['vanishing_layers'] or analysis['exploding_layers']:\n",
    "            analysis['status'] = 'problematic'\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_activation_distributions(network):\n",
    "        \"\"\"\n",
    "        Analyze activation distributions across layers\n",
    "        \n",
    "        Parameters:\n",
    "        network: neural network instance\n",
    "        \n",
    "        Returns:\n",
    "        analysis: dictionary with activation analysis\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'layer_stats': {},\n",
    "            'potential_issues': []\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAnalyzing activation distributions:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for layer in range(network.num_layers):\n",
    "            if f'A{layer}' in network.cache:\n",
    "                activations = network.cache[f'A{layer}']\n",
    "                \n",
    "                stats = {\n",
    "                    'mean': activations.mean(),\n",
    "                    'std': activations.std(),\n",
    "                    'min': activations.min(),\n",
    "                    'max': activations.max(),\n",
    "                    'zeros_percentage': (activations == 0).mean() * 100\n",
    "                }\n",
    "                \n",
    "                analysis['layer_stats'][f'Layer_{layer}'] = stats\n",
    "                \n",
    "                print(f\"Layer {layer}:\")\n",
    "                print(f\"  Shape: {activations.shape}\")\n",
    "                print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "                print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "                print(f\"  Zeros: {stats['zeros_percentage']:.1f}%\")\n",
    "                \n",
    "                # Check for potential issues\n",
    "                if layer > 0:  # Skip input layer\n",
    "                    if stats['zeros_percentage'] > 50:\n",
    "                        analysis['potential_issues'].append(\n",
    "                            f\"Layer {layer}: High percentage of zeros ({stats['zeros_percentage']:.1f}%) - possible dead neurons\"\n",
    "                        )\n",
    "                    \n",
    "                    if abs(stats['mean']) > 10:\n",
    "                        analysis['potential_issues'].append(\n",
    "                            f\"Layer {layer}: Large mean activation ({stats['mean']:.4f}) - possible exploding activations\"\n",
    "                        )\n",
    "                    \n",
    "                    if stats['std'] < 0.01:\n",
    "                        analysis['potential_issues'].append(\n",
    "                            f\"Layer {layer}: Very low standard deviation ({stats['std']:.4f}) - possible vanishing activations\"\n",
    "                        )\n",
    "                \n",
    "                print()\n",
    "        \n",
    "        if analysis['potential_issues']:\n",
    "            print(\"âš ï¸  Potential Issues Detected:\")\n",
    "            for issue in analysis['potential_issues']:\n",
    "                print(f\"  - {issue}\")\n",
    "        else:\n",
    "            print(\"âœ… No activation distribution issues detected\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_gradient_flow(network):\n",
    "        \"\"\"\n",
    "        Visualize gradient flow across layers\n",
    "        \n",
    "        Parameters:\n",
    "        network: neural network instance\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        gradient_norms = []\n",
    "        \n",
    "        for key in sorted(network.gradients.keys()):\n",
    "            if 'dW' in key:\n",
    "                layer_num = int(key.replace('dW', ''))\n",
    "                grad_norm = np.linalg.norm(network.gradients[key])\n",
    "                layers.append(layer_num)\n",
    "                gradient_norms.append(grad_norm)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot gradient norms\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(layers, gradient_norms, 'bo-', linewidth=2, markersize=8)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Layer Number')\n",
    "        plt.ylabel('Gradient Norm (log scale)')\n",
    "        plt.title('Gradient Flow Across Layers')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add horizontal lines for thresholds\n",
    "        plt.axhline(y=1e-6, color='r', linestyle='--', alpha=0.7, label='Vanishing threshold')\n",
    "        plt.axhline(y=10, color='r', linestyle='--', alpha=0.7, label='Exploding threshold')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot gradient ratio between consecutive layers\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if len(gradient_norms) > 1:\n",
    "            ratios = [gradient_norms[i+1] / gradient_norms[i] for i in range(len(gradient_norms)-1)]\n",
    "            layer_pairs = [f\"{layers[i]}->{layers[i+1]}\" for i in range(len(layers)-1)]\n",
    "            \n",
    "            plt.bar(range(len(ratios)), ratios, alpha=0.7)\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('Layer Transition')\n",
    "            plt.ylabel('Gradient Ratio (log scale)')\n",
    "            plt.title('Gradient Ratio Between Consecutive Layers')\n",
    "            plt.xticks(range(len(ratios)), layer_pairs, rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add reference line at ratio = 1\n",
    "            plt.axhline(y=1, color='g', linestyle='-', alpha=0.7, label='Ratio = 1')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test the debugging tools\n",
    "print(\"Testing Propagation Debugging Tools:\")\n",
    "print()\n",
    "\n",
    "# Create test scenario with potential issues\n",
    "debugger = PropagationDebugger()\n",
    "\n",
    "# Run forward and backward propagation first\n",
    "AL_debug = test_network.forward_propagation(X_vis)\n",
    "gradients_debug = test_network.backward_propagation(AL_debug, Y_vis)\n",
    "\n",
    "# Check for vanishing/exploding gradients\n",
    "gradient_analysis = debugger.check_vanishing_exploding_gradients(test_network)\n",
    "\n",
    "# Analyze activation distributions\n",
    "activation_analysis = debugger.analyze_activation_distributions(test_network)\n",
    "\n",
    "# Visualize gradient flow\n",
    "print(\"\\nVisualizing gradient flow:\")\n",
    "debugger.visualize_gradient_flow(test_network)\n",
    "\n",
    "print(\"\\nâœ… Debugging analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Optimization\n",
    "\n",
    "Let's explore techniques to optimize propagation performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_propagation(layer_sizes, num_examples, num_iterations=10):\n",
    "    \"\"\"\n",
    "    Benchmark forward and backward propagation performance\n",
    "    \n",
    "    Parameters:\n",
    "    layer_sizes: architecture of the network\n",
    "    num_examples: number of training examples\n",
    "    num_iterations: number of iterations to average over\n",
    "    \n",
    "    Returns:\n",
    "    results: dictionary with timing results\n",
    "    \"\"\"\n",
    "    print(f\"Benchmarking propagation for architecture {layer_sizes} with {num_examples} examples:\")\n",
    "    \n",
    "    # Create network\n",
    "    network = DeepNeuralNetworkWithPropagation(\n",
    "        layer_sizes=layer_sizes,\n",
    "        hidden_activation='relu',\n",
    "        output_activation='sigmoid',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Create test data\n",
    "    X = np.random.randn(layer_sizes[0], num_examples)\n",
    "    Y = np.random.randint(0, 2, (layer_sizes[-1], num_examples))\n",
    "    \n",
    "    # Benchmark forward propagation\n",
    "    forward_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        AL = network.forward_propagation(X)\n",
    "        forward_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Benchmark backward propagation\n",
    "    backward_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        AL = network.forward_propagation(X)  # Need forward pass for backward\n",
    "        start_time = time.time()\n",
    "        gradients = network.backward_propagation(AL, Y)\n",
    "        backward_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Benchmark full forward+backward pass\n",
    "    full_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        AL = network.forward_propagation(X)\n",
    "        gradients = network.backward_propagation(AL, Y)\n",
    "        full_times.append(time.time() - start_time)\n",
    "    \n",
    "    results = {\n",
    "        'forward_time': np.mean(forward_times),\n",
    "        'forward_std': np.std(forward_times),\n",
    "        'backward_time': np.mean(backward_times),\n",
    "        'backward_std': np.std(backward_times),\n",
    "        'full_time': np.mean(full_times),\n",
    "        'full_std': np.std(full_times),\n",
    "        'parameters': network._count_parameters(),\n",
    "        'architecture': layer_sizes,\n",
    "        'examples': num_examples\n",
    "    }\n",
    "    \n",
    "    print(f\"Forward propagation: {results['forward_time']*1000:.2f} Â± {results['forward_std']*1000:.2f} ms\")\n",
    "    print(f\"Backward propagation: {results['backward_time']*1000:.2f} Â± {results['backward_std']*1000:.2f} ms\")\n",
    "    print(f\"Full pass: {results['full_time']*1000:.2f} Â± {results['full_std']*1000:.2f} ms\")\n",
    "    print(f\"Parameters: {results['parameters']:,}\")\n",
    "    print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_architectures():\n",
    "    \"\"\"\n",
    "    Compare performance of different architectures\n",
    "    \"\"\"\n",
    "    print(\"Performance Comparison of Different Architectures:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    architectures = [\n",
    "        [100, 50, 10],\n",
    "        [100, 100, 50, 10],\n",
    "        [100, 200, 100, 50, 10],\n",
    "        [100, 500, 200, 50, 10],\n",
    "    ]\n",
    "    \n",
    "    num_examples = 1000\n",
    "    results = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        print(f\"\\nTesting architecture: {arch}\")\n",
    "        print(\"-\" * 40)\n",
    "        result = benchmark_propagation(arch, num_examples, num_iterations=5)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot timing comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    architectures_labels = [str(r['architecture']) for r in results]\n",
    "    forward_times = [r['forward_time']*1000 for r in results]\n",
    "    backward_times = [r['backward_time']*1000 for r in results]\n",
    "    \n",
    "    x = range(len(results))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar([i - width/2 for i in x], forward_times, width, label='Forward', alpha=0.8)\n",
    "    plt.bar([i + width/2 for i in x], backward_times, width, label='Backward', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Architecture')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.title('Propagation Time Comparison')\n",
    "    plt.xticks(x, [f'Arch {i+1}' for i in range(len(results))], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot parameters vs time\n",
    "    plt.subplot(2, 2, 2)\n",
    "    parameters = [r['parameters'] for r in results]\n",
    "    full_times = [r['full_time']*1000 for r in results]\n",
    "    \n",
    "    plt.scatter(parameters, full_times, s=100, alpha=0.7)\n",
    "    plt.xlabel('Number of Parameters')\n",
    "    plt.ylabel('Full Pass Time (ms)')\n",
    "    plt.title('Parameters vs Execution Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, (param, time) in enumerate(zip(parameters, full_times)):\n",
    "        plt.annotate(f'Arch {i+1}', (param, time), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Plot depth vs time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    depths = [len(r['architecture']) - 1 for r in results]  # Number of hidden layers\n",
    "    \n",
    "    plt.scatter(depths, full_times, s=100, alpha=0.7, color='green')\n",
    "    plt.xlabel('Network Depth (Hidden Layers)')\n",
    "    plt.ylabel('Full Pass Time (ms)')\n",
    "    plt.title('Network Depth vs Execution Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot efficiency (time per parameter)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    efficiency = [r['full_time']*1000 / r['parameters'] for r in results]\n",
    "    \n",
    "    plt.bar(range(len(results)), efficiency, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Architecture')\n",
    "    plt.ylabel('Time per Parameter (ms/param Ã— 1000)')\n",
    "    plt.title('Computational Efficiency')\n",
    "    plt.xticks(range(len(results)), [f'Arch {i+1}' for i in range(len(results))], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance benchmarks\n",
    "print(\"Performance Benchmarking:\")\n",
    "print()\n",
    "\n",
    "benchmark_results = compare_architectures()\n",
    "\n",
    "print(\"\\nðŸ“Š Performance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Progress Tracking and Key Concepts Summary\n",
    "\n",
    "Let's summarize what we've learned and check our progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Tracking Checklist\n",
    "progress_checklist = {\n",
    "    \"Implementing activation functions and derivatives\": True,\n",
    "    \"Building deep network with forward propagation\": True,\n",
    "    \"Implementing backward propagation with chain rule\": True,\n",
    "    \"Creating gradient checking for verification\": True,\n",
    "    \"Visualizing propagation flow through layers\": True,\n",
    "    \"Debugging vanishing/exploding gradient issues\": True,\n",
    "    \"Analyzing activation distributions\": True,\n",
    "    \"Benchmarking propagation performance\": True,\n",
    "    \"Understanding caching for efficient backprop\": True\n",
    "}\n",
    "\n",
    "print(\"Progress Tracking Checklist:\")\n",
    "print(\"=\" * 50)\n",
    "for item, completed in progress_checklist.items():\n",
    "    status = \"âœ…\" if completed else \"âŒ\"\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "completed_items = sum(progress_checklist.values())\n",
    "total_items = len(progress_checklist)\n",
    "print(f\"\\nProgress: {completed_items}/{total_items} ({completed_items/total_items*100:.1f}%) Complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY CONCEPTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "key_concepts = {\n",
    "    \"Forward Propagation\": \"Sequential computation of outputs through network layers\",\n",
    "    \"Backward Propagation\": \"Gradient computation using chain rule in reverse order\",\n",
    "    \"Chain Rule\": \"Mathematical foundation for computing gradients in deep networks\",\n",
    "    \"Activation Functions\": \"Non-linear functions that enable networks to learn complex patterns\",\n",
    "    \"Caching\": \"Storing intermediate values for efficient gradient computation\",\n",
    "    \"Gradient Checking\": \"Numerical verification of analytical gradient computations\",\n",
    "    \"Vanishing Gradients\": \"Problem where gradients become too small in deep networks\",\n",
    "    \"Exploding Gradients\": \"Problem where gradients become too large and unstable\",\n",
    "    \"Performance Optimization\": \"Techniques to improve computational efficiency of propagation\"\n",
    "}\n",
    "\n",
    "for concept, description in key_concepts.items():\n",
    "    print(f\"\\n{concept}:\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MATHEMATICAL FOUNDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "math_foundations = [\n",
    "    \"Forward Pass: A[l] = g(Z[l]) where Z[l] = W[l]A[l-1] + b[l]\",\n",
    "    \"Chain Rule: dC/dW[l] = dC/dA[l] * dA[l]/dZ[l] * dZ[l]/dW[l]\",\n",
    "    \"Weight Gradient: dW[l] = (1/m) * dZ[l] * A[l-1].T\",\n",
    "    \"Bias Gradient: db[l] = (1/m) * sum(dZ[l])\",\n",
    "    \"Activation Gradient: dA[l-1] = W[l].T * dZ[l]\",\n",
    "    \"ReLU Derivative: g'(z) = 1 if z > 0, else 0\",\n",
    "    \"Sigmoid Derivative: g'(z) = g(z) * (1 - g(z))\"\n",
    "]\n",
    "\n",
    "for i, formula in enumerate(math_foundations, 1):\n",
    "    print(f\"{i}. {formula}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBUGGING CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "debugging_checklist = [\n",
    "    \"âœ“ Check gradient magnitudes (not too small/large)\",\n",
    "    \"âœ“ Verify activation distributions are reasonable\",\n",
    "    \"âœ“ Ensure no NaN or Inf values in computations\",\n",
    "    \"âœ“ Confirm gradient checking passes (<1e-7 difference)\",\n",
    "    \"âœ“ Monitor for dead neurons (high percentage of zeros)\",\n",
    "    \"âœ“ Validate input data preprocessing and scaling\",\n",
    "    \"âœ“ Check weight initialization is appropriate\",\n",
    "    \"âœ“ Verify mathematical implementation matches theory\"\n",
    "]\n",
    "\n",
    "for item in debugging_checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Implement training loops with optimization algorithms\")\n",
    "print(\"2. Add regularization techniques (L1, L2, dropout)\")\n",
    "print(\"3. Implement different cost functions\")\n",
    "print(\"4. Add learning rate scheduling\")\n",
    "print(\"5. Test on real classification/regression problems\")\n",
    "print(\"6. Implement batch processing and mini-batch gradient descent\")\n",
    "print(\"7. Add momentum and advanced optimizers (Adam, RMSprop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Cleanup Instructions\n",
    "\n",
    "### Windows Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in the command prompt to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close command prompt\n",
    "\n",
    "### Mac Users:\n",
    "1. Close all Jupyter notebook tabs\n",
    "2. Press `Ctrl+C` in terminal to stop Jupyter server\n",
    "3. Type `conda deactivate` or `deactivate` to exit virtual environment\n",
    "4. Close terminal\n",
    "\n",
    "### Save Your Work:\n",
    "- Your notebook is automatically saved\n",
    "- Consider saving a copy with your name: `lab_4_2_[your_name].ipynb`\n",
    "- Export as HTML for offline viewing: File â†’ Download as â†’ HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1: Gradient checking fails**\n",
    "- **Solution**: Check activation function derivatives\n",
    "- **Check**: Ensure epsilon value is appropriate (1e-7)\n",
    "- **Debug**: Print intermediate gradient values to locate error\n",
    "\n",
    "**Issue 2: NaN or Inf values in gradients**\n",
    "- **Solution**: Add numerical stability (clip extreme values)\n",
    "- **Check**: Weight initialization may be too large\n",
    "- **Fix**: Use proper initialization (He or Xavier)\n",
    "\n",
    "**Issue 3: Very slow execution**\n",
    "- **Solution**: Reduce network size or number of examples\n",
    "- **Optimize**: Use vectorized operations instead of loops\n",
    "- **Check**: Memory usage and available RAM\n",
    "\n",
    "**Issue 4: Vanishing gradients detected**\n",
    "- **Solution**: Use ReLU activation instead of sigmoid/tanh\n",
    "- **Try**: Different initialization method (He for ReLU)\n",
    "- **Consider**: Batch normalization or residual connections\n",
    "\n",
    "**Issue 5: Memory errors with large networks**\n",
    "- **Solution**: Reduce batch size or network complexity\n",
    "- **Alternative**: Process data in smaller chunks\n",
    "- **Check**: System memory and close other applications\n",
    "\n",
    "**Issue 6: Incorrect gradient shapes**\n",
    "- **Solution**: Verify matrix dimensions in forward pass\n",
    "- **Check**: Transpose operations are correct\n",
    "- **Debug**: Print shapes at each step\n",
    "\n",
    "### Getting Help:\n",
    "- Check error messages for specific line numbers\n",
    "- Try restarting the kernel: Kernel â†’ Restart & Clear Output\n",
    "- Use print statements to debug intermediate values\n",
    "- Ask instructor or teaching assistant for complex issues\n",
    "- Refer to NumPy documentation: https://numpy.org/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}