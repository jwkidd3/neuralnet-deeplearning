{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3: Introduction to Convolutional Neural Networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand why CNNs are perfect for image data\n",
    "- Build your first CNN using TensorFlow/Keras\n",
    "- Compare CNN performance with regular Dense networks\n",
    "- Use Conv2D, MaxPooling, and Flatten layers\n",
    "- Apply CNNs to real image classification problems\n",
    "\n",
    "## Prerequisites\n",
    "- **Lab 4.1 & 4.2 completed** (TensorFlow basics and deep networks)\n",
    "- Understanding of neural networks\n",
    "- Basic knowledge of images as data\n",
    "\n",
    "## Key Concepts\n",
    "- **Convolution**: Feature detection using filters/kernels\n",
    "- **Pooling**: Dimensionality reduction and translation invariance\n",
    "- **Feature Maps**: How CNNs detect patterns at different scales\n",
    "- **Spatial Hierarchy**: Learning from simple edges to complex objects\n",
    "- **Parameter Sharing**: Why CNNs need fewer parameters than Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Introduction\n",
    "\n",
    "Let's start by understanding why we need CNNs for image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"The Dense Network Approach:\")\nprint(\"=\" * 40)\n\n# For Dense layers, we need to flatten images\nX_train_flat = X_train.reshape(X_train.shape[0], -1)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)\n\nprint(f\"Original image shape: {X_train.shape[1:]}\")\nprint(f\"Flattened shape: {X_train_flat.shape[1:]}\")\nprint(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n\n# Build a Dense network\ndense_model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name='Dense_Network')\n\ndense_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(f\"\\nDense Model Summary:\")\ndense_model.summary()\n\nprint(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\nprint(\"\\n‚ùó Problems with Dense approach:\")\nprint(\"  ‚Ä¢ Loses spatial information (pixel relationships)\")\nprint(\"  ‚Ä¢ Treats each pixel independently\")\nprint(\"  ‚Ä¢ Lots of parameters (prone to overfitting)\")\nprint(\"  ‚Ä¢ Not translation invariant\")\nprint(\"  ‚Ä¢ Computationally expensive\")\n\n# Train briefly\nprint(\"\\nTraining Dense network (this may take a moment...)\")\ndense_history = dense_model.fit(\n    X_train_flat, y_train,\n    validation_data=(X_test_flat, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose=1\n)\n\ndense_accuracy = dense_history.history['val_accuracy'][-1]\nprint(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Building Different CNN Architectures\\n\\nLet's experiment with different CNN designs and see how they perform:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Function to visualize feature maps\\ndef visualize_feature_maps(model, image, layer_name):\\n    \\\"\\\"\\\"Visualize what a specific layer detects\\\"\\\"\\\"\\n    \\n    # Create a model that outputs the feature maps\\n    layer_output = model.get_layer(layer_name).output\\n    feature_model = keras.Model(inputs=model.input, outputs=layer_output)\\n    \\n    # Get feature maps for our image\\n    image = image.reshape(1, 28, 28, 1)  # Add batch dimension\\n    feature_maps = feature_model.predict(image, verbose=0)\\n    \\n    # Plot the feature maps\\n    n_features = min(16, feature_maps.shape[-1])  # Show up to 16 features\\n    fig, axes = plt.subplots(4, 4, figsize=(12, 10))\\n    \\n    for i in range(n_features):\\n        ax = axes[i // 4, i % 4]\\n        ax.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n        ax.set_title(f'Filter {i+1}')\\n        ax.axis('off')\\n    \\n    # Remove unused subplots\\n    for i in range(n_features, 16):\\n        axes[i // 4, i % 4].remove()\\n    \\n    plt.suptitle(f'Feature Maps from {layer_name}', fontsize=16)\\n    plt.tight_layout()\\n    plt.show()\\n    \\n    return feature_maps\\n\\n# Visualize features for a sample image\\nsample_image = X_train_cnn[0]  # First training image\\nprint(f\\\"Analyzing image of digit: {y_train[0]}\\\")\\n\\n# Show original image\\nplt.figure(figsize=(6, 4))\\nplt.subplot(1, 2, 1)\\nplt.imshow(sample_image.squeeze(), cmap='gray')\\nplt.title('Original Image')\\nplt.axis('off')\\n\\n# Show what the model predicts\\nprediction = cnn_model.predict(sample_image.reshape(1, 28, 28, 1), verbose=0)\\npredicted_class = np.argmax(prediction[0])\\nconfidence = prediction[0][predicted_class]\\n\\nplt.subplot(1, 2, 2)\\nplt.bar(range(10), prediction[0])\\nplt.title(f'Predictions (Predicted: {predicted_class}, Confidence: {confidence:.2%})')\\nplt.xlabel('Digit Class')\\nplt.ylabel('Probability')\\nplt.xticks(range(10))\\nplt.tight_layout()\\nplt.show()\\n\\nprint(\\\"\\\\nüîç Understanding Feature Maps:\\\")\\nprint(\\\"Feature maps show what each filter detects:\\\")\\nprint(\\\"  ‚Ä¢ Bright areas = filter strongly activated\\\")\\nprint(\\\"  ‚Ä¢ Dark areas = filter not activated\\\")\\nprint(\\\"  ‚Ä¢ Different filters detect different patterns\\\")\\n\\n# Visualize first convolutional layer\\nprint(\\\"\\\\nFirst Convolutional Layer - Basic Features (edges, corners):\\\")\\nfeature_maps_1 = visualize_feature_maps(cnn_model, sample_image, 'conv2d')\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Visualizing CNN Features\\n\\nLet's see what the CNN actually learns to detect:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Understanding CNN Layers:\")\nprint(\"=\" * 50)\n\n# Let's build a simple CNN and examine each layer\nsimple_cnn = keras.Sequential([\n    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    layers.MaxPooling2D((2, 2), name='pool1'),\n    layers.Conv2D(16, (3, 3), activation='relu', name='conv2'),\n    layers.MaxPooling2D((2, 2), name='pool2'),\n    layers.Flatten(name='flatten'),\n    layers.Dense(10, activation='softmax', name='classifier')\n])\n\nprint(\"Layer-by-layer analysis:\")\nprint(\"-\" * 30)\n\n# Trace through the network\ninput_shape = (28, 28, 1)\nprint(f\"Input: {input_shape} ‚Üí Raw 28x28 grayscale image\")\n\nfor i, layer in enumerate(simple_cnn.layers):\n    # Build the model up to this layer to see output shape\n    temp_model = keras.Sequential(simple_cnn.layers[:i+1])\n    temp_model.build(input_shape=(None,) + input_shape)\n    output_shape = temp_model.output_shape[1:]  # Remove batch dimension\n    \n    if 'conv' in layer.name:\n        filters = layer.filters\n        kernel_size = layer.kernel_size\n        print(f\"{layer.name}: {output_shape} ‚Üí {filters} filters of {kernel_size} detecting features\")\n    elif 'pool' in layer.name:\n        pool_size = layer.pool_size\n        print(f\"{layer.name}: {output_shape} ‚Üí Downsample by {pool_size}, keep strongest signals\")\n    elif 'flatten' in layer.name:\n        print(f\"{layer.name}: {output_shape} ‚Üí Convert 2D feature maps to 1D vector\")\n    elif 'dense' in layer.name or 'classifier' in layer.name:\n        units = layer.units\n        print(f\"{layer.name}: {output_shape} ‚Üí Final classification into {units} classes\")\n\nprint(\"\\nüîç Key Insights:\")\nprint(\"  ‚Ä¢ Each Conv2D layer detects increasingly complex features\")\nprint(\"  ‚Ä¢ MaxPooling reduces size while keeping important information\")\nprint(\"  ‚Ä¢ Flatten converts spatial features to vector for classification\")\nprint(\"  ‚Ä¢ Final Dense layer maps features to class probabilities\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Understanding CNN Components\\n\\nLet's dive deeper into what makes CNNs work so well:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"The CNN Approach:\")\nprint(\"=\" * 40)\n\n# For CNNs, we keep the spatial structure\nprint(f\"CNN input shape: {X_train.shape[1:]} (height, width, channels)\")\nprint(f\"We DON'T flatten the images!\")\n\n# Reshape for CNN (add channel dimension)\nX_train_cnn = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test_cnn = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\nprint(f\"CNN data shape: {X_train_cnn.shape}\")\n\n# Build a CNN\ncnn_model = keras.Sequential([\n    # First convolutional block\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    \n    # Second convolutional block  \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    # Third convolutional block\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    # Flatten and classify\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name='CNN_Network')\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(f\"\\nCNN Model Summary:\")\ncnn_model.summary()\n\nprint(f\"\\nTotal parameters in CNN model: {cnn_model.count_params():,}\")\nprint(\"\\n‚úÖ Advantages of CNN approach:\")\nprint(\"  ‚Ä¢ Preserves spatial relationships\")\nprint(\"  ‚Ä¢ Parameter sharing (same filter across image)\")\nprint(\"  ‚Ä¢ Translation invariant\")\nprint(\"  ‚Ä¢ Hierarchical feature learning\")\nprint(\"  ‚Ä¢ Fewer parameters than Dense\")\n\n# Train the CNN\nprint(\"\\nTraining CNN (this will take a moment...)\")\ncnn_history = cnn_model.fit(\n    X_train_cnn, y_train,\n    validation_data=(X_test_cnn, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose=1\n)\n\ncnn_accuracy = cnn_history.history['val_accuracy'][-1]\nprint(f\"CNN accuracy: {cnn_accuracy:.4f}\")\n\n# Compare the results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARISON RESULTS:\")\nprint(\"=\" * 60)\nprint(f\"Dense Network Accuracy: {dense_accuracy:.4f}\")\nprint(f\"CNN Network Accuracy:   {cnn_accuracy:.4f}\")\nprint(f\"Improvement: {((cnn_accuracy - dense_accuracy) * 100):.2f} percentage points\")\nprint(f\"Parameters - Dense: {dense_model.count_params():,}\")\nprint(f\"Parameters - CNN:   {cnn_model.count_params():,}\")\nprint(\"\\nüéâ CNN wins with better accuracy AND fewer parameters!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: The CNN Solution - Convolutional Layers\\n\\nNow let's build a CNN and see why it's so much better for image data:\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Problem with Dense Layers for Images\n",
    "\n",
    "Let's first see what happens when we use a regular Dense network on image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"The Dense Network Approach:\")\nprint(\"=\" * 40)\n\n# For Dense layers, we need to flatten images\nX_train_flat = X_train.reshape(X_train.shape[0], -1)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)\n\nprint(f\"Original image shape: {X_train.shape[1:]}\")\nprint(f\"Flattened shape: {X_train_flat.shape[1:]}\")\nprint(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n\n# Build a Dense network\ndense_model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name='Dense_Network')\n\ndense_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(f\"\\nDense Model Summary:\")\ndense_model.summary()\n\nprint(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\nprint(\"\\n‚ùó Problems with Dense approach:\")\nprint(\"  ‚Ä¢ Loses spatial information (pixel relationships)\")\nprint(\"  ‚Ä¢ Treats each pixel independently\")\nprint(\"  ‚Ä¢ Lots of parameters (prone to overfitting)\")\nprint(\"  ‚Ä¢ Not translation invariant\")\nprint(\"  ‚Ä¢ Computationally expensive\")\n\n# Train briefly\nprint(\"\\nTraining Dense network (this may take a moment...)\")\ndense_history = dense_model.fit(\n    X_train_flat, y_train,\n    validation_data=(X_test_flat, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose=1\n)\n\ndense_accuracy = dense_history.history['val_accuracy'][-1]\nprint(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}