{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3: Introduction to Convolutional Neural Networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand why CNNs are perfect for image data\n",
    "- Build your first CNN using TensorFlow/Keras\n",
    "- Compare CNN performance with regular Dense networks\n",
    "- Use Conv2D, MaxPooling, and Flatten layers\n",
    "- Apply CNNs to real image classification problems\n",
    "\n",
    "## Prerequisites\n",
    "- **Lab 4.1 & 4.2 completed** (TensorFlow basics and deep networks)\n",
    "- Understanding of neural networks\n",
    "- Basic knowledge of images as data\n",
    "\n",
    "## Key Concepts\n",
    "- **Convolution**: Feature detection using filters/kernels\n",
    "- **Pooling**: Dimensionality reduction and translation invariance\n",
    "- **Feature Maps**: How CNNs detect patterns at different scales\n",
    "- **Spatial Hierarchy**: Learning from simple edges to complex objects\n",
    "- **Parameter Sharing**: Why CNNs need fewer parameters than Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Introduction\n",
    "\n",
    "Let's start by understanding why we need CNNs for image data:"
   ]
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"Lab 4.3: Introduction to Convolutional Neural Networks\")\nprint(\"=\" * 60)\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# Load the MNIST dataset\nprint(\"\\nLoading MNIST dataset...\")\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Normalize pixel values to [0, 1]\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Test data shape: {X_test.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")\nprint(f\"Number of classes: {len(np.unique(y_train))}\")\n\n# Visualize some sample images\nprint(\"\\nüìä Sample MNIST Images:\")\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_train[i], cmap='gray')\n    ax.set_title(f'Label: {y_train[i]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Data loaded and ready for CNN experiments!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Dense Network Approach:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# For Dense layers, we need to flatten images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Original image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape[1:]}\")\n",
    "print(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n",
    "\n",
    "# Build a Dense network\n",
    "dense_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='Dense_Network')\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nDense Model Summary:\")\n",
    "dense_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\n",
    "print(\"\\n‚ùó Problems with Dense approach:\")\n",
    "print(\"  ‚Ä¢ Loses spatial information (pixel relationships)\")\n",
    "print(\"  ‚Ä¢ Treats each pixel independently\")\n",
    "print(\"  ‚Ä¢ Lots of parameters (prone to overfitting)\")\n",
    "print(\"  ‚Ä¢ Not translation invariant\")\n",
    "print(\"  ‚Ä¢ Computationally expensive\")\n",
    "\n",
    "# Train briefly\n",
    "print(\"\\nTraining Dense network (this may take a moment...)\")\n",
    "dense_history = dense_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    validation_data=(X_test_flat, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dense_accuracy = dense_history.history['val_accuracy'][-1]\n",
    "print(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Building Different CNN Architectures\\n\\nLet's experiment with different CNN designs and see how they perform:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize feature maps\\ndef visualize_feature_maps(model, image, layer_name):\\n    \\\"\\\"\\\"Visualize what a specific layer detects\\\"\\\"\\\"\\n    \\n    # Create a model that outputs the feature maps\\n    layer_output = model.get_layer(layer_name).output\\n    feature_model = keras.Model(inputs=model.input, outputs=layer_output)\\n    \\n    # Get feature maps for our image\\n    image = image.reshape(1, 28, 28, 1)  # Add batch dimension\\n    feature_maps = feature_model.predict(image, verbose=0)\\n    \\n    # Plot the feature maps\\n    n_features = min(16, feature_maps.shape[-1])  # Show up to 16 features\\n    fig, axes = plt.subplots(4, 4, figsize=(12, 10))\\n    \\n    for i in range(n_features):\\n        ax = axes[i // 4, i % 4]\\n        ax.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n        ax.set_title(f'Filter {i+1}')\\n        ax.axis('off')\\n    \\n    # Remove unused subplots\\n    for i in range(n_features, 16):\\n        axes[i // 4, i % 4].remove()\\n    \\n    plt.suptitle(f'Feature Maps from {layer_name}', fontsize=16)\\n    plt.tight_layout()\\n    plt.show()\\n    \\n    return feature_maps\\n\\n# Visualize features for a sample image\\nsample_image = X_train_cnn[0]  # First training image\\nprint(f\\\"Analyzing image of digit: {y_train[0]}\\\")\\n\\n# Show original image\\nplt.figure(figsize=(6, 4))\\nplt.subplot(1, 2, 1)\\nplt.imshow(sample_image.squeeze(), cmap='gray')\\nplt.title('Original Image')\\nplt.axis('off')\\n\\n# Show what the model predicts\\nprediction = cnn_model.predict(sample_image.reshape(1, 28, 28, 1), verbose=0)\\npredicted_class = np.argmax(prediction[0])\\nconfidence = prediction[0][predicted_class]\\n\\nplt.subplot(1, 2, 2)\\nplt.bar(range(10), prediction[0])\\nplt.title(f'Predictions (Predicted: {predicted_class}, Confidence: {confidence:.2%})')\\nplt.xlabel('Digit Class')\\nplt.ylabel('Probability')\\nplt.xticks(range(10))\\nplt.tight_layout()\\nplt.show()\\n\\nprint(\\\"\\\\nüîç Understanding Feature Maps:\\\")\\nprint(\\\"Feature maps show what each filter detects:\\\")\\nprint(\\\"  ‚Ä¢ Bright areas = filter strongly activated\\\")\\nprint(\\\"  ‚Ä¢ Dark areas = filter not activated\\\")\\nprint(\\\"  ‚Ä¢ Different filters detect different patterns\\\")\\n\\n# Visualize first convolutional layer\\nprint(\\\"\\\\nFirst Convolutional Layer - Basic Features (edges, corners):\\\")\\nfeature_maps_1 = visualize_feature_maps(cnn_model, sample_image, 'conv2d')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing CNN Features\\n\\nLet's see what the CNN actually learns to detect:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Understanding CNN Layers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Let's build a simple CNN and examine each layer\n",
    "simple_cnn = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(10, activation='softmax', name='classifier')\n",
    "])\n",
    "\n",
    "print(\"Layer-by-layer analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Trace through the network\n",
    "input_shape = (28, 28, 1)\n",
    "print(f\"Input: {input_shape} ‚Üí Raw 28x28 grayscale image\")\n",
    "\n",
    "for i, layer in enumerate(simple_cnn.layers):\n",
    "    # Build the model up to this layer to see output shape\n",
    "    temp_model = keras.Sequential(simple_cnn.layers[:i+1])\n",
    "    temp_model.build(input_shape=(None,) + input_shape)\n",
    "    output_shape = temp_model.output_shape[1:]  # Remove batch dimension\n",
    "    \n",
    "    if 'conv' in layer.name:\n",
    "        filters = layer.filters\n",
    "        kernel_size = layer.kernel_size\n",
    "        print(f\"{layer.name}: {output_shape} ‚Üí {filters} filters of {kernel_size} detecting features\")\n",
    "    elif 'pool' in layer.name:\n",
    "        pool_size = layer.pool_size\n",
    "        print(f\"{layer.name}: {output_shape} ‚Üí Downsample by {pool_size}, keep strongest signals\")\n",
    "    elif 'flatten' in layer.name:\n",
    "        print(f\"{layer.name}: {output_shape} ‚Üí Convert 2D feature maps to 1D vector\")\n",
    "    elif 'dense' in layer.name or 'classifier' in layer.name:\n",
    "        units = layer.units\n",
    "        print(f\"{layer.name}: {output_shape} ‚Üí Final classification into {units} classes\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(\"  ‚Ä¢ Each Conv2D layer detects increasingly complex features\")\n",
    "print(\"  ‚Ä¢ MaxPooling reduces size while keeping important information\")\n",
    "print(\"  ‚Ä¢ Flatten converts spatial features to vector for classification\")\n",
    "print(\"  ‚Ä¢ Final Dense layer maps features to class probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding CNN Components\\n\\nLet's dive deeper into what makes CNNs work so well:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The CNN Approach:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# For CNNs, we keep the spatial structure\n",
    "print(f\"CNN input shape: {X_train.shape[1:]} (height, width, channels)\")\n",
    "print(f\"We DON'T flatten the images!\")\n",
    "\n",
    "# Reshape for CNN (add channel dimension)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "print(f\"CNN data shape: {X_train_cnn.shape}\")\n",
    "\n",
    "# Build a CNN\n",
    "cnn_model = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second convolutional block  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Flatten and classify\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='CNN_Network')\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nCNN Model Summary:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters in CNN model: {cnn_model.count_params():,}\")\n",
    "print(\"\\n‚úÖ Advantages of CNN approach:\")\n",
    "print(\"  ‚Ä¢ Preserves spatial relationships\")\n",
    "print(\"  ‚Ä¢ Parameter sharing (same filter across image)\")\n",
    "print(\"  ‚Ä¢ Translation invariant\")\n",
    "print(\"  ‚Ä¢ Hierarchical feature learning\")\n",
    "print(\"  ‚Ä¢ Fewer parameters than Dense\")\n",
    "\n",
    "# Train the CNN\n",
    "print(\"\\nTraining CNN (this will take a moment...)\")\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    validation_data=(X_test_cnn, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_accuracy = cnn_history.history['val_accuracy'][-1]\n",
    "print(f\"CNN accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "# Compare the results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dense Network Accuracy: {dense_accuracy:.4f}\")\n",
    "print(f\"CNN Network Accuracy:   {cnn_accuracy:.4f}\")\n",
    "print(f\"Improvement: {((cnn_accuracy - dense_accuracy) * 100):.2f} percentage points\")\n",
    "print(f\"Parameters - Dense: {dense_model.count_params():,}\")\n",
    "print(f\"Parameters - CNN:   {cnn_model.count_params():,}\")\n",
    "print(\"\\nüéâ CNN wins with better accuracy AND fewer parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The CNN Solution - Convolutional Layers\\n\\nNow let's build a CNN and see why it's so much better for image data:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Problem with Dense Layers for Images\n",
    "\n",
    "Let's first see what happens when we use a regular Dense network on image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Dense Network Approach:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# For Dense layers, we need to flatten images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Original image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape[1:]}\")\n",
    "print(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n",
    "\n",
    "# Build a Dense network\n",
    "dense_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='Dense_Network')\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nDense Model Summary:\")\n",
    "dense_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\n",
    "print(\"\\n‚ùó Problems with Dense approach:\")\n",
    "print(\"  ‚Ä¢ Loses spatial information (pixel relationships)\")\n",
    "print(\"  ‚Ä¢ Treats each pixel independently\")\n",
    "print(\"  ‚Ä¢ Lots of parameters (prone to overfitting)\")\n",
    "print(\"  ‚Ä¢ Not translation invariant\")\n",
    "print(\"  ‚Ä¢ Computationally expensive\")\n",
    "\n",
    "# Train briefly\n",
    "print(\"\\nTraining Dense network (this may take a moment...)\")\n",
    "dense_history = dense_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    validation_data=(X_test_flat, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dense_accuracy = dense_history.history['val_accuracy'][-1]\n",
    "print(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}