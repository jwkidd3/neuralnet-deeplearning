{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3: Introduction to Convolutional Neural Networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand why CNNs are perfect for image data\n",
    "- Build your first CNN using TensorFlow/Keras\n",
    "- Compare CNN performance with regular Dense networks\n",
    "- Use Conv2D, MaxPooling, and Flatten layers\n",
    "- Apply CNNs to real image classification problems\n",
    "\n",
    "## Prerequisites\n",
    "- **Lab 4.1 & 4.2 completed** (TensorFlow basics and deep networks)\n",
    "- Understanding of neural networks\n",
    "- Basic knowledge of images as data\n",
    "\n",
    "## Key Concepts\n",
    "- **Convolution**: Feature detection using filters/kernels\n",
    "- **Pooling**: Dimensionality reduction and translation invariance\n",
    "- **Feature Maps**: How CNNs detect patterns at different scales\n",
    "- **Spatial Hierarchy**: Learning from simple edges to complex objects\n",
    "- **Parameter Sharing**: Why CNNs need fewer parameters than Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Introduction\n",
    "\n",
    "Let's start by understanding why we need CNNs for image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab 4.3: Introduction to Convolutional Neural Networks\n",
      "============================================================\n",
      "TensorFlow version: 2.20.0\n",
      "\n",
      "Loading MNIST dataset...\n",
      "Training data shape: (60000, 28, 28)\n",
      "Training labels shape: (60000,)\n",
      "Test data shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n",
      "Number of classes: 10\n",
      "\n",
      "ğŸ“Š Sample MNIST Images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data loaded and ready for CNN experiments!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Lab 4.3: Introduction to Convolutional Neural Networks\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "print(\"\\nLoading MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Visualize some sample images\n",
    "print(\"\\nğŸ“Š Sample MNIST Images:\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Data loaded and ready for CNN experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dense Network Approach:\n",
      "========================================\n",
      "Original image shape: (28, 28)\n",
      "Flattened shape: (784,)\n",
      "Total pixels per image: 784\n",
      "\n",
      "Dense Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Dense_Network\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Dense_Network\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_31 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m100,480\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_32 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_33 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m650\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters in Dense model: 109,386\n",
      "\n",
      "â— Problems with Dense approach:\n",
      "  â€¢ Loses spatial information (pixel relationships)\n",
      "  â€¢ Treats each pixel independently\n",
      "  â€¢ Lots of parameters (prone to overfitting)\n",
      "  â€¢ Not translation invariant\n",
      "  â€¢ Computationally expensive\n",
      "\n",
      "Training Dense network (this may take a moment...)\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - accuracy: 0.9083 - loss: 0.3302 - val_accuracy: 0.9532 - val_loss: 0.1557\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9607 - loss: 0.1333 - val_accuracy: 0.9652 - val_loss: 0.1087\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.9729 - loss: 0.0929 - val_accuracy: 0.9710 - val_loss: 0.0911\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.9791 - loss: 0.0704 - val_accuracy: 0.9725 - val_loss: 0.0861\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - accuracy: 0.9836 - loss: 0.0549 - val_accuracy: 0.9744 - val_loss: 0.0839\n",
      "Dense network accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "print(\"The Dense Network Approach:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# For Dense layers, we need to flatten images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Original image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape[1:]}\")\n",
    "print(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n",
    "\n",
    "# Build a Dense network\n",
    "dense_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='Dense_Network')\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nDense Model Summary:\")\n",
    "dense_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\n",
    "print(\"\\nâ— Problems with Dense approach:\")\n",
    "print(\"  â€¢ Loses spatial information (pixel relationships)\")\n",
    "print(\"  â€¢ Treats each pixel independently\")\n",
    "print(\"  â€¢ Lots of parameters (prone to overfitting)\")\n",
    "print(\"  â€¢ Not translation invariant\")\n",
    "print(\"  â€¢ Computationally expensive\")\n",
    "\n",
    "# Train briefly\n",
    "print(\"\\nTraining Dense network (this may take a moment...)\")\n",
    "dense_history = dense_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    validation_data=(X_test_flat, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dense_accuracy = dense_history.history['val_accuracy'][-1]\n",
    "print(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Visualizing CNN Features\n\nNow let's visualize what the CNN actually learns to detect:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Understanding CNN Layer Architecture\n\nLet's see what CNN actually learns and how layers work together:"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Experimenting with Different CNN Architectures\n\nLet's try different CNN designs and see how architecture choices affect performance:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Experimenting with Different CNN Architectures:\")\nprint(\"=\" * 60)\n\n# Architecture 1: Shallow CNN (fewer layers)\nshallow_cnn = keras.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(10, activation='softmax')\n], name='Shallow_CNN')\n\n# Architecture 2: Deep CNN (more layers)\ndeep_cnn = keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n], name='Deep_CNN')\n\n# Architecture 3: CNN with larger filters\nlarge_filter_cnn = keras.Sequential([\n    layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (5, 5), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name='Large_Filter_CNN')\n\n# Compare architectures\narchitectures = {\n    'Shallow CNN': shallow_cnn,\n    'Deep CNN': deep_cnn,\n    'Large Filter CNN': large_filter_cnn,\n    'Original CNN': cnn_model\n}\n\nprint(\"\\nğŸ“Š Architecture Comparison:\")\nprint(\"-\" * 40)\nfor name, model in architectures.items():\n    # Build model to get parameter count\n    if name != 'Original CNN':  # Original is already built\n        model.build(input_shape=(None, 28, 28, 1))\n    \n    total_params = model.count_params()\n    conv_layers = sum(1 for layer in model.layers if 'conv' in layer.__class__.__name__.lower())\n    \n    print(f\"\\n{name}:\")\n    print(f\"  â€¢ Total parameters: {total_params:,}\")\n    print(f\"  â€¢ Conv layers: {conv_layers}\")\n    print(f\"  â€¢ Architecture depth: {len(model.layers)} layers\")\n\n# Train and compare the shallow model\nprint(\"\\nğŸ”¬ Training Shallow CNN for comparison...\")\nshallow_cnn.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nshallow_history = shallow_cnn.fit(\n    X_train_cnn, y_train,\n    validation_data=(X_test_cnn, y_test),\n    epochs=3,\n    batch_size=128,\n    verbose=0\n)\n\nshallow_accuracy = max(shallow_history.history['val_accuracy'])\n\nprint(f\"\\nPerformance Comparison:\")\nprint(f\"  â€¢ Dense Network: {dense_accuracy:.4f}\")\nprint(f\"  â€¢ Shallow CNN: {shallow_accuracy:.4f}\")\nprint(f\"  â€¢ Original CNN: {cnn_accuracy:.4f}\")\n\nprint(\"\\nğŸ’¡ Key Insights:\")\nprint(\"  â€¢ Even shallow CNNs outperform dense networks\")\nprint(\"  â€¢ Deeper networks can learn more complex features\")\nprint(\"  â€¢ Filter size affects receptive field\")\nprint(\"  â€¢ More parameters â‰  always better (risk of overfitting)\")\n\n# Visualize the architectures side by side\nplt.figure(figsize=(15, 8))\n\n# Plot training curves comparison\nplt.subplot(2, 2, 1)\nplt.plot(dense_history.history['val_accuracy'], label='Dense Network', linewidth=2)\nplt.plot(shallow_history.history['val_accuracy'], label='Shallow CNN', linewidth=2)\nplt.plot(cnn_history.history['val_accuracy'][:3], label='Original CNN', linewidth=2)\nplt.title('Validation Accuracy Comparison')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot parameter counts\nplt.subplot(2, 2, 2)\nmodel_names = ['Dense', 'Shallow CNN', 'Original CNN']\nparam_counts = [dense_model.count_params(), shallow_cnn.count_params(), cnn_model.count_params()]\ncolors = ['red', 'yellow', 'green']\nbars = plt.bar(model_names, param_counts, color=colors, alpha=0.7)\nplt.title('Parameter Count Comparison')\nplt.ylabel('Number of Parameters')\nfor bar, count in zip(bars, param_counts):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,\n             f'{count:,}', ha='center', fontweight='bold')\nplt.grid(axis='y', alpha=0.3)\n\n# Show sample predictions\nplt.subplot(2, 2, 3)\nsample_idx = 10\nsample_img = X_test_cnn[sample_idx:sample_idx+1]\nplt.imshow(sample_img.squeeze(), cmap='gray')\nplt.title(f'Test Image (True Label: {y_test[sample_idx]})')\nplt.axis('off')\n\n# Prediction comparison\nplt.subplot(2, 2, 4)\ndense_pred = dense_model.predict(X_test_flat[sample_idx:sample_idx+1], verbose=0)[0]\nshallow_pred = shallow_cnn.predict(sample_img, verbose=0)[0]\ncnn_pred = cnn_model.predict(sample_img, verbose=0)[0]\n\nx = np.arange(10)\nwidth = 0.25\nplt.bar(x - width, dense_pred, width, label='Dense', alpha=0.7)\nplt.bar(x, shallow_pred, width, label='Shallow CNN', alpha=0.7)\nplt.bar(x + width, cnn_pred, width, label='Original CNN', alpha=0.7)\nplt.xlabel('Digit Class')\nplt.ylabel('Probability')\nplt.title('Prediction Comparison')\nplt.legend()\nplt.xticks(x)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nğŸ¯ Conclusion: CNNs consistently outperform Dense networks on image data!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing CNN Features\\n\\nLet's see what the CNN actually learns to detect:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding CNN Layers:\n",
      "==================================================\n",
      "Layer-by-layer analysis:\n",
      "------------------------------\n",
      "Input: (28, 28, 1) â†’ Raw 28x28 grayscale image\n",
      "conv1: (26, 26, 8) â†’ 8 filters of (3, 3) detecting features\n",
      "pool1: (13, 13, 8) â†’ Downsample by (2, 2), keep strongest signals\n",
      "conv2: (11, 11, 16) â†’ 16 filters of (3, 3) detecting features\n",
      "pool2: (5, 5, 16) â†’ Downsample by (2, 2), keep strongest signals\n",
      "flatten: (400,) â†’ Convert 2D feature maps to 1D vector\n",
      "classifier: (10,) â†’ Final classification into 10 classes\n",
      "\n",
      "ğŸ” Key Insights:\n",
      "  â€¢ Each Conv2D layer detects increasingly complex features\n",
      "  â€¢ MaxPooling reduces size while keeping important information\n",
      "  â€¢ Flatten converts spatial features to vector for classification\n",
      "  â€¢ Final Dense layer maps features to class probabilities\n"
     ]
    }
   ],
   "source": [
    "print(\"Understanding CNN Layers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Let's build a simple CNN and examine each layer\n",
    "simple_cnn = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(10, activation='softmax', name='classifier')\n",
    "])\n",
    "\n",
    "print(\"Layer-by-layer analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Trace through the network\n",
    "input_shape = (28, 28, 1)\n",
    "print(f\"Input: {input_shape} â†’ Raw 28x28 grayscale image\")\n",
    "\n",
    "for i, layer in enumerate(simple_cnn.layers):\n",
    "    # Build the model up to this layer to see output shape\n",
    "    temp_model = keras.Sequential(simple_cnn.layers[:i+1])\n",
    "    temp_model.build(input_shape=(None,) + input_shape)\n",
    "    output_shape = temp_model.output_shape[1:]  # Remove batch dimension\n",
    "    \n",
    "    if 'conv' in layer.name:\n",
    "        filters = layer.filters\n",
    "        kernel_size = layer.kernel_size\n",
    "        print(f\"{layer.name}: {output_shape} â†’ {filters} filters of {kernel_size} detecting features\")\n",
    "    elif 'pool' in layer.name:\n",
    "        pool_size = layer.pool_size\n",
    "        print(f\"{layer.name}: {output_shape} â†’ Downsample by {pool_size}, keep strongest signals\")\n",
    "    elif 'flatten' in layer.name:\n",
    "        print(f\"{layer.name}: {output_shape} â†’ Convert 2D feature maps to 1D vector\")\n",
    "    elif 'dense' in layer.name or 'classifier' in layer.name:\n",
    "        units = layer.units\n",
    "        print(f\"{layer.name}: {output_shape} â†’ Final classification into {units} classes\")\n",
    "\n",
    "print(\"\\nğŸ” Key Insights:\")\n",
    "print(\"  â€¢ Each Conv2D layer detects increasingly complex features\")\n",
    "print(\"  â€¢ MaxPooling reduces size while keeping important information\")\n",
    "print(\"  â€¢ Flatten converts spatial features to vector for classification\")\n",
    "print(\"  â€¢ Final Dense layer maps features to class probabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 2: The CNN Solution - Convolutional Layers\n\nNow let's build a CNN and see why it's so much better for image data:"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dense Network Approach:\n",
      "========================================\n",
      "Original image shape: (28, 28)\n",
      "Flattened shape: (784,)\n",
      "Total pixels per image: 784\n",
      "\n",
      "Dense Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Dense_Network\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Dense_Network\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_36 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m100,480\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_37 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_38 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m650\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters in Dense model: 109,386\n",
      "\n",
      "â— Problems with Dense approach:\n",
      "  â€¢ Loses spatial information (pixel relationships)\n",
      "  â€¢ Treats each pixel independently\n",
      "  â€¢ Lots of parameters (prone to overfitting)\n",
      "  â€¢ Not translation invariant\n",
      "  â€¢ Computationally expensive\n",
      "\n",
      "Training Dense network (this may take a moment...)\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9067 - loss: 0.3311 - val_accuracy: 0.9528 - val_loss: 0.1589\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9616 - loss: 0.1321 - val_accuracy: 0.9659 - val_loss: 0.1096\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.9736 - loss: 0.0904 - val_accuracy: 0.9728 - val_loss: 0.0882\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.9802 - loss: 0.0677 - val_accuracy: 0.9746 - val_loss: 0.0798\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - accuracy: 0.9847 - loss: 0.0523 - val_accuracy: 0.9750 - val_loss: 0.0795\n",
      "Dense network accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "print(\"The Dense Network Approach:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# For Dense layers, we need to flatten images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Original image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape[1:]}\")\n",
    "print(f\"Total pixels per image: {X_train_flat.shape[1]}\")\n",
    "\n",
    "# Build a Dense network\n",
    "dense_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='Dense_Network')\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nDense Model Summary:\")\n",
    "dense_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters in Dense model: {dense_model.count_params():,}\")\n",
    "print(\"\\nâ— Problems with Dense approach:\")\n",
    "print(\"  â€¢ Loses spatial information (pixel relationships)\")\n",
    "print(\"  â€¢ Treats each pixel independently\")\n",
    "print(\"  â€¢ Lots of parameters (prone to overfitting)\")\n",
    "print(\"  â€¢ Not translation invariant\")\n",
    "print(\"  â€¢ Computationally expensive\")\n",
    "\n",
    "# Train briefly\n",
    "print(\"\\nTraining Dense network (this may take a moment...)\")\n",
    "dense_history = dense_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    validation_data=(X_test_flat, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dense_accuracy = dense_history.history['val_accuracy'][-1]\n",
    "print(f\"Dense network accuracy: {dense_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}