{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.3: Vanishing/Exploding Gradients with TensorFlow\n\n**Duration**: 45 minutes\n\n## Learning Objectives\nBy the end of this lab, you will be able to:\n- Identify vanishing and exploding gradient problems using TensorFlow tools\n- Use TensorFlow's built-in solutions (BatchNormalization, gradient clipping)\n- Monitor gradient flow with TensorFlow callbacks and metrics\n- Apply modern gradient stabilization techniques in production code\n- Compare manual understanding with TensorFlow's automated solutions\n\n## Prerequisites\n- Completed Labs 4.1 and 4.2\n- Understanding of gradient problems from manual implementation\n- Basic TensorFlow/Keras knowledge\n\n## Lab Overview\nThis lab demonstrates how TensorFlow provides built-in solutions for gradient problems we learned to diagnose and solve manually. You'll see how modern frameworks make gradient stabilization accessible while building on your foundational understanding."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Imports\n",
    "\n",
    "### Instructions:\n",
    "1. Run this cell to import all necessary libraries\n",
    "2. Verify all imports are successful\n",
    "3. If any imports fail, install missing packages using: `pip install numpy matplotlib scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks, optimizers\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Configure matplotlib\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"🚀 TensorFlow Gradient Analysis Lab Ready!\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MANUAL vs TENSORFLOW GRADIENT SOLUTIONS\")\nprint(\"=\"*60)\nprint(\"\"\"\nWhat we learned manually:\n📚 Gradient computation: dW = (1/m) * dZ * A_prev.T\n📚 Problem diagnosis: Checking ||dW|| across layers\n📚 Manual gradient clipping: scaling gradients by norm\n📚 Batch norm implementation: (X - μ) / σ normalization\n\nWhat TensorFlow provides:\n✅ tf.keras.layers.BatchNormalization() - Optimized implementation\n✅ tf.keras.utils.clip_gradients() - Built-in clipping\n✅ tf.keras.callbacks for gradient monitoring\n✅ Automatic gradient computation with GradientTape\n✅ Initialization strategies in layer constructors\n\nKey insight: Understanding the manual implementation helps you:\n💡 Debug when TensorFlow solutions don't work as expected\n💡 Customize gradient handling for specific use cases\n💡 Understand what's happening \"under the hood\"\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Gradient Flow\n",
    "\n",
    "### Instructions:\n",
    "1. Run the following cells to create a deep network architecture\n",
    "2. Observe how gradients change as they flow backward through layers\n",
    "3. Identify patterns that lead to vanishing or exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    \"\"\"Deep neural network with gradient tracking capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='sigmoid', init_type='random'):\n",
    "        \"\"\"\n",
    "        Initialize deep neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_dims: List of layer dimensions [input, hidden1, hidden2, ..., output]\n",
    "            activation: Activation function type ('sigmoid', 'tanh', 'relu')\n",
    "            init_type: Weight initialization type ('random', 'xavier', 'he')\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        self.activation = activation\n",
    "        self.init_type = init_type\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.parameters = self._initialize_parameters()\n",
    "        \n",
    "        # Storage for gradient analysis\n",
    "        self.gradient_norms = {}\n",
    "        self.activation_stats = {}\n",
    "        self.cache = {}\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize network parameters based on initialization type\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            n_prev = self.layer_dims[l-1]\n",
    "            n_curr = self.layer_dims[l]\n",
    "            \n",
    "            if self.init_type == 'random':\n",
    "                # Random initialization (prone to gradient problems)\n",
    "                parameters[f'W{l}'] = np.random.randn(n_curr, n_prev) * 0.01\n",
    "            elif self.init_type == 'xavier':\n",
    "                # Xavier initialization (good for sigmoid/tanh)\n",
    "                parameters[f'W{l}'] = np.random.randn(n_curr, n_prev) * np.sqrt(1.0 / n_prev)\n",
    "            elif self.init_type == 'he':\n",
    "                # He initialization (good for ReLU)\n",
    "                parameters[f'W{l}'] = np.random.randn(n_curr, n_prev) * np.sqrt(2.0 / n_prev)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown initialization type: {self.init_type}\")\n",
    "            \n",
    "            parameters[f'b{l}'] = np.zeros((n_curr, 1))\n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    def _activate(self, Z, activation):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def _activate_derivative(self, Z, activation):\n",
    "        \"\"\"Compute derivative of activation function\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            A = self._activate(Z, 'sigmoid')\n",
    "            return A * (1 - A)\n",
    "        elif activation == 'tanh':\n",
    "            A = self._activate(Z, 'tanh')\n",
    "            return 1 - A**2\n",
    "        elif activation == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation with activation tracking\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            # Choose activation for last layer (sigmoid) or hidden layers\n",
    "            if l == self.num_layers:\n",
    "                A = self._activate(Z, 'sigmoid')\n",
    "            else:\n",
    "                A = self._activate(Z, self.activation)\n",
    "            \n",
    "            # Store for backward propagation\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "            \n",
    "            # Track activation statistics\n",
    "            self.activation_stats[f'layer_{l}'] = {\n",
    "                'mean': np.mean(A),\n",
    "                'std': np.std(A),\n",
    "                'min': np.min(A),\n",
    "                'max': np.max(A)\n",
    "            }\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        \"\"\"Backward propagation with gradient tracking\"\"\"\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        \n",
    "        # Output layer gradient\n",
    "        AL = self.cache[f'A{self.num_layers}']\n",
    "        dAL = -(Y / (AL + 1e-8) - (1 - Y) / (1 - AL + 1e-8))\n",
    "        \n",
    "        # Backward through layers\n",
    "        dA = dAL\n",
    "        for l in reversed(range(1, self.num_layers + 1)):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            Z = self.cache[f'Z{l}']\n",
    "            W = self.parameters[f'W{l}']\n",
    "            \n",
    "            # Compute gradients\n",
    "            if l == self.num_layers:\n",
    "                dZ = dA * self._activate_derivative(Z, 'sigmoid')\n",
    "            else:\n",
    "                dZ = dA * self._activate_derivative(Z, self.activation)\n",
    "            \n",
    "            dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dA = np.dot(W.T, dZ)\n",
    "            \n",
    "            gradients[f'dW{l}'] = dW\n",
    "            gradients[f'db{l}'] = db\n",
    "            \n",
    "            # Track gradient norms\n",
    "            self.gradient_norms[f'layer_{l}'] = {\n",
    "                'dW_norm': np.linalg.norm(dW),\n",
    "                'db_norm': np.linalg.norm(db),\n",
    "                'dZ_norm': np.linalg.norm(dZ)\n",
    "            }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"Compute binary cross-entropy cost\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = -(1/m) * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "print(\"Deep Neural Network class created successfully!\")\n",
    "print(\"Features: Gradient tracking, multiple activation functions, various initialization methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Demonstrating Vanishing Gradients\n",
    "\n",
    "### Instructions:\n",
    "1. Create a deep network with sigmoid activation (prone to vanishing gradients)\n",
    "2. Observe how gradient magnitudes decrease through layers\n",
    "3. Analyze the impact on learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X_data, y_data = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                                     n_redundant=5, n_clusters_per_class=2,\n",
    "                                     random_state=42)\n",
    "X_data = X_data.T\n",
    "y_data = y_data.reshape(1, -1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data.T, y_data.T, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "y_train, y_test = y_train.T, y_test.T\n",
    "\n",
    "print(f\"Dataset created: {X_train.shape[1]} training samples, {X_test.shape[1]} test samples\")\n",
    "print(f\"Input features: {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_flow(network, X, Y, title=\"Gradient Flow Analysis\"):\n",
    "    \"\"\"Visualize gradient flow through network layers\"\"\"\n",
    "    # Perform forward and backward propagation\n",
    "    AL = network.forward_propagation(X)\n",
    "    gradients = network.backward_propagation(X, Y)\n",
    "    \n",
    "    # Extract gradient norms\n",
    "    layers = []\n",
    "    dW_norms = []\n",
    "    dZ_norms = []\n",
    "    \n",
    "    for l in range(1, network.num_layers + 1):\n",
    "        layers.append(f\"Layer {l}\")\n",
    "        dW_norms.append(network.gradient_norms[f'layer_{l}']['dW_norm'])\n",
    "        dZ_norms.append(network.gradient_norms[f'layer_{l}']['dZ_norm'])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Weight gradient norms\n",
    "    axes[0, 0].bar(layers, dW_norms, color='blue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Weight Gradient Norms by Layer')\n",
    "    axes[0, 0].set_ylabel('Gradient Norm')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Activation gradient norms\n",
    "    axes[0, 1].bar(layers, dZ_norms, color='green', alpha=0.7)\n",
    "    axes[0, 1].set_title('Activation Gradient Norms by Layer')\n",
    "    axes[0, 1].set_ylabel('Gradient Norm')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Activation statistics\n",
    "    layer_nums = list(range(1, network.num_layers + 1))\n",
    "    means = [network.activation_stats[f'layer_{l}']['mean'] for l in layer_nums]\n",
    "    stds = [network.activation_stats[f'layer_{l}']['std'] for l in layer_nums]\n",
    "    \n",
    "    axes[1, 0].errorbar(layer_nums, means, yerr=stds, marker='o', capsize=5)\n",
    "    axes[1, 0].set_title('Activation Statistics by Layer')\n",
    "    axes[1, 0].set_xlabel('Layer Number')\n",
    "    axes[1, 0].set_ylabel('Activation Mean ± Std')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Gradient flow ratio\n",
    "    gradient_ratios = [dW_norms[i] / dW_norms[0] if dW_norms[0] != 0 else 0 \n",
    "                      for i in range(len(dW_norms))]\n",
    "    axes[1, 1].plot(layer_nums, gradient_ratios, marker='s', color='red', linewidth=2)\n",
    "    axes[1, 1].set_title('Gradient Flow Ratio (Relative to First Layer)')\n",
    "    axes[1, 1].set_xlabel('Layer Number')\n",
    "    axes[1, 1].set_ylabel('Gradient Ratio')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return dW_norms, dZ_norms\n",
    "\n",
    "# Test with vanishing gradient prone network (sigmoid activation, random init)\n",
    "print(\"Creating deep network with sigmoid activation (prone to vanishing gradients)...\")\n",
    "vanishing_network = DeepNeuralNetwork(\n",
    "    layer_dims=[20, 50, 40, 30, 20, 10, 1],  # 6 hidden layers\n",
    "    activation='sigmoid',\n",
    "    init_type='random'\n",
    ")\n",
    "\n",
    "# Visualize initial gradient flow\n",
    "dW_norms, dZ_norms = visualize_gradient_flow(\n",
    "    vanishing_network, X_train[:, :100], y_train[:, :100],\n",
    "    title=\"Vanishing Gradients with Sigmoid Activation\"\n",
    ")\n",
    "\n",
    "print(\"\\nGradient Analysis:\")\n",
    "print(f\"First layer gradient norm: {dW_norms[0]:.6f}\")\n",
    "print(f\"Last layer gradient norm: {dW_norms[-1]:.6f}\")\n",
    "print(f\"Gradient decay ratio: {dW_norms[-1]/dW_norms[0]:.6f}\")\n",
    "\n",
    "if dW_norms[-1]/dW_norms[0] < 0.01:\n",
    "    print(\"\\n⚠️ WARNING: Severe vanishing gradient detected!\")\n",
    "    print(\"The network will have difficulty learning in early layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Demonstrating Exploding Gradients\n",
    "\n",
    "### Instructions:\n",
    "1. Create a network configuration that leads to exploding gradients\n",
    "2. Observe how gradient magnitudes increase exponentially\n",
    "3. Understand the numerical instability this causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplodingGradientNetwork(DeepNeuralNetwork):\n",
    "    \"\"\"Network configured to demonstrate exploding gradients\"\"\"\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize with large weights to cause exploding gradients\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            n_prev = self.layer_dims[l-1]\n",
    "            n_curr = self.layer_dims[l]\n",
    "            \n",
    "            # Intentionally large initialization\n",
    "            parameters[f'W{l}'] = np.random.randn(n_curr, n_prev) * 10\n",
    "            parameters[f'b{l}'] = np.zeros((n_curr, 1))\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "print(\"Creating network prone to exploding gradients...\")\n",
    "exploding_network = ExplodingGradientNetwork(\n",
    "    layer_dims=[20, 50, 40, 30, 20, 10, 1],\n",
    "    activation='tanh',\n",
    "    init_type='custom'  # Will use large weights\n",
    ")\n",
    "\n",
    "# Visualize gradient explosion\n",
    "try:\n",
    "    dW_norms_exp, dZ_norms_exp = visualize_gradient_flow(\n",
    "        exploding_network, X_train[:, :100], y_train[:, :100],\n",
    "        title=\"Exploding Gradients with Large Weight Initialization\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGradient Analysis:\")\n",
    "    print(f\"First layer gradient norm: {dW_norms_exp[0]:.2e}\")\n",
    "    print(f\"Last layer gradient norm: {dW_norms_exp[-1]:.2e}\")\n",
    "    print(f\"Gradient amplification ratio: {dW_norms_exp[0]/dW_norms_exp[-1]:.2e}\")\n",
    "    \n",
    "    if dW_norms_exp[0] > 1000:\n",
    "        print(\"\\n⚠️ WARNING: Exploding gradients detected!\")\n",
    "        print(\"The network parameters will become unstable during training.\")\n",
    "except:\n",
    "    print(\"\\n❌ Gradient computation failed due to numerical overflow!\")\n",
    "    print(\"This demonstrates the severe instability caused by exploding gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implementing Gradient Clipping\n",
    "\n",
    "### Instructions:\n",
    "1. Implement gradient clipping to prevent exploding gradients\n",
    "2. Compare different clipping strategies\n",
    "3. Observe the stabilization effect on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_norm=5.0, clip_type='norm'):\n",
    "    \"\"\"\n",
    "    Clip gradients to prevent explosion\n",
    "    \n",
    "    Args:\n",
    "        gradients: Dictionary of gradients\n",
    "        max_norm: Maximum allowed norm\n",
    "        clip_type: 'norm' for norm clipping, 'value' for value clipping\n",
    "    \n",
    "    Returns:\n",
    "        Clipped gradients and clipping statistics\n",
    "    \"\"\"\n",
    "    clipped_gradients = {}\n",
    "    clip_stats = {'original_norm': 0, 'clipped_norm': 0, 'clip_ratio': 1.0}\n",
    "    \n",
    "    if clip_type == 'norm':\n",
    "        # Calculate total gradient norm\n",
    "        total_norm = 0\n",
    "        for key in gradients:\n",
    "            total_norm += np.sum(gradients[key] ** 2)\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        clip_stats['original_norm'] = total_norm\n",
    "        \n",
    "        # Clip if necessary\n",
    "        clip_ratio = max_norm / (total_norm + 1e-8)\n",
    "        clip_ratio = min(clip_ratio, 1.0)\n",
    "        \n",
    "        for key in gradients:\n",
    "            clipped_gradients[key] = gradients[key] * clip_ratio\n",
    "        \n",
    "        clip_stats['clip_ratio'] = clip_ratio\n",
    "        clip_stats['clipped_norm'] = min(total_norm, max_norm)\n",
    "        \n",
    "    elif clip_type == 'value':\n",
    "        # Clip individual gradient values\n",
    "        for key in gradients:\n",
    "            clipped_gradients[key] = np.clip(gradients[key], -max_norm, max_norm)\n",
    "            clip_stats['original_norm'] += np.sum(gradients[key] ** 2)\n",
    "            clip_stats['clipped_norm'] += np.sum(clipped_gradients[key] ** 2)\n",
    "        \n",
    "        clip_stats['original_norm'] = np.sqrt(clip_stats['original_norm'])\n",
    "        clip_stats['clipped_norm'] = np.sqrt(clip_stats['clipped_norm'])\n",
    "        clip_stats['clip_ratio'] = clip_stats['clipped_norm'] / (clip_stats['original_norm'] + 1e-8)\n",
    "    \n",
    "    return clipped_gradients, clip_stats\n",
    "\n",
    "# Test gradient clipping\n",
    "print(\"Testing gradient clipping on exploding gradient network...\\n\")\n",
    "\n",
    "# Reinitialize network\n",
    "exploding_network = ExplodingGradientNetwork(\n",
    "    layer_dims=[20, 50, 40, 30, 20, 10, 1],\n",
    "    activation='tanh',\n",
    "    init_type='custom'\n",
    ")\n",
    "\n",
    "# Forward and backward pass\n",
    "AL = exploding_network.forward_propagation(X_train[:, :100])\n",
    "gradients = exploding_network.backward_propagation(X_train[:, :100], y_train[:, :100])\n",
    "\n",
    "# Apply different clipping strategies\n",
    "print(\"Gradient Clipping Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# No clipping\n",
    "original_norm = sum([np.sum(gradients[key]**2) for key in gradients])**0.5\n",
    "print(f\"Original gradient norm: {original_norm:.2e}\")\n",
    "\n",
    "# Norm clipping\n",
    "clipped_grads_norm, stats_norm = clip_gradients(gradients, max_norm=5.0, clip_type='norm')\n",
    "print(f\"\\nNorm clipping (max_norm=5.0):\")\n",
    "print(f\"  Clipped norm: {stats_norm['clipped_norm']:.2f}\")\n",
    "print(f\"  Clip ratio: {stats_norm['clip_ratio']:.4f}\")\n",
    "\n",
    "# Value clipping\n",
    "clipped_grads_value, stats_value = clip_gradients(gradients, max_norm=1.0, clip_type='value')\n",
    "print(f\"\\nValue clipping (max_value=1.0):\")\n",
    "print(f\"  Clipped norm: {stats_value['clipped_norm']:.2f}\")\n",
    "print(f\"  Clip ratio: {stats_value['clip_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Gradient clipping successfully stabilizes gradient magnitudes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Batch Normalization Implementation\n",
    "\n",
    "### Instructions:\n",
    "1. Implement batch normalization to stabilize gradient flow\n",
    "2. Compare network behavior with and without batch norm\n",
    "3. Observe the effect on activation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNetwork(DeepNeuralNetwork):\n",
    "    \"\"\"Neural network with batch normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu', init_type='he', use_batch_norm=True):\n",
    "        super().__init__(layer_dims, activation, init_type)\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "        # Initialize batch norm parameters\n",
    "        if use_batch_norm:\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.parameters[f'gamma{l}'] = np.ones((layer_dims[l], 1))\n",
    "                self.parameters[f'beta{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    def batch_normalize(self, Z, layer_num, mode='train'):\n",
    "        \"\"\"\n",
    "        Apply batch normalization\n",
    "        \n",
    "        Args:\n",
    "            Z: Pre-activation values\n",
    "            layer_num: Current layer number\n",
    "            mode: 'train' or 'test'\n",
    "        \"\"\"\n",
    "        if not self.use_batch_norm or layer_num == self.num_layers:\n",
    "            return Z\n",
    "        \n",
    "        # Compute batch statistics\n",
    "        mu = np.mean(Z, axis=1, keepdims=True)\n",
    "        var = np.var(Z, axis=1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        Z_norm = (Z - mu) / np.sqrt(var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        gamma = self.parameters[f'gamma{layer_num}']\n",
    "        beta = self.parameters[f'beta{layer_num}']\n",
    "        Z_tilde = gamma * Z_norm + beta\n",
    "        \n",
    "        # Store for backward pass\n",
    "        self.cache[f'Z_norm{layer_num}'] = Z_norm\n",
    "        self.cache[f'mu{layer_num}'] = mu\n",
    "        self.cache[f'var{layer_num}'] = var\n",
    "        \n",
    "        return Z_tilde\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation with batch normalization\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            # Apply batch normalization before activation\n",
    "            Z = self.batch_normalize(Z, l)\n",
    "            \n",
    "            # Apply activation\n",
    "            if l == self.num_layers:\n",
    "                A = self._activate(Z, 'sigmoid')\n",
    "            else:\n",
    "                A = self._activate(Z, self.activation)\n",
    "            \n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "            \n",
    "            # Track activation statistics\n",
    "            self.activation_stats[f'layer_{l}'] = {\n",
    "                'mean': np.mean(A),\n",
    "                'std': np.std(A),\n",
    "                'min': np.min(A),\n",
    "                'max': np.max(A)\n",
    "            }\n",
    "        \n",
    "        return A\n",
    "\n",
    "# Compare networks with and without batch normalization\n",
    "print(\"Creating networks for batch normalization comparison...\\n\")\n",
    "\n",
    "# Network without batch norm\n",
    "network_no_bn = BatchNormNetwork(\n",
    "    layer_dims=[20, 50, 40, 30, 20, 10, 1],\n",
    "    activation='relu',\n",
    "    init_type='he',\n",
    "    use_batch_norm=False\n",
    ")\n",
    "\n",
    "# Network with batch norm\n",
    "network_with_bn = BatchNormNetwork(\n",
    "    layer_dims=[20, 50, 40, 30, 20, 10, 1],\n",
    "    activation='relu',\n",
    "    init_type='he',\n",
    "    use_batch_norm=True\n",
    ")\n",
    "\n",
    "# Visualize both networks\n",
    "print(\"Network WITHOUT Batch Normalization:\")\n",
    "dW_no_bn, _ = visualize_gradient_flow(\n",
    "    network_no_bn, X_train[:, :100], y_train[:, :100],\n",
    "    title=\"Gradient Flow WITHOUT Batch Normalization\"\n",
    ")\n",
    "\n",
    "print(\"\\nNetwork WITH Batch Normalization:\")\n",
    "dW_with_bn, _ = visualize_gradient_flow(\n",
    "    network_with_bn, X_train[:, :100], y_train[:, :100],\n",
    "    title=\"Gradient Flow WITH Batch Normalization\"\n",
    ")\n",
    "\n",
    "# Compare gradient stability\n",
    "print(\"\\nGradient Stability Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Without Batch Norm - Gradient variance: {np.var(dW_no_bn):.4f}\")\n",
    "print(f\"With Batch Norm - Gradient variance: {np.var(dW_with_bn):.4f}\")\n",
    "print(f\"\\nImprovement factor: {np.var(dW_no_bn) / (np.var(dW_with_bn) + 1e-8):.2f}x\")\n",
    "print(\"\\n✅ Batch normalization significantly stabilizes gradient flow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training Comparison\n",
    "\n",
    "### Instructions:\n",
    "1. Train networks with different gradient stabilization techniques\n",
    "2. Compare convergence speed and final performance\n",
    "3. Analyze which techniques work best for your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, X_train, y_train, X_test, y_test, \n",
    "                  epochs=100, learning_rate=0.01, use_clipping=False, \n",
    "                  clip_norm=5.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Train neural network with optional gradient clipping\n",
    "    \n",
    "    Returns:\n",
    "        Training history (costs, accuracies)\n",
    "    \"\"\"\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        AL_train = network.forward_propagation(X_train)\n",
    "        train_cost = network.compute_cost(AL_train, y_train)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = network.backward_propagation(X_train, y_train)\n",
    "        \n",
    "        # Track gradient norm\n",
    "        grad_norm = sum([np.sum(gradients[key]**2) for key in gradients])**0.5\n",
    "        gradient_norms.append(grad_norm)\n",
    "        \n",
    "        # Apply gradient clipping if requested\n",
    "        if use_clipping:\n",
    "            gradients, _ = clip_gradients(gradients, max_norm=clip_norm, clip_type='norm')\n",
    "        \n",
    "        # Update parameters\n",
    "        for l in range(1, network.num_layers + 1):\n",
    "            network.parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "            network.parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "            \n",
    "            # Update batch norm parameters if they exist\n",
    "            if hasattr(network, 'use_batch_norm') and network.use_batch_norm and l < network.num_layers:\n",
    "                if f'dgamma{l}' in gradients:\n",
    "                    network.parameters[f'gamma{l}'] -= learning_rate * gradients[f'dgamma{l}']\n",
    "                    network.parameters[f'beta{l}'] -= learning_rate * gradients[f'dbeta{l}']\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        AL_test = network.forward_propagation(X_test)\n",
    "        test_cost = network.compute_cost(AL_test, y_test)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_pred = (AL_train > 0.5).astype(float)\n",
    "        test_pred = (AL_test > 0.5).astype(float)\n",
    "        train_acc = np.mean(train_pred == y_train) * 100\n",
    "        test_acc = np.mean(test_pred == y_test) * 100\n",
    "        \n",
    "        # Store history\n",
    "        train_costs.append(train_cost)\n",
    "        test_costs.append(test_cost)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Train Cost={train_cost:.4f}, \"\n",
    "                  f\"Test Cost={test_cost:.4f}, Train Acc={train_acc:.1f}%, \"\n",
    "                  f\"Test Acc={test_acc:.1f}%, Grad Norm={grad_norm:.2e}\")\n",
    "    \n",
    "    return {\n",
    "        'train_costs': train_costs,\n",
    "        'test_costs': test_costs,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'gradient_norms': gradient_norms\n",
    "    }\n",
    "\n",
    "# Train different network configurations\n",
    "print(\"Training networks with different gradient stabilization techniques...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration 1: Baseline (prone to vanishing gradients)\n",
    "print(\"\\n1. BASELINE (Sigmoid, Random Init, No Stabilization):\")\n",
    "baseline_network = DeepNeuralNetwork(\n",
    "    layer_dims=[20, 30, 20, 10, 1],\n",
    "    activation='sigmoid',\n",
    "    init_type='random'\n",
    ")\n",
    "baseline_history = train_network(\n",
    "    baseline_network, X_train, y_train, X_test, y_test,\n",
    "    epochs=100, learning_rate=0.1, use_clipping=False, verbose=True\n",
    ")\n",
    "\n",
    "# Configuration 2: Better initialization\n",
    "print(\"\\n2. IMPROVED INITIALIZATION (ReLU, He Init):\")\n",
    "better_init_network = DeepNeuralNetwork(\n",
    "    layer_dims=[20, 30, 20, 10, 1],\n",
    "    activation='relu',\n",
    "    init_type='he'\n",
    ")\n",
    "better_init_history = train_network(\n",
    "    better_init_network, X_train, y_train, X_test, y_test,\n",
    "    epochs=100, learning_rate=0.01, use_clipping=False, verbose=True\n",
    ")\n",
    "\n",
    "# Configuration 3: With gradient clipping\n",
    "print(\"\\n3. WITH GRADIENT CLIPPING (ReLU, He Init, Clipping):\")\n",
    "clipped_network = DeepNeuralNetwork(\n",
    "    layer_dims=[20, 30, 20, 10, 1],\n",
    "    activation='relu',\n",
    "    init_type='he'\n",
    ")\n",
    "clipped_history = train_network(\n",
    "    clipped_network, X_train, y_train, X_test, y_test,\n",
    "    epochs=100, learning_rate=0.01, use_clipping=True, clip_norm=5.0, verbose=True\n",
    ")\n",
    "\n",
    "# Configuration 4: With batch normalization\n",
    "print(\"\\n4. WITH BATCH NORMALIZATION (ReLU, He Init, BatchNorm):\")\n",
    "bn_network = BatchNormNetwork(\n",
    "    layer_dims=[20, 30, 20, 10, 1],\n",
    "    activation='relu',\n",
    "    init_type='he',\n",
    "    use_batch_norm=True\n",
    ")\n",
    "bn_history = train_network(\n",
    "    bn_network, X_train, y_train, X_test, y_test,\n",
    "    epochs=100, learning_rate=0.01, use_clipping=False, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Performance Comparison and Analysis\n",
    "\n",
    "### Instructions:\n",
    "1. Visualize training curves for all configurations\n",
    "2. Compare convergence speed and stability\n",
    "3. Draw conclusions about best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_comparison(histories, labels):\n",
    "    \"\"\"Compare training histories of different configurations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Training cost\n",
    "    for history, label in zip(histories, labels):\n",
    "        axes[0, 0].plot(history['train_costs'], label=label, linewidth=2)\n",
    "    axes[0, 0].set_title('Training Cost Over Time')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Cost')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Test accuracy\n",
    "    for history, label in zip(histories, labels):\n",
    "        axes[0, 1].plot(history['test_accuracies'], label=label, linewidth=2)\n",
    "    axes[0, 1].set_title('Test Accuracy Over Time')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Gradient norms\n",
    "    for history, label in zip(histories, labels):\n",
    "        axes[1, 0].plot(history['gradient_norms'], label=label, linewidth=2, alpha=0.7)\n",
    "    axes[1, 0].set_title('Gradient Norm Evolution')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Gradient Norm')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Final comparison bar chart\n",
    "    final_accuracies = [history['test_accuracies'][-1] for history in histories]\n",
    "    colors = ['red', 'orange', 'green', 'blue']\n",
    "    bars = axes[1, 1].bar(labels, final_accuracies, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_title('Final Test Accuracy Comparison')\n",
    "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 1].set_ylim([0, 100])\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, final_accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                       f'{acc:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle('Gradient Stabilization Techniques Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create comparison plots\n",
    "histories = [baseline_history, better_init_history, clipped_history, bn_history]\n",
    "labels = ['Baseline', 'Better Init', 'Gradient Clipping', 'Batch Norm']\n",
    "\n",
    "plot_training_comparison(histories, labels)\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Configuration':<20} {'Final Train Acc':<18} {'Final Test Acc':<18} {'Avg Gradient Norm'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for history, label in zip(histories, labels):\n",
    "    final_train_acc = history['train_accuracies'][-1]\n",
    "    final_test_acc = history['test_accuracies'][-1]\n",
    "    avg_grad_norm = np.mean(history['gradient_norms'])\n",
    "    print(f\"{label:<20} {final_train_acc:>15.1f}%  {final_test_acc:>15.1f}%  {avg_grad_norm:>18.2e}\")\n",
    "\n",
    "# Identify best configuration\n",
    "best_idx = np.argmax([h['test_accuracies'][-1] for h in histories])\n",
    "print(f\"\\n🏆 Best Configuration: {labels[best_idx]}\")\n",
    "print(f\"   Final Test Accuracy: {histories[best_idx]['test_accuracies'][-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Gradient Analysis\n",
    "\n",
    "### Instructions:\n",
    "1. Perform detailed gradient flow analysis\n",
    "2. Identify problem layers in deep networks\n",
    "3. Apply targeted solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_gradient_health(network, X, Y, threshold_vanishing=0.01, threshold_exploding=100):\n",
    "    \"\"\"\n",
    "    Comprehensive gradient health diagnosis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with diagnostic information\n",
    "    \"\"\"\n",
    "    # Forward and backward pass\n",
    "    AL = network.forward_propagation(X)\n",
    "    gradients = network.backward_propagation(X, Y)\n",
    "    \n",
    "    diagnosis = {\n",
    "        'healthy_layers': [],\n",
    "        'vanishing_layers': [],\n",
    "        'exploding_layers': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Analyze each layer\n",
    "    for l in range(1, network.num_layers + 1):\n",
    "        grad_norm = np.linalg.norm(gradients[f'dW{l}'])\n",
    "        \n",
    "        if grad_norm < threshold_vanishing:\n",
    "            diagnosis['vanishing_layers'].append(l)\n",
    "        elif grad_norm > threshold_exploding:\n",
    "            diagnosis['exploding_layers'].append(l)\n",
    "        else:\n",
    "            diagnosis['healthy_layers'].append(l)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if diagnosis['vanishing_layers']:\n",
    "        diagnosis['recommendations'].append(\n",
    "            f\"⚠️ Vanishing gradients detected in layers {diagnosis['vanishing_layers']}\\n\"\n",
    "            \"   Recommendations:\\n\"\n",
    "            \"   - Use ReLU or LeakyReLU activation instead of sigmoid/tanh\\n\"\n",
    "            \"   - Apply batch normalization\\n\"\n",
    "            \"   - Use residual connections for very deep networks\\n\"\n",
    "            \"   - Consider Xavier or He initialization\"\n",
    "        )\n",
    "    \n",
    "    if diagnosis['exploding_layers']:\n",
    "        diagnosis['recommendations'].append(\n",
    "            f\"⚠️ Exploding gradients detected in layers {diagnosis['exploding_layers']}\\n\"\n",
    "            \"   Recommendations:\\n\"\n",
    "            \"   - Apply gradient clipping (norm or value based)\\n\"\n",
    "            \"   - Reduce learning rate\\n\"\n",
    "            \"   - Use proper weight initialization\\n\"\n",
    "            \"   - Consider L2 regularization\"\n",
    "        )\n",
    "    \n",
    "    if not diagnosis['vanishing_layers'] and not diagnosis['exploding_layers']:\n",
    "        diagnosis['recommendations'].append(\n",
    "            \"✅ Gradient flow appears healthy!\\n\"\n",
    "            \"   Continue training with current configuration.\"\n",
    "        )\n",
    "    \n",
    "    # Calculate gradient flow efficiency\n",
    "    first_layer_norm = np.linalg.norm(gradients[f'dW1'])\n",
    "    last_layer_norm = np.linalg.norm(gradients[f'dW{network.num_layers}'])\n",
    "    \n",
    "    if last_layer_norm > 0:\n",
    "        flow_ratio = first_layer_norm / last_layer_norm\n",
    "    else:\n",
    "        flow_ratio = float('inf')\n",
    "    \n",
    "    diagnosis['flow_ratio'] = flow_ratio\n",
    "    diagnosis['gradient_norms'] = {f'layer_{l}': np.linalg.norm(gradients[f'dW{l}']) \n",
    "                                   for l in range(1, network.num_layers + 1)}\n",
    "    \n",
    "    return diagnosis\n",
    "\n",
    "# Test gradient health diagnosis on different networks\n",
    "print(\"Performing Gradient Health Diagnosis...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_configs = [\n",
    "    ('Sigmoid Network (Deep)', [20, 40, 30, 20, 10, 5, 1], 'sigmoid', 'random'),\n",
    "    ('ReLU Network (Optimized)', [20, 40, 30, 20, 10, 5, 1], 'relu', 'he'),\n",
    "    ('Tanh Network (Xavier)', [20, 40, 30, 20, 10, 5, 1], 'tanh', 'xavier')\n",
    "]\n",
    "\n",
    "for name, layers, activation, init in test_configs:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_network = DeepNeuralNetwork(layers, activation, init)\n",
    "    diagnosis = diagnose_gradient_health(test_network, X_train[:, :100], y_train[:, :100])\n",
    "    \n",
    "    print(f\"Healthy layers: {diagnosis['healthy_layers']}\")\n",
    "    print(f\"Vanishing gradient layers: {diagnosis['vanishing_layers']}\")\n",
    "    print(f\"Exploding gradient layers: {diagnosis['exploding_layers']}\")\n",
    "    print(f\"Gradient flow ratio (first/last): {diagnosis['flow_ratio']:.4f}\")\n",
    "    \n",
    "    for rec in diagnosis['recommendations']:\n",
    "        print(f\"\\n{rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Best Practices Summary\n",
    "\n",
    "### Instructions:\n",
    "1. Review the summary of techniques learned\n",
    "2. Understand when to apply each technique\n",
    "3. Complete the exercises to reinforce learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference implementation with all best practices\n",
    "class OptimizedDeepNetwork:\n",
    "    \"\"\"Production-ready deep network with gradient stabilization\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, config=None):\n",
    "        \"\"\"\n",
    "        Initialize optimized deep network\n",
    "        \n",
    "        Args:\n",
    "            layer_dims: List of layer dimensions\n",
    "            config: Dictionary with configuration options\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        \n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'activation': 'relu',\n",
    "            'output_activation': 'sigmoid',\n",
    "            'initialization': 'he',\n",
    "            'use_batch_norm': True,\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.2,\n",
    "            'gradient_clipping': True,\n",
    "            'clip_norm': 5.0,\n",
    "            'learning_rate': 0.001,\n",
    "            'learning_rate_decay': 0.99,\n",
    "            'regularization': 'l2',\n",
    "            'reg_lambda': 0.01\n",
    "        }\n",
    "        \n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        print(\"Optimized Deep Network Configuration:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in self.config.items():\n",
    "            print(f\"{key:<20}: {value}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"\\nNetwork Architecture: {layer_dims}\")\n",
    "        print(f\"Total Parameters: {self._count_parameters():,}\")\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        \"\"\"Count total number of parameters\"\"\"\n",
    "        total = 0\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            total += self.layer_dims[l] * self.layer_dims[l-1]  # Weights\n",
    "            total += self.layer_dims[l]  # Biases\n",
    "            if self.config['use_batch_norm'] and l < self.num_layers:\n",
    "                total += 2 * self.layer_dims[l]  # Gamma and beta\n",
    "        return total\n",
    "\n",
    "# Create optimized network\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING PRODUCTION-READY OPTIMIZED NETWORK\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "optimized_network = OptimizedDeepNetwork(\n",
    "    layer_dims=[20, 64, 32, 16, 8, 1],\n",
    "    config={\n",
    "        'activation': 'relu',\n",
    "        'initialization': 'he',\n",
    "        'use_batch_norm': True,\n",
    "        'gradient_clipping': True,\n",
    "        'clip_norm': 5.0,\n",
    "        'learning_rate': 0.001,\n",
    "        'regularization': 'l2',\n",
    "        'reg_lambda': 0.01\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions and Key Takeaways\n",
    "\n",
    "### Key Techniques for Gradient Stabilization:\n",
    "\n",
    "1. **Proper Weight Initialization**\n",
    "   - Xavier/Glorot: Best for sigmoid/tanh activations\n",
    "   - He: Best for ReLU and variants\n",
    "   - Avoid random initialization with very small or large values\n",
    "\n",
    "2. **Activation Function Selection**\n",
    "   - ReLU: Reduces vanishing gradients, but can cause dead neurons\n",
    "   - LeakyReLU/ELU: Addresses dead neuron problem\n",
    "   - Avoid sigmoid/tanh in hidden layers of deep networks\n",
    "\n",
    "3. **Gradient Clipping**\n",
    "   - Norm clipping: Preserves gradient direction\n",
    "   - Value clipping: Simple but can change gradient direction\n",
    "   - Essential for RNNs and very deep networks\n",
    "\n",
    "4. **Batch Normalization**\n",
    "   - Normalizes inputs to each layer\n",
    "   - Allows higher learning rates\n",
    "   - Reduces sensitivity to initialization\n",
    "\n",
    "5. **Architecture Considerations**\n",
    "   - Residual connections for very deep networks\n",
    "   - Skip connections to preserve gradient flow\n",
    "   - Careful layer width design\n",
    "\n",
    "### Troubleshooting Guide:\n",
    "\n",
    "| Problem | Symptoms | Solutions |\n",
    "|---------|----------|----------|\n",
    "| Vanishing Gradients | Slow/no learning in early layers, gradients → 0 | Use ReLU, batch norm, residual connections |\n",
    "| Exploding Gradients | NaN values, unstable training, gradients → ∞ | Gradient clipping, reduce learning rate, proper init |\n",
    "| Dead Neurons | Zero activations, no learning | LeakyReLU, careful learning rate, batch norm |\n",
    "| Slow Convergence | High loss persists, poor accuracy | Better initialization, normalize inputs, adjust architecture |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Complete! 🎉\n",
    "\n",
    "### What You've Learned:\n",
    "✅ Identified and diagnosed vanishing/exploding gradient problems  \n",
    "✅ Implemented gradient clipping techniques  \n",
    "✅ Applied batch normalization for stability  \n",
    "✅ Compared different initialization strategies  \n",
    "✅ Built production-ready gradient stabilization solutions  \n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different network depths and observe gradient behavior\n",
    "2. Try implementing Layer Normalization as an alternative to Batch Norm\n",
    "3. Explore gradient accumulation for large batch training\n",
    "4. Investigate adaptive learning rate methods (Adam, RMSprop)\n",
    "\n",
    "### Additional Challenges:\n",
    "1. Implement gradient checkpointing for memory-efficient training\n",
    "2. Create a custom gradient clipping strategy based on layer depth\n",
    "3. Build an automatic gradient health monitoring system\n",
    "4. Compare different normalization techniques (Group Norm, Instance Norm)\n",
    "\n",
    "### Cleanup:\n",
    "```python\n",
    "# Clear variables to free memory\n",
    "import gc\n",
    "gc.collect()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}