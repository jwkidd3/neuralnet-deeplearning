{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4: Weight Initialization Strategies for Deep Networks\n",
    "\n",
    "**Duration**: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the critical importance of proper weight initialization\n",
    "- Implement various initialization methods (Zero, Random, Xavier, He, etc.)\n",
    "- Analyze the impact of initialization on gradient flow and convergence\n",
    "- Choose appropriate initialization strategies for different activation functions\n",
    "- Design custom initialization schemes for specific architectures\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Labs 4.1, 4.2, and 4.3\n",
    "- Understanding of activation functions and backpropagation\n",
    "- Familiarity with gradient flow problems\n",
    "\n",
    "## Lab Overview\n",
    "Weight initialization is one of the most critical factors in successfully training deep neural networks. Poor initialization can lead to vanishing/exploding gradients, slow convergence, or complete training failure. This lab explores various initialization strategies and their mathematical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Mathematical Foundations\n",
    "\n",
    "### Instructions:\n",
    "1. Run this cell to import all necessary libraries\n",
    "2. Review the mathematical foundation for initialization strategies\n",
    "3. Verify all imports are successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Ready to explore weight initialization strategies!\")\n",
    "\n",
    "# Mathematical foundations for initialization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATHEMATICAL FOUNDATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Key Principles for Weight Initialization:\n",
    "\n",
    "1. VARIANCE PRESERVATION:\n",
    "   - Forward pass: Var(output) ≈ Var(input)\n",
    "   - Backward pass: Var(gradient) should remain stable\n",
    "\n",
    "2. XAVIER/GLOROT INITIALIZATION:\n",
    "   - For sigmoid/tanh: Var(W) = 1/n_in\n",
    "   - Uniform: W ~ U(-√(6/(n_in+n_out)), √(6/(n_in+n_out)))\n",
    "   - Normal: W ~ N(0, √(2/(n_in+n_out)))\n",
    "\n",
    "3. HE INITIALIZATION:\n",
    "   - For ReLU: Var(W) = 2/n_in\n",
    "   - Normal: W ~ N(0, √(2/n_in))\n",
    "   - Uniform: W ~ U(-√(6/n_in), √(6/n_in))\n",
    "\n",
    "4. LECUN INITIALIZATION:\n",
    "   - For SELU: Var(W) = 1/n_in\n",
    "   - Normal: W ~ N(0, √(1/n_in))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Initialization Strategies\n",
    "\n",
    "### Instructions:\n",
    "1. Implement various initialization methods\n",
    "2. Understand the mathematical reasoning behind each method\n",
    "3. Observe the statistical properties of different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    \"\"\"Comprehensive weight initialization class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        \"\"\"Zero initialization (not recommended for weights)\"\"\"\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones(shape):\n",
    "        \"\"\"Ones initialization (not recommended for weights)\"\"\"\n",
    "        return np.ones(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_small(shape, scale=0.01):\n",
    "        \"\"\"Small random values (traditional approach)\"\"\"\n",
    "        return np.random.randn(*shape) * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_large(shape, scale=1.0):\n",
    "        \"\"\"Large random values (problematic)\"\"\"\n",
    "        return np.random.randn(*shape) * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_normal(shape):\n",
    "        \"\"\"Xavier/Glorot normal initialization\"\"\"\n",
    "        n_in, n_out = shape[1], shape[0]\n",
    "        std = np.sqrt(2.0 / (n_in + n_out))\n",
    "        return np.random.randn(*shape) * std\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_uniform(shape):\n",
    "        \"\"\"Xavier/Glorot uniform initialization\"\"\"\n",
    "        n_in, n_out = shape[1], shape[0]\n",
    "        limit = np.sqrt(6.0 / (n_in + n_out))\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_normal(shape):\n",
    "        \"\"\"He normal initialization (for ReLU)\"\"\"\n",
    "        n_in = shape[1]\n",
    "        std = np.sqrt(2.0 / n_in)\n",
    "        return np.random.randn(*shape) * std\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_uniform(shape):\n",
    "        \"\"\"He uniform initialization (for ReLU)\"\"\"\n",
    "        n_in = shape[1]\n",
    "        limit = np.sqrt(6.0 / n_in)\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lecun_normal(shape):\n",
    "        \"\"\"LeCun normal initialization (for SELU)\"\"\"\n",
    "        n_in = shape[1]\n",
    "        std = np.sqrt(1.0 / n_in)\n",
    "        return np.random.randn(*shape) * std\n",
    "    \n",
    "    @staticmethod\n",
    "    def orthogonal(shape):\n",
    "        \"\"\"Orthogonal initialization (good for RNNs)\"\"\"\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Orthogonal initialization only supports 2D arrays\")\n",
    "        \n",
    "        # Generate random matrix\n",
    "        a = np.random.randn(*shape)\n",
    "        \n",
    "        # SVD decomposition\n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        \n",
    "        # Pick the one with the correct shape\n",
    "        q = u if u.shape == shape else v\n",
    "        return q.reshape(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def variance_scaling(shape, scale=1.0, mode='fan_in', distribution='normal'):\n",
    "        \"\"\"General variance scaling initialization\"\"\"\n",
    "        n_in, n_out = shape[1], shape[0]\n",
    "        \n",
    "        if mode == 'fan_in':\n",
    "            n = n_in\n",
    "        elif mode == 'fan_out':\n",
    "            n = n_out\n",
    "        elif mode == 'fan_avg':\n",
    "            n = (n_in + n_out) / 2.0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "        \n",
    "        if distribution == 'normal':\n",
    "            std = np.sqrt(scale / n)\n",
    "            return np.random.randn(*shape) * std\n",
    "        elif distribution == 'uniform':\n",
    "            limit = np.sqrt(3.0 * scale / n)\n",
    "            return np.random.uniform(-limit, limit, shape)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid distribution: {distribution}\")\n",
    "\n",
    "# Test initialization methods\n",
    "test_shape = (64, 128)  # (n_out, n_in)\n",
    "print(f\"Testing initialization methods for shape {test_shape}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "initializers = {\n",
    "    'Random Small (0.01)': lambda: WeightInitializer.random_small(test_shape, 0.01),\n",
    "    'Random Large (1.0)': lambda: WeightInitializer.random_large(test_shape, 1.0),\n",
    "    'Xavier Normal': lambda: WeightInitializer.xavier_normal(test_shape),\n",
    "    'Xavier Uniform': lambda: WeightInitializer.xavier_uniform(test_shape),\n",
    "    'He Normal': lambda: WeightInitializer.he_normal(test_shape),\n",
    "    'He Uniform': lambda: WeightInitializer.he_uniform(test_shape),\n",
    "    'LeCun Normal': lambda: WeightInitializer.lecun_normal(test_shape),\n",
    "    'Orthogonal': lambda: WeightInitializer.orthogonal(test_shape)\n",
    "}\n",
    "\n",
    "# Analyze statistical properties\n",
    "for name, init_func in initializers.items():\n",
    "    weights = init_func()\n",
    "    print(f\"{name:<20}: Mean={np.mean(weights):.6f}, Std={np.std(weights):.6f}, \"\n",
    "          f\"Min={np.min(weights):.6f}, Max={np.max(weights):.6f}\")\n",
    "\n",
    "print(\"\\n✅ All initialization methods implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing Weight Distributions\n",
    "\n",
    "### Instructions:\n",
    "1. Create comprehensive visualizations of weight distributions\n",
    "2. Compare how different initializations affect the distribution shape\n",
    "3. Understand the relationship between initialization and activation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_distributions(shape=(100, 200), num_samples=5000):\n",
    "    \"\"\"Visualize weight distributions for different initialization methods\"\"\"\n",
    "    \n",
    "    # Select key initializations to compare\n",
    "    init_methods = {\n",
    "        'Random Small (0.01)': lambda: WeightInitializer.random_small(shape, 0.01),\n",
    "        'Random Large (1.0)': lambda: WeightInitializer.random_large(shape, 1.0),\n",
    "        'Xavier Normal': lambda: WeightInitializer.xavier_normal(shape),\n",
    "        'He Normal': lambda: WeightInitializer.he_normal(shape),\n",
    "        'LeCun Normal': lambda: WeightInitializer.lecun_normal(shape),\n",
    "        'Orthogonal': lambda: WeightInitializer.orthogonal(shape)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['red', 'orange', 'green', 'blue', 'purple', 'brown']\n",
    "    \n",
    "    for idx, (name, init_func) in enumerate(init_methods.items()):\n",
    "        # Generate weights\n",
    "        weights = init_func().flatten()\n",
    "        \n",
    "        # Create histogram\n",
    "        axes[idx].hist(weights, bins=50, alpha=0.7, color=colors[idx], \n",
    "                       density=True, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean = np.mean(weights)\n",
    "        std = np.std(weights)\n",
    "        axes[idx].axvline(mean, color='red', linestyle='--', alpha=0.8, linewidth=2, label=f'Mean: {mean:.3f}')\n",
    "        axes[idx].axvline(mean + std, color='orange', linestyle=':', alpha=0.8, label=f'±1 Std: {std:.3f}')\n",
    "        axes[idx].axvline(mean - std, color='orange', linestyle=':', alpha=0.8)\n",
    "        \n",
    "        axes[idx].set_title(f'{name}\\n(Shape: {shape})', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Weight Value')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].legend(fontsize=9)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Weight Distribution Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize distributions\n",
    "visualize_weight_distributions()\n",
    "\n",
    "# Create theoretical comparison\n",
    "def theoretical_vs_empirical_comparison():\n",
    "    \"\"\"Compare theoretical and empirical standard deviations\"\"\"\n",
    "    shape = (64, 128)  # (n_out, n_in)\n",
    "    n_in, n_out = shape[1], shape[0]\n",
    "    \n",
    "    print(\"\\nTheoretical vs Empirical Standard Deviations:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Network layer shape: {shape} (n_out={n_out}, n_in={n_in})\")\n",
    "    print()\n",
    "    \n",
    "    # Xavier Normal\n",
    "    xavier_theoretical = np.sqrt(2.0 / (n_in + n_out))\n",
    "    xavier_empirical = np.std(WeightInitializer.xavier_normal(shape))\n",
    "    print(f\"Xavier Normal:  Theoretical={xavier_theoretical:.6f}, Empirical={xavier_empirical:.6f}\")\n",
    "    \n",
    "    # He Normal\n",
    "    he_theoretical = np.sqrt(2.0 / n_in)\n",
    "    he_empirical = np.std(WeightInitializer.he_normal(shape))\n",
    "    print(f\"He Normal:      Theoretical={he_theoretical:.6f}, Empirical={he_empirical:.6f}\")\n",
    "    \n",
    "    # LeCun Normal\n",
    "    lecun_theoretical = np.sqrt(1.0 / n_in)\n",
    "    lecun_empirical = np.std(WeightInitializer.lecun_normal(shape))\n",
    "    print(f\"LeCun Normal:   Theoretical={lecun_theoretical:.6f}, Empirical={lecun_empirical:.6f}\")\n",
    "    \n",
    "    print(\"\\n✅ Empirical values closely match theoretical predictions!\")\n",
    "\n",
    "theoretical_vs_empirical_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Impact on Activation Statistics\n",
    "\n",
    "### Instructions:\n",
    "1. Create a deep network to analyze activation statistics\n",
    "2. Compare how different initializations affect activation distributions\n",
    "3. Understand the connection between initialization and gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationAnalyzer:\n",
    "    \"\"\"Analyze activation statistics for different initializations\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu'):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        self.activation = activation\n",
    "        self.activations = {}\n",
    "        self.activation_stats = {}\n",
    "    \n",
    "    def initialize_network(self, init_method):\n",
    "        \"\"\"Initialize network with specified method\"\"\"\n",
    "        self.parameters = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            shape = (self.layer_dims[l], self.layer_dims[l-1])\n",
    "            \n",
    "            if init_method == 'random_small':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.random_small(shape, 0.01)\n",
    "            elif init_method == 'random_large':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.random_large(shape, 1.0)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.xavier_normal(shape)\n",
    "            elif init_method == 'he_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.he_normal(shape)\n",
    "            elif init_method == 'lecun_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.lecun_normal(shape)\n",
    "            elif init_method == 'orthogonal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.orthogonal(shape)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown initialization: {init_method}\")\n",
    "            \n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "    \n",
    "    def activate(self, Z):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        elif self.activation == 'linear':\n",
    "            return Z\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"Forward pass with activation tracking\"\"\"\n",
    "        self.activations = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            # Apply activation (sigmoid for output layer, specified for others)\n",
    "            if l == self.num_layers:\n",
    "                A = self.activate(Z) if self.activation != 'sigmoid' else 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "            else:\n",
    "                A = self.activate(Z)\n",
    "            \n",
    "            # Store activations and statistics\n",
    "            self.activations[f'Z{l}'] = Z\n",
    "            self.activations[f'A{l}'] = A\n",
    "            \n",
    "            # Calculate statistics\n",
    "            self.activation_stats[f'layer_{l}'] = {\n",
    "                'pre_activation': {\n",
    "                    'mean': np.mean(Z),\n",
    "                    'std': np.std(Z),\n",
    "                    'min': np.min(Z),\n",
    "                    'max': np.max(Z)\n",
    "                },\n",
    "                'post_activation': {\n",
    "                    'mean': np.mean(A),\n",
    "                    'std': np.std(A),\n",
    "                    'min': np.min(A),\n",
    "                    'max': np.max(A),\n",
    "                    'fraction_dead': np.mean(A == 0) if self.activation == 'relu' else 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return A\n",
    "\n",
    "# Generate test data\n",
    "X_test, _ = make_classification(n_samples=1000, n_features=50, n_informative=30,\n",
    "                                n_redundant=10, random_state=42)\n",
    "X_test = X_test.T  # Shape: (features, samples)\n",
    "\n",
    "# Network architecture\n",
    "architecture = [50, 100, 80, 60, 40, 20, 1]\n",
    "\n",
    "# Test different initializations\n",
    "init_methods = ['random_small', 'random_large', 'xavier_normal', 'he_normal']\n",
    "activation_types = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "print(\"Analyzing activation statistics for different initialization methods...\\n\")\n",
    "\n",
    "def analyze_activation_flow(architecture, X_test, init_method, activation):\n",
    "    \"\"\"Analyze activation flow for specific configuration\"\"\"\n",
    "    analyzer = ActivationAnalyzer(architecture, activation)\n",
    "    analyzer.initialize_network(init_method)\n",
    "    analyzer.forward_pass(X_test)\n",
    "    \n",
    "    # Extract statistics\n",
    "    layers = []\n",
    "    pre_means = []\n",
    "    pre_stds = []\n",
    "    post_means = []\n",
    "    post_stds = []\n",
    "    dead_fractions = []\n",
    "    \n",
    "    for l in range(1, analyzer.num_layers + 1):\n",
    "        stats = analyzer.activation_stats[f'layer_{l}']\n",
    "        layers.append(l)\n",
    "        pre_means.append(stats['pre_activation']['mean'])\n",
    "        pre_stds.append(stats['pre_activation']['std'])\n",
    "        post_means.append(stats['post_activation']['mean'])\n",
    "        post_stds.append(stats['post_activation']['std'])\n",
    "        dead_fractions.append(stats['post_activation']['fraction_dead'])\n",
    "    \n",
    "    return {\n",
    "        'layers': layers,\n",
    "        'pre_means': pre_means,\n",
    "        'pre_stds': pre_stds,\n",
    "        'post_means': post_means,\n",
    "        'post_stds': post_stds,\n",
    "        'dead_fractions': dead_fractions\n",
    "    }\n",
    "\n",
    "# Create comprehensive comparison for ReLU networks\n",
    "print(\"ReLU Networks - Initialization Impact:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "relu_results = {}\n",
    "for init_method in init_methods:\n",
    "    relu_results[init_method] = analyze_activation_flow(architecture, X_test, init_method, 'relu')\n",
    "    \n",
    "    # Print summary statistics\n",
    "    result = relu_results[init_method]\n",
    "    avg_pre_std = np.mean(result['pre_stds'])\n",
    "    avg_post_std = np.mean(result['post_stds'])\n",
    "    avg_dead_fraction = np.mean(result['dead_fractions'])\n",
    "    \n",
    "    print(f\"{init_method:<15}: Avg Pre-Act Std={avg_pre_std:.4f}, \"\n",
    "          f\"Avg Post-Act Std={avg_post_std:.4f}, Avg Dead Neurons={avg_dead_fraction:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Activation analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualization of Activation Statistics\n",
    "\n",
    "### Instructions:\n",
    "1. Create detailed visualizations of activation statistics\n",
    "2. Compare initialization methods across different metrics\n",
    "3. Identify optimal initialization for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activation_analysis(results, activation_type='ReLU'):\n",
    "    \"\"\"Create comprehensive visualization of activation statistics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    colors = {'random_small': 'red', 'random_large': 'orange', \n",
    "              'xavier_normal': 'green', 'he_normal': 'blue'}\n",
    "    \n",
    "    # Plot 1: Pre-activation standard deviation\n",
    "    for method, result in results.items():\n",
    "        axes[0, 0].plot(result['layers'], result['pre_stds'], \n",
    "                       marker='o', label=method, color=colors[method], linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_title(f'Pre-Activation Standard Deviation ({activation_type})')\n",
    "    axes[0, 0].set_xlabel('Layer Number')\n",
    "    axes[0, 0].set_ylabel('Standard Deviation')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Post-activation standard deviation\n",
    "    for method, result in results.items():\n",
    "        axes[0, 1].plot(result['layers'], result['post_stds'], \n",
    "                       marker='s', label=method, color=colors[method], linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_title(f'Post-Activation Standard Deviation ({activation_type})')\n",
    "    axes[0, 1].set_xlabel('Layer Number')\n",
    "    axes[0, 1].set_ylabel('Standard Deviation')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # Plot 3: Dead neurons (for ReLU)\n",
    "    if activation_type.lower() == 'relu':\n",
    "        for method, result in results.items():\n",
    "            axes[1, 0].plot(result['layers'], [f*100 for f in result['dead_fractions']], \n",
    "                           marker='^', label=method, color=colors[method], linewidth=2)\n",
    "        \n",
    "        axes[1, 0].set_title('Dead Neurons Percentage (ReLU)')\n",
    "        axes[1, 0].set_xlabel('Layer Number')\n",
    "        axes[1, 0].set_ylabel('Dead Neurons (%)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # For non-ReLU activations, show mean activations\n",
    "        for method, result in results.items():\n",
    "            axes[1, 0].plot(result['layers'], result['post_means'], \n",
    "                           marker='^', label=method, color=colors[method], linewidth=2)\n",
    "        \n",
    "        axes[1, 0].set_title(f'Mean Activations ({activation_type})')\n",
    "        axes[1, 0].set_xlabel('Layer Number')\n",
    "        axes[1, 0].set_ylabel('Mean Activation')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Variance preservation score\n",
    "    for method, result in results.items():\n",
    "        # Calculate how well variance is preserved (closer to 1 is better)\n",
    "        variance_scores = []\n",
    "        for i in range(len(result['pre_stds'])):\n",
    "            if i == 0:\n",
    "                input_std = 1.0  # Normalized input\n",
    "            else:\n",
    "                input_std = result['post_stds'][i-1]\n",
    "            \n",
    "            if input_std > 0:\n",
    "                score = result['pre_stds'][i] / input_std\n",
    "            else:\n",
    "                score = 0\n",
    "            variance_scores.append(score)\n",
    "        \n",
    "        axes[1, 1].plot(result['layers'], variance_scores, \n",
    "                       marker='d', label=method, color=colors[method], linewidth=2)\n",
    "    \n",
    "    axes[1, 1].axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Ideal')\n",
    "    axes[1, 1].set_title('Variance Preservation Score')\n",
    "    axes[1, 1].set_xlabel('Layer Number')\n",
    "    axes[1, 1].set_ylabel('Variance Ratio')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.suptitle(f'Activation Statistics Analysis - {activation_type} Networks', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize ReLU results\n",
    "visualize_activation_analysis(relu_results, 'ReLU')\n",
    "\n",
    "# Compare different activation functions with best initialization\n",
    "print(\"\\nComparing activation functions with He initialization:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "activation_comparison = {}\n",
    "for activation in ['relu', 'sigmoid', 'tanh']:\n",
    "    result = analyze_activation_flow(architecture, X_test, 'he_normal', activation)\n",
    "    activation_comparison[activation] = result\n",
    "    \n",
    "    # Print summary\n",
    "    avg_pre_std = np.mean(result['pre_stds'])\n",
    "    avg_post_std = np.mean(result['post_stds'])\n",
    "    print(f\"{activation:<8}: Avg Pre-Act Std={avg_pre_std:.4f}, Avg Post-Act Std={avg_post_std:.4f}\")\n",
    "\n",
    "# Visualize activation function comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "act_colors = {'relu': 'blue', 'sigmoid': 'red', 'tanh': 'green'}\n",
    "\n",
    "for idx, (metric, title) in enumerate([\n",
    "    ('pre_stds', 'Pre-Activation Std'),\n",
    "    ('post_stds', 'Post-Activation Std'),\n",
    "    ('post_means', 'Post-Activation Mean')\n",
    "]):\n",
    "    for activation, result in activation_comparison.items():\n",
    "        axes[idx].plot(result['layers'], result[metric], \n",
    "                      marker='o', label=activation, color=act_colors[activation], linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f'{title} (He Initialization)')\n",
    "    axes[idx].set_xlabel('Layer Number')\n",
    "    axes[idx].set_ylabel(title.split()[1])\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    if 'Std' in title:\n",
    "        axes[idx].set_yscale('log')\n",
    "\n",
    "plt.suptitle('Activation Function Comparison with He Initialization', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Comprehensive activation analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Performance Comparison\n",
    "\n",
    "### Instructions:\n",
    "1. Train networks with different initialization methods\n",
    "2. Compare convergence speed and final accuracy\n",
    "3. Understand the practical impact of initialization choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitializationTrainer:\n",
    "    \"\"\"Neural network trainer for initialization comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu'):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        self.activation = activation\n",
    "        self.training_history = {}\n",
    "    \n",
    "    def initialize_parameters(self, method):\n",
    "        \"\"\"Initialize network parameters\"\"\"\n",
    "        self.parameters = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            shape = (self.layer_dims[l], self.layer_dims[l-1])\n",
    "            \n",
    "            if method == 'random_small':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.random_small(shape, 0.01)\n",
    "            elif method == 'xavier_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.xavier_normal(shape)\n",
    "            elif method == 'he_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.he_normal(shape)\n",
    "            elif method == 'lecun_normal':\n",
    "                self.parameters[f'W{l}'] = WeightInitializer.lecun_normal(shape)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "    \n",
    "    def activate(self, Z, activation_type=None):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation_type is None:\n",
    "            activation_type = self.activation\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "        elif activation_type == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation_type}\")\n",
    "    \n",
    "    def activate_derivative(self, Z, activation_type=None):\n",
    "        \"\"\"Compute activation derivative\"\"\"\n",
    "        if activation_type is None:\n",
    "            activation_type = self.activation\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            A = self.activate(Z, 'sigmoid')\n",
    "            return A * (1 - A)\n",
    "        elif activation_type == 'tanh':\n",
    "            A = self.activate(Z, 'tanh')\n",
    "            return 1 - A**2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation_type}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            # Use sigmoid for output layer, specified activation for hidden layers\n",
    "            if l == self.num_layers:\n",
    "                A = self.activate(Z, 'sigmoid')\n",
    "            else:\n",
    "                A = self.activate(Z)\n",
    "            \n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        \n",
    "        # Output layer\n",
    "        AL = self.cache[f'A{self.num_layers}']\n",
    "        dAL = -(Y / (AL + 1e-8) - (1 - Y) / (1 - AL + 1e-8))\n",
    "        \n",
    "        # Backward through layers\n",
    "        dA = dAL\n",
    "        for l in reversed(range(1, self.num_layers + 1)):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            Z = self.cache[f'Z{l}']\n",
    "            W = self.parameters[f'W{l}']\n",
    "            \n",
    "            # Compute gradients\n",
    "            if l == self.num_layers:\n",
    "                dZ = dA * self.activate_derivative(Z, 'sigmoid')\n",
    "            else:\n",
    "                dZ = dA * self.activate_derivative(Z)\n",
    "            \n",
    "            dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dA = np.dot(W.T, dZ)\n",
    "            \n",
    "            gradients[f'dW{l}'] = dW\n",
    "            gradients[f'db{l}'] = db\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"Compute binary cross-entropy cost\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = -(1/m) * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, init_method, \n",
    "              epochs=200, learning_rate=0.01, verbose=False):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        # Initialize parameters\n",
    "        self.initialize_parameters(init_method)\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_costs': [],\n",
    "            'test_costs': [],\n",
    "            'train_accuracies': [],\n",
    "            'test_accuracies': [],\n",
    "            'gradient_norms': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            AL_train = self.forward_propagation(X_train)\n",
    "            train_cost = self.compute_cost(AL_train, y_train)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_propagation(X_train, y_train)\n",
    "            \n",
    "            # Calculate gradient norm\n",
    "            grad_norm = sum([np.sum(gradients[key]**2) for key in gradients])**0.5\n",
    "            \n",
    "            # Update parameters\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                self.parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "                self.parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "            \n",
    "            # Evaluate\n",
    "            AL_test = self.forward_propagation(X_test)\n",
    "            test_cost = self.compute_cost(AL_test, y_test)\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            train_pred = (AL_train > 0.5).astype(float)\n",
    "            test_pred = (AL_test > 0.5).astype(float)\n",
    "            train_acc = np.mean(train_pred == y_train) * 100\n",
    "            test_acc = np.mean(test_pred == y_test) * 100\n",
    "            \n",
    "            # Store history\n",
    "            history['train_costs'].append(train_cost)\n",
    "            history['test_costs'].append(test_cost)\n",
    "            history['train_accuracies'].append(train_acc)\n",
    "            history['test_accuracies'].append(test_acc)\n",
    "            history['gradient_norms'].append(grad_norm)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch:3d}: Train Cost={train_cost:.4f}, \"\n",
    "                      f\"Test Cost={test_cost:.4f}, Train Acc={train_acc:.1f}%, \"\n",
    "                      f\"Test Acc={test_acc:.1f}%\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Prepare dataset\n",
    "X_data, y_data = make_classification(n_samples=2000, n_features=20, n_informative=15,\n",
    "                                     n_redundant=5, n_clusters_per_class=2,\n",
    "                                     random_state=42)\n",
    "X_data = X_data.T\n",
    "y_data = y_data.reshape(1, -1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data.T, y_data.T, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "y_train, y_test = y_train.T, y_test.T\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.T).T\n",
    "X_test_scaled = scaler.transform(X_test.T).T\n",
    "\n",
    "# Network architecture\n",
    "network_architecture = [20, 64, 32, 16, 8, 1]\n",
    "\n",
    "print(\"Training networks with different initialization methods...\\n\")\n",
    "print(\"Network Architecture:\", network_architecture)\n",
    "print(\"Dataset: {} training samples, {} test samples\".format(\n",
    "    X_train_scaled.shape[1], X_test_scaled.shape[1]))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train with different initializations\n",
    "init_methods = ['random_small', 'xavier_normal', 'he_normal', 'lecun_normal']\n",
    "training_results = {}\n",
    "\n",
    "for init_method in init_methods:\n",
    "    print(f\"\\nTraining with {init_method} initialization...\")\n",
    "    \n",
    "    trainer = InitializationTrainer(network_architecture, activation='relu')\n",
    "    history = trainer.train(\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "        init_method=init_method, epochs=200, learning_rate=0.01, verbose=True\n",
    "    )\n",
    "    \n",
    "    training_results[init_method] = history\n",
    "    \n",
    "    # Print final results\n",
    "    final_train_acc = history['train_accuracies'][-1]\n",
    "    final_test_acc = history['test_accuracies'][-1]\n",
    "    print(f\"Final Results - Train Acc: {final_train_acc:.1f}%, Test Acc: {final_test_acc:.1f}%\")\n",
    "\n",
    "print(\"\\n✅ Training comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance Visualization and Analysis\n",
    "\n",
    "### Instructions:\n",
    "1. Create comprehensive training performance comparisons\n",
    "2. Analyze convergence characteristics for each initialization method\n",
    "3. Draw conclusions about best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_comparison(training_results):\n",
    "    \"\"\"Visualize training performance comparison\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    colors = {'random_small': 'red', 'xavier_normal': 'green', \n",
    "              'he_normal': 'blue', 'lecun_normal': 'purple'}\n",
    "    \n",
    "    # Plot 1: Training cost\n",
    "    for method, history in training_results.items():\n",
    "        axes[0, 0].plot(history['train_costs'], label=method.replace('_', ' ').title(), \n",
    "                       color=colors[method], linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_title('Training Cost Evolution')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Cost')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Test accuracy\n",
    "    for method, history in training_results.items():\n",
    "        axes[0, 1].plot(history['test_accuracies'], label=method.replace('_', ' ').title(), \n",
    "                       color=colors[method], linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_title('Test Accuracy Evolution')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([40, 100])\n",
    "    \n",
    "    # Plot 3: Gradient norms\n",
    "    for method, history in training_results.items():\n",
    "        axes[1, 0].plot(history['gradient_norms'], label=method.replace('_', ' ').title(), \n",
    "                       color=colors[method], linewidth=2, alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_title('Gradient Norm Evolution')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Gradient Norm')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    \n",
    "    # Plot 4: Final performance comparison\n",
    "    methods = list(training_results.keys())\n",
    "    final_accuracies = [training_results[method]['test_accuracies'][-1] for method in methods]\n",
    "    convergence_epochs = []\n",
    "    \n",
    "    for method in methods:\n",
    "        # Find epoch where accuracy reaches 90% of final value\n",
    "        final_acc = training_results[method]['test_accuracies'][-1]\n",
    "        target_acc = final_acc * 0.9\n",
    "        \n",
    "        convergence_epoch = len(training_results[method]['test_accuracies'])\n",
    "        for epoch, acc in enumerate(training_results[method]['test_accuracies']):\n",
    "            if acc >= target_acc:\n",
    "                convergence_epoch = epoch\n",
    "                break\n",
    "        \n",
    "        convergence_epochs.append(convergence_epoch)\n",
    "    \n",
    "    x_pos = np.arange(len(methods))\n",
    "    \n",
    "    # Create dual y-axis bar chart\n",
    "    ax4_twin = axes[1, 1].twinx()\n",
    "    \n",
    "    bars1 = axes[1, 1].bar(x_pos - 0.2, final_accuracies, 0.4, \n",
    "                          label='Final Accuracy', color='skyblue', alpha=0.8)\n",
    "    bars2 = ax4_twin.bar(x_pos + 0.2, convergence_epochs, 0.4, \n",
    "                        label='Convergence Speed', color='orange', alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Initialization Method')\n",
    "    axes[1, 1].set_ylabel('Final Test Accuracy (%)', color='blue')\n",
    "    ax4_twin.set_ylabel('Epochs to 90% Convergence', color='orange')\n",
    "    \n",
    "    axes[1, 1].set_title('Final Performance & Convergence Speed')\n",
    "    axes[1, 1].set_xticks(x_pos)\n",
    "    axes[1, 1].set_xticklabels([m.replace('_', ' ').title() for m in methods], rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, final_accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                       f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for bar, epochs in zip(bars2, convergence_epochs):\n",
    "        height = bar.get_height()\n",
    "        ax4_twin.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                     f'{epochs}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = axes[1, 1].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "    axes[1, 1].legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Initialization Methods Training Comparison', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training comparison\n",
    "visualize_training_comparison(training_results)\n",
    "\n",
    "# Detailed performance analysis\n",
    "def analyze_initialization_performance(training_results):\n",
    "    \"\"\"Analyze and compare initialization performance\"\"\"\n",
    "    \n",
    "    print(\"\\nDetailed Performance Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Method':<15} {'Final Acc':<12} {'Best Acc':<12} {'Stability':<12} {'Conv. Speed':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    performance_metrics = {}\n",
    "    \n",
    "    for method, history in training_results.items():\n",
    "        # Calculate metrics\n",
    "        final_acc = history['test_accuracies'][-1]\n",
    "        best_acc = max(history['test_accuracies'])\n",
    "        \n",
    "        # Stability (inverse of variance in last 50 epochs)\n",
    "        stability = 1.0 / (np.var(history['test_accuracies'][-50:]) + 1e-8)\n",
    "        \n",
    "        # Convergence speed (epochs to reach 90% of final accuracy)\n",
    "        target_acc = final_acc * 0.9\n",
    "        conv_speed = len(history['test_accuracies'])\n",
    "        for epoch, acc in enumerate(history['test_accuracies']):\n",
    "            if acc >= target_acc:\n",
    "                conv_speed = epoch\n",
    "                break\n",
    "        \n",
    "        performance_metrics[method] = {\n",
    "            'final_accuracy': final_acc,\n",
    "            'best_accuracy': best_acc,\n",
    "            'stability': stability,\n",
    "            'convergence_speed': conv_speed\n",
    "        }\n",
    "        \n",
    "        print(f\"{method.replace('_', ' '):<15} {final_acc:>10.1f}% {best_acc:>10.1f}% \"\n",
    "              f\"{stability:>10.1f} {conv_speed:>13d} epochs\")\n",
    "    \n",
    "    # Determine best method\n",
    "    print(\"\\nRankings by Metric:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Rank by final accuracy\n",
    "    acc_ranking = sorted(performance_metrics.items(), \n",
    "                        key=lambda x: x[1]['final_accuracy'], reverse=True)\n",
    "    print(\"Final Accuracy:\")\n",
    "    for i, (method, metrics) in enumerate(acc_ranking):\n",
    "        print(f\"  {i+1}. {method.replace('_', ' ').title()}: {metrics['final_accuracy']:.1f}%\")\n",
    "    \n",
    "    # Rank by convergence speed\n",
    "    speed_ranking = sorted(performance_metrics.items(), \n",
    "                          key=lambda x: x[1]['convergence_speed'])\n",
    "    print(\"\\nConvergence Speed:\")\n",
    "    for i, (method, metrics) in enumerate(speed_ranking):\n",
    "        print(f\"  {i+1}. {method.replace('_', ' ').title()}: {metrics['convergence_speed']} epochs\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_overall = acc_ranking[0][0]\n",
    "    fastest = speed_ranking[0][0]\n",
    "    \n",
    "    print(f\"🏆 Best Overall Performance: {best_overall.replace('_', ' ').title()}\")\n",
    "    print(f\"🚀 Fastest Convergence: {fastest.replace('_', ' ').title()}\")\n",
    "    \n",
    "    if best_overall == 'he_normal':\n",
    "        print(\"\\n✅ He initialization performs best for ReLU networks (as expected)\")\n",
    "    if fastest == 'he_normal':\n",
    "        print(\"✅ He initialization also provides fastest convergence\")\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "# Analyze performance\n",
    "performance_metrics = analyze_initialization_performance(training_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Custom Initialization Strategies\n",
    "\n",
    "### Instructions:\n",
    "1. Design custom initialization methods for specific scenarios\n",
    "2. Test adaptive initialization strategies\n",
    "3. Implement layer-specific initialization schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomInitializer:\n",
    "    \"\"\"Advanced custom initialization strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaptive_he(shape, activation='relu', layer_depth=1):\n",
    "        \"\"\"\n",
    "        Adaptive He initialization that considers layer depth\n",
    "        Deeper layers get slightly smaller initialization to prevent explosion\n",
    "        \"\"\"\n",
    "        n_in = shape[1]\n",
    "        \n",
    "        # Base He initialization\n",
    "        if activation == 'relu':\n",
    "            base_std = np.sqrt(2.0 / n_in)\n",
    "        elif activation == 'leaky_relu':\n",
    "            base_std = np.sqrt(2.0 / n_in)  # Can be adjusted based on leak parameter\n",
    "        else:\n",
    "            base_std = np.sqrt(2.0 / n_in)\n",
    "        \n",
    "        # Depth adjustment factor\n",
    "        depth_factor = 1.0 / np.sqrt(1 + 0.1 * layer_depth)\n",
    "        \n",
    "        adjusted_std = base_std * depth_factor\n",
    "        return np.random.randn(*shape) * adjusted_std\n",
    "    \n",
    "    @staticmethod\n",
    "    def layer_sequential_init(shapes, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize all layers considering the full network architecture\n",
    "        Each layer's initialization depends on previous layers\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        cumulative_factor = 1.0\n",
    "        \n",
    "        for l, shape in enumerate(shapes, 1):\n",
    "            n_in = shape[1]\n",
    "            \n",
    "            if activation == 'relu':\n",
    "                base_std = np.sqrt(2.0 / n_in)\n",
    "            else:\n",
    "                base_std = np.sqrt(1.0 / n_in)\n",
    "            \n",
    "            # Sequential adjustment\n",
    "            if l > 1:\n",
    "                cumulative_factor *= 0.95  # Slightly reduce variance with depth\n",
    "            \n",
    "            adjusted_std = base_std * cumulative_factor\n",
    "            parameters[f'W{l}'] = np.random.randn(*shape) * adjusted_std\n",
    "            parameters[f'b{l}'] = np.zeros((shape[0], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    @staticmethod\n",
    "    def residual_aware_init(shape, has_residual=False):\n",
    "        \"\"\"\n",
    "        Initialization aware of residual connections\n",
    "        Residual layers can be initialized with smaller values\n",
    "        \"\"\"\n",
    "        n_in = shape[1]\n",
    "        \n",
    "        if has_residual:\n",
    "            # Smaller initialization for residual layers\n",
    "            std = np.sqrt(1.0 / n_in)\n",
    "        else:\n",
    "            # Standard He initialization\n",
    "            std = np.sqrt(2.0 / n_in)\n",
    "        \n",
    "        return np.random.randn(*shape) * std\n",
    "    \n",
    "    @staticmethod\n",
    "    def width_aware_init(shape, target_std=1.0):\n",
    "        \"\"\"\n",
    "        Initialization that considers layer width\n",
    "        Maintains target standard deviation regardless of layer size\n",
    "        \"\"\"\n",
    "        n_in, n_out = shape[1], shape[0]\n",
    "        \n",
    "        # Adjust for both input and output dimensions\n",
    "        width_factor = np.sqrt(n_out / (n_in + n_out))\n",
    "        std = target_std * width_factor / np.sqrt(n_in)\n",
    "        \n",
    "        return np.random.randn(*shape) * std\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_norm_init(shape, spectral_radius=0.9):\n",
    "        \"\"\"\n",
    "        Initialize weights with controlled spectral radius\n",
    "        Useful for RNNs and very deep networks\n",
    "        \"\"\"\n",
    "        # Generate random matrix\n",
    "        W = np.random.randn(*shape)\n",
    "        \n",
    "        # Compute spectral norm (largest singular value)\n",
    "        _, s, _ = np.linalg.svd(W)\n",
    "        current_spectral_norm = s[0]\n",
    "        \n",
    "        # Scale to desired spectral radius\n",
    "        W_normalized = W * (spectral_radius / current_spectral_norm)\n",
    "        \n",
    "        return W_normalized\n",
    "\n",
    "# Test custom initialization strategies\n",
    "print(\"Testing Custom Initialization Strategies:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_shape = (64, 128)\n",
    "test_architectures = [(128, 100), (100, 80), (80, 60), (60, 40), (40, 1)]\n",
    "\n",
    "# Test adaptive He initialization\n",
    "print(\"\\n1. Adaptive He Initialization (depth-aware):\")\n",
    "for depth in [1, 2, 3, 4, 5]:\n",
    "    weights = CustomInitializer.adaptive_he(test_shape, layer_depth=depth)\n",
    "    print(f\"   Layer {depth}: std={np.std(weights):.6f}\")\n",
    "\n",
    "# Test layer sequential initialization\n",
    "print(\"\\n2. Layer Sequential Initialization:\")\n",
    "seq_params = CustomInitializer.layer_sequential_init(test_architectures)\n",
    "for l in range(1, len(test_architectures) + 1):\n",
    "    std = np.std(seq_params[f'W{l}'])\n",
    "    print(f\"   Layer {l}: std={std:.6f}\")\n",
    "\n",
    "# Test spectral norm initialization\n",
    "print(\"\\n3. Spectral Norm Initialization:\")\n",
    "for radius in [0.5, 0.9, 1.0, 1.2]:\n",
    "    weights = CustomInitializer.spectral_norm_init(test_shape, spectral_radius=radius)\n",
    "    actual_radius = np.linalg.svd(weights)[1][0]\n",
    "    print(f\"   Target radius: {radius:.1f}, Actual: {actual_radius:.6f}\")\n",
    "\n",
    "print(\"\\n✅ Custom initialization strategies tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Initialization Decision Framework\n",
    "\n",
    "### Instructions:\n",
    "1. Create a decision framework for choosing initialization methods\n",
    "2. Implement automatic initialization selection\n",
    "3. Test the framework on different network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitializationRecommender:\n",
    "    \"\"\"Intelligent initialization recommendation system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.recommendations = {\n",
    "            'relu': {\n",
    "                'shallow': 'he_normal',\n",
    "                'deep': 'adaptive_he',\n",
    "                'very_deep': 'spectral_norm'\n",
    "            },\n",
    "            'leaky_relu': {\n",
    "                'shallow': 'he_normal',\n",
    "                'deep': 'he_normal',\n",
    "                'very_deep': 'adaptive_he'\n",
    "            },\n",
    "            'sigmoid': {\n",
    "                'shallow': 'xavier_normal',\n",
    "                'deep': 'xavier_normal',\n",
    "                'very_deep': 'layer_sequential'\n",
    "            },\n",
    "            'tanh': {\n",
    "                'shallow': 'xavier_normal',\n",
    "                'deep': 'xavier_normal', \n",
    "                'very_deep': 'layer_sequential'\n",
    "            },\n",
    "            'selu': {\n",
    "                'shallow': 'lecun_normal',\n",
    "                'deep': 'lecun_normal',\n",
    "                'very_deep': 'lecun_normal'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_network(self, layer_dims, activation='relu'):\n",
    "        \"\"\"Analyze network characteristics\"\"\"\n",
    "        num_hidden = len(layer_dims) - 2\n",
    "        total_params = sum(layer_dims[i] * layer_dims[i+1] + layer_dims[i+1] \n",
    "                          for i in range(len(layer_dims)-1))\n",
    "        \n",
    "        # Determine network depth category\n",
    "        if num_hidden <= 3:\n",
    "            depth_category = 'shallow'\n",
    "        elif num_hidden <= 10:\n",
    "            depth_category = 'deep'\n",
    "        else:\n",
    "            depth_category = 'very_deep'\n",
    "        \n",
    "        # Analyze potential issues\n",
    "        issues = []\n",
    "        if max(layer_dims) > 1000:\n",
    "            issues.append('wide_layers')\n",
    "        if min(layer_dims[1:-1]) < 10:\n",
    "            issues.append('narrow_bottleneck')\n",
    "        if num_hidden > 20:\n",
    "            issues.append('very_deep')\n",
    "        \n",
    "        # Check for irregular architecture\n",
    "        width_ratios = [layer_dims[i+1]/layer_dims[i] for i in range(len(layer_dims)-1)]\n",
    "        if max(width_ratios) > 10 or min(width_ratios) < 0.1:\n",
    "            issues.append('irregular_width')\n",
    "        \n",
    "        return {\n",
    "            'depth_category': depth_category,\n",
    "            'num_hidden': num_hidden,\n",
    "            'total_params': total_params,\n",
    "            'issues': issues\n",
    "        }\n",
    "    \n",
    "    def recommend_initialization(self, layer_dims, activation='relu', \n",
    "                               problem_type='classification', verbose=True):\n",
    "        \"\"\"Recommend best initialization strategy\"\"\"\n",
    "        analysis = self.analyze_network(layer_dims, activation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Network Analysis:\")\n",
    "            print(f\"  Architecture: {layer_dims}\")\n",
    "            print(f\"  Activation: {activation}\")\n",
    "            print(f\"  Hidden layers: {analysis['num_hidden']}\")\n",
    "            print(f\"  Total parameters: {analysis['total_params']:,}\")\n",
    "            print(f\"  Depth category: {analysis['depth_category']}\")\n",
    "            if analysis['issues']:\n",
    "                print(f\"  Potential issues: {', '.join(analysis['issues'])}\")\n",
    "        \n",
    "        # Base recommendation\n",
    "        if activation.lower() in self.recommendations:\n",
    "            base_rec = self.recommendations[activation.lower()][analysis['depth_category']]\n",
    "        else:\n",
    "            base_rec = 'he_normal'  # Default fallback\n",
    "        \n",
    "        # Adjust for specific issues\n",
    "        final_rec = base_rec\n",
    "        additional_advice = []\n",
    "        \n",
    "        if 'wide_layers' in analysis['issues']:\n",
    "            additional_advice.append(\"Consider batch normalization for wide layers\")\n",
    "            if base_rec == 'he_normal':\n",
    "                final_rec = 'adaptive_he'\n",
    "        \n",
    "        if 'narrow_bottleneck' in analysis['issues']:\n",
    "            additional_advice.append(\"Narrow layers may cause information loss\")\n",
    "        \n",
    "        if 'irregular_width' in analysis['issues']:\n",
    "            additional_advice.append(\"Consider layer-specific initialization\")\n",
    "            if base_rec in ['he_normal', 'xavier_normal']:\n",
    "                final_rec = 'width_aware'\n",
    "        \n",
    "        if 'very_deep' in analysis['issues']:\n",
    "            additional_advice.append(\"Consider residual connections\")\n",
    "            final_rec = 'spectral_norm'\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRecommendation: {final_rec.replace('_', ' ').title()}\")\n",
    "            if additional_advice:\n",
    "                print(\"Additional advice:\")\n",
    "                for advice in additional_advice:\n",
    "                    print(f\"  - {advice}\")\n",
    "        \n",
    "        return {\n",
    "            'method': final_rec,\n",
    "            'confidence': self._calculate_confidence(analysis, activation),\n",
    "            'alternatives': self._get_alternatives(final_rec, activation),\n",
    "            'advice': additional_advice,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, analysis, activation):\n",
    "        \"\"\"Calculate confidence in recommendation\"\"\"\n",
    "        base_confidence = 0.8\n",
    "        \n",
    "        # Higher confidence for common scenarios\n",
    "        if activation.lower() in ['relu', 'sigmoid', 'tanh']:\n",
    "            base_confidence += 0.1\n",
    "        \n",
    "        # Lower confidence for problematic architectures\n",
    "        if len(analysis['issues']) > 2:\n",
    "            base_confidence -= 0.2\n",
    "        \n",
    "        return min(max(base_confidence, 0.3), 0.95)\n",
    "    \n",
    "    def _get_alternatives(self, primary, activation):\n",
    "        \"\"\"Get alternative initialization methods\"\"\"\n",
    "        alternatives = []\n",
    "        \n",
    "        if primary == 'he_normal':\n",
    "            alternatives = ['he_uniform', 'adaptive_he']\n",
    "        elif primary == 'xavier_normal':\n",
    "            alternatives = ['xavier_uniform', 'lecun_normal']\n",
    "        elif primary == 'adaptive_he':\n",
    "            alternatives = ['he_normal', 'spectral_norm']\n",
    "        else:\n",
    "            alternatives = ['he_normal', 'xavier_normal']\n",
    "        \n",
    "        return alternatives\n",
    "\n",
    "# Create recommendation system\n",
    "recommender = InitializationRecommender()\n",
    "\n",
    "print(\"Testing Initialization Recommendation System:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test various network architectures\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Simple Classification Network',\n",
    "        'architecture': [784, 128, 64, 10],\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Very Deep Network',\n",
    "        'architecture': [100, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1],\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Wide Shallow Network',\n",
    "        'architecture': [1000, 2000, 1000, 1],\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Sigmoid Network',\n",
    "        'architecture': [50, 100, 50, 25, 1],\n",
    "        'activation': 'sigmoid'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Irregular Architecture',\n",
    "        'architecture': [10, 1000, 5, 500, 1],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{i}. {test_case['name']}:\")\n",
    "    print(\"-\" * (len(test_case['name']) + 4))\n",
    "    \n",
    "    recommendation = recommender.recommend_initialization(\n",
    "        test_case['architecture'], \n",
    "        test_case['activation'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"   Confidence: {recommendation['confidence']:.1%}\")\n",
    "    print(f\"   Alternatives: {', '.join(recommendation['alternatives'])}\")\n",
    "\n",
    "print(\"\\n✅ Recommendation system testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Best Practices Summary and Guidelines\n",
    "\n",
    "### Instructions:\n",
    "1. Review comprehensive best practices for weight initialization\n",
    "2. Understand when to use each initialization method\n",
    "3. Complete practical exercises to reinforce learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive best practices guide\n",
    "def create_initialization_guide():\n",
    "    \"\"\"Create comprehensive initialization best practices guide\"\"\"\n",
    "    \n",
    "    guide = \"\"\"\n",
    "    🎯 WEIGHT INITIALIZATION BEST PRACTICES GUIDE\n",
    "    ============================================\n",
    "    \n",
    "    1. ACTIVATION FUNCTION SPECIFIC:\n",
    "    \n",
    "    ReLU Networks:\n",
    "    ✅ USE: He initialization (He Normal or He Uniform)\n",
    "    📈 Variance: Var(W) = 2/n_in\n",
    "    🎯 Why: Accounts for half the neurons being zero\n",
    "    \n",
    "    Sigmoid/Tanh Networks:\n",
    "    ✅ USE: Xavier/Glorot initialization\n",
    "    📈 Variance: Var(W) = 1/n_in or 2/(n_in + n_out)\n",
    "    🎯 Why: Maintains variance for symmetric activations\n",
    "    \n",
    "    SELU Networks:\n",
    "    ✅ USE: LeCun initialization\n",
    "    📈 Variance: Var(W) = 1/n_in\n",
    "    🎯 Why: Designed for self-normalizing properties\n",
    "    \n",
    "    2. NETWORK DEPTH CONSIDERATIONS:\n",
    "    \n",
    "    Shallow Networks (≤3 hidden layers):\n",
    "    ✅ Standard initialization methods work well\n",
    "    ✅ He Normal for ReLU, Xavier for sigmoid/tanh\n",
    "    \n",
    "    Deep Networks (4-10 hidden layers):\n",
    "    ✅ Use proper initialization + batch normalization\n",
    "    ✅ Consider gradient clipping\n",
    "    ⚠️ Monitor gradient flow carefully\n",
    "    \n",
    "    Very Deep Networks (>10 hidden layers):\n",
    "    ✅ Residual connections + careful initialization\n",
    "    ✅ Layer-specific initialization strategies\n",
    "    ✅ Spectral norm control\n",
    "    ❌ Avoid standard methods without modifications\n",
    "    \n",
    "    3. COMMON PITFALLS TO AVOID:\n",
    "    \n",
    "    ❌ Zero initialization for weights (symmetry breaking problem)\n",
    "    ❌ Same initialization for all weights (no learning)\n",
    "    ❌ Too large values (exploding gradients)\n",
    "    ❌ Too small values (vanishing gradients)\n",
    "    ❌ Ignoring activation function choice\n",
    "    ❌ Using sigmoid/tanh in deep networks without proper init\n",
    "    \n",
    "    4. PRACTICAL RECOMMENDATIONS:\n",
    "    \n",
    "    Starting Point:\n",
    "    🟢 Use He Normal for ReLU networks (90% of cases)\n",
    "    🟢 Use Xavier Normal for sigmoid/tanh networks\n",
    "    🟢 Always initialize biases to zero\n",
    "    \n",
    "    If Training Fails:\n",
    "    🔄 Check gradient flow (use gradient norm tracking)\n",
    "    🔄 Try different initialization methods\n",
    "    🔄 Add batch normalization\n",
    "    🔄 Implement gradient clipping\n",
    "    🔄 Reduce learning rate\n",
    "    \n",
    "    For Special Cases:\n",
    "    🎯 Transfer Learning: Initialize only last layer\n",
    "    🎯 GANs: Careful initialization of both networks\n",
    "    🎯 RNNs: Use orthogonal initialization\n",
    "    🎯 Autoencoders: Symmetric initialization\n",
    "    \n",
    "    5. DEBUGGING CHECKLIST:\n",
    "    \n",
    "    Before Training:\n",
    "    □ Check weight distributions match theoretical expectations\n",
    "    □ Verify no NaN or infinite values\n",
    "    □ Confirm reasonable activation magnitudes\n",
    "    \n",
    "    During Training:\n",
    "    □ Monitor gradient norms (should be stable)\n",
    "    □ Track activation statistics\n",
    "    □ Watch for dead neurons (ReLU)\n",
    "    □ Check for gradient explosion/vanishing\n",
    "    \n",
    "    6. PERFORMANCE OPTIMIZATION:\n",
    "    \n",
    "    Speed up Convergence:\n",
    "    ⚡ Use batch normalization\n",
    "    ⚡ Proper learning rate scheduling\n",
    "    ⚡ Warm-up initialization schemes\n",
    "    \n",
    "    Improve Stability:\n",
    "    🛡️ Gradient clipping\n",
    "    🛡️ Spectral normalization\n",
    "    🛡️ Layer-wise adaptive rates\n",
    "    \n",
    "    7. MODERN BEST PRACTICES:\n",
    "    \n",
    "    Current State-of-Art:\n",
    "    🌟 He initialization + Batch Normalization + ReLU\n",
    "    🌟 Residual connections for very deep networks\n",
    "    🌟 Attention mechanisms with scaled initialization\n",
    "    🌟 Layer normalization for transformers\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return guide\n",
    "\n",
    "# Display the guide\n",
    "print(create_initialization_guide())\n",
    "\n",
    "# Create practical decision tree\n",
    "def initialization_decision_tree():\n",
    "    \"\"\"Interactive decision tree for initialization\"\"\"\n",
    "    \n",
    "    print(\"\\n🌳 INITIALIZATION DECISION TREE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    decision_tree = \"\"\"\n",
    "    Start Here: What's your activation function?\n",
    "    ├── ReLU Family (ReLU, LeakyReLU, ELU)\n",
    "    │   ├── Shallow Network (≤3 layers)\n",
    "    │   │   └── ✅ Use He Normal\n",
    "    │   ├── Deep Network (4-10 layers)\n",
    "    │   │   └── ✅ Use He Normal + Batch Norm\n",
    "    │   └── Very Deep (>10 layers)\n",
    "    │       └── ✅ Use Adaptive He + Residual Connections\n",
    "    │\n",
    "    ├── Sigmoid/Tanh\n",
    "    │   ├── Shallow Network\n",
    "    │   │   └── ✅ Use Xavier Normal\n",
    "    │   └── Deep Network\n",
    "    │       └── ⚠️ Consider ReLU instead, or Xavier + Batch Norm\n",
    "    │\n",
    "    ├── SELU\n",
    "    │   └── ✅ Use LeCun Normal (any depth)\n",
    "    │\n",
    "    └── Custom/Other\n",
    "        └── 🔬 Experiment with variance scaling\n",
    "    \n",
    "    Special Considerations:\n",
    "    • Wide layers (>1000 units): Add batch normalization\n",
    "    • Irregular architecture: Consider layer-specific initialization\n",
    "    • Transfer learning: Initialize only new layers\n",
    "    • Training instability: Add gradient clipping\n",
    "    \"\"\"\n",
    "    \n",
    "    print(decision_tree)\n",
    "\n",
    "initialization_decision_tree()\n",
    "\n",
    "# Create quick reference table\n",
    "print(\"\\n📋 QUICK REFERENCE TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Scenario':<25} {'Recommended Method':<20} {'Alternative':<20} {'Notes'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "scenarios = [\n",
    "    (\"ReLU + Shallow\", \"He Normal\", \"He Uniform\", \"Standard choice\"),\n",
    "    (\"ReLU + Deep\", \"He Normal + BN\", \"Adaptive He\", \"Add batch norm\"),\n",
    "    (\"ReLU + Very Deep\", \"Residual + He\", \"Spectral Norm\", \"Need skip connections\"),\n",
    "    (\"Sigmoid + Any\", \"Xavier Normal\", \"Xavier Uniform\", \"Consider ReLU instead\"),\n",
    "    (\"Tanh + Any\", \"Xavier Normal\", \"LeCun Normal\", \"Symmetric activation\"),\n",
    "    (\"SELU + Any\", \"LeCun Normal\", \"LeCun Uniform\", \"Self-normalizing\"),\n",
    "    (\"Transfer Learning\", \"Pretrained + He\", \"Fine-tune only\", \"Last layer only\"),\n",
    "    (\"GAN Generator\", \"Xavier Normal\", \"He Normal\", \"Stable training\"),\n",
    "    (\"GAN Discriminator\", \"He Normal\", \"Spectral Norm\", \"Prevent mode collapse\"),\n",
    "    (\"RNN/LSTM\", \"Orthogonal\", \"Xavier Normal\", \"Recurrent weights\"),\n",
    "]\n",
    "\n",
    "for scenario, method, alt, notes in scenarios:\n",
    "    print(f\"{scenario:<25} {method:<20} {alt:<20} {notes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎓 CONGRATULATIONS! You've mastered weight initialization strategies!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Complete! 🎉\n",
    "\n",
    "### What You've Accomplished:\n",
    "✅ **Mastered Initialization Theory**: Understood the mathematical foundations behind different initialization methods  \n",
    "✅ **Implemented All Major Methods**: Built Xavier, He, LeCun, and custom initialization strategies  \n",
    "✅ **Analyzed Impact on Training**: Compared how initialization affects convergence and stability  \n",
    "✅ **Created Custom Solutions**: Developed adaptive and architecture-aware initialization  \n",
    "✅ **Built Decision Framework**: Created an intelligent recommendation system  \n",
    "✅ **Established Best Practices**: Learned when and how to apply each method  \n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Activation Function Matters**: ReLU needs He, Sigmoid/Tanh need Xavier\n",
    "2. **Network Depth is Critical**: Deeper networks need more sophisticated initialization\n",
    "3. **One Size Doesn't Fit All**: Different architectures need different approaches\n",
    "4. **Monitor and Adapt**: Track gradient flow and activation statistics\n",
    "5. **Combine with Other Techniques**: Initialization works best with batch norm and proper regularization\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment with Your Own Data**: Apply these techniques to real-world problems\n",
    "2. **Explore Advanced Methods**: Look into LSUV, FIXUP, and other modern techniques\n",
    "3. **Study Architecture-Specific Methods**: Learn about transformer, CNN, and RNN initialization\n",
    "4. **Implement in Deep Learning Frameworks**: Apply these concepts in PyTorch/TensorFlow\n",
    "\n",
    "### Additional Challenges:\n",
    "1. **Create Dynamic Initialization**: Build initialization that adapts during training\n",
    "2. **Analyze Real Networks**: Study initialization in popular architectures (ResNet, BERT, etc.)\n",
    "3. **Develop New Methods**: Design initialization for novel activation functions\n",
    "4. **Benchmark Performance**: Create comprehensive comparison across different domains\n",
    "\n",
    "### Troubleshooting Summary:\n",
    "- **Training not converging?** → Check initialization scale and gradient flow\n",
    "- **Gradients exploding?** → Reduce initialization variance or add clipping\n",
    "- **Gradients vanishing?** → Use proper initialization for activation function\n",
    "- **Slow convergence?** → Add batch normalization or adjust learning rate\n",
    "- **Unstable training?** → Consider spectral normalization or residual connections\n",
    "\n",
    "### Cleanup:\n",
    "```python\n",
    "# Clear variables to free memory\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Memory cleanup complete!\")\n",
    "```\n",
    "\n",
    "**Remember**: Good initialization is the foundation of successful deep learning. It's often the difference between a model that learns and one that doesn't! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}