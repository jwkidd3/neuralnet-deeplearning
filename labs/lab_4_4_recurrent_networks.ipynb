{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4: Introduction to Recurrent Neural Networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand why RNNs are perfect for sequence data\n",
    "- Build your first RNN using TensorFlow/Keras\n",
    "- Compare RNN performance with regular Dense networks on sequence tasks\n",
    "- Use SimpleRNN, LSTM, and GRU layers\n",
    "- Apply RNNs to text classification and time series problems\n",
    "\n",
    "## Prerequisites\n",
    "- **Labs 4.1, 4.2, & 4.3 completed** (TensorFlow basics, deep networks, CNNs)\n",
    "- Understanding of neural networks\n",
    "- Basic knowledge of sequences and time series\n",
    "\n",
    "## Key Concepts\n",
    "- **Sequences**: Data where order matters (text, time series, audio)\n",
    "- **Memory**: How RNNs remember information from previous steps\n",
    "- **LSTM**: Long Short-Term Memory for long sequences\n",
    "- **GRU**: Gated Recurrent Unit (simpler than LSTM)\n",
    "- **Text Processing**: Converting words to numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: The Problem - Dense Networks Can't Handle Sequences\n\nLet's first see why regular networks fail on sequential data:"
  },
  {
   "cell_type": "code",
   "source": "print(\"Understanding Sequential Data Problems:\")\nprint(\"=\" * 50)\n\n# Load IMDB movie review dataset for sentiment analysis\nprint(\"Loading IMDB movie reviews dataset...\")\n(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(num_words=10000)\n\nprint(f\"Training samples: {len(X_train_imdb)}\")\nprint(f\"Test samples: {len(X_test_imdb)}\")\nprint(f\"Classes: {len(set(y_train_imdb))} (0=negative, 1=positive)\")\n\n# Look at some sample reviews\nword_index = imdb.get_word_index()\nreverse_word_index = {value: key for key, value in word_index.items()}\n\ndef decode_review(encoded_review):\n    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n\nprint(\"\\nSample Reviews:\")\nprint(\"-\" * 30)\nfor i in range(3):\n    review_text = decode_review(X_train_imdb[i])\n    sentiment = \"POSITIVE\" if y_train_imdb[i] == 1 else \"NEGATIVE\"\n    print(f\"\\nReview {i+1} ({sentiment}):\")\n    print(f\"Length: {len(X_train_imdb[i])} words\")\n    print(f\"Text: {review_text[:200]}...\")\n\n# Show the sequence length problem\nsequence_lengths = [len(x) for x in X_train_imdb]\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(sequence_lengths, bins=50, alpha=0.7)\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\nplt.axvline(np.mean(sequence_lengths), color='red', linestyle='--', \n           label=f'Mean: {np.mean(sequence_lengths):.0f}')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(sequence_lengths[:1000])), sorted(sequence_lengths[:1000]))\nplt.title('Sequence Lengths (First 1000 samples)')\nplt.xlabel('Sample Index')\nplt.ylabel('Number of Words')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Sequence Statistics:\")\nprint(f\"  Shortest review: {min(sequence_lengths)} words\")\nprint(f\"  Longest review: {max(sequence_lengths)} words\")\nprint(f\"  Average length: {np.mean(sequence_lengths):.1f} words\")\nprint(f\"  Standard deviation: {np.std(sequence_lengths):.1f} words\")\n\nprint(\"\\n‚ùó Problems for Dense Networks:\")\nprint(\"  ‚Ä¢ Variable length inputs (can't handle different sizes)\")\nprint(\"  ‚Ä¢ Would need to pad/truncate all sequences\")\nprint(\"  ‚Ä¢ No understanding of word order\")\nprint(\"  ‚Ä¢ Can't capture long-range dependencies\")\nprint(\"  ‚Ä¢ 'movie great' vs 'great movie' treated identically\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Comparing RNN Types - LSTM vs GRU vs SimpleRNN\\n\\nLet's build LSTM and GRU models and compare their performance:\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Building Your First SimpleRNN\n\nNow let's create a Simple RNN to understand the basics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Preparing Sequential Data for RNNs:\")\nprint(\"=\" * 50)\n\n# Pad sequences to make them the same length\nmax_length = 200  # Reasonable length that captures most reviews\n\nprint(f\"Padding sequences to length {max_length}...\")\nX_train_padded = pad_sequences(X_train_imdb, maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test_imdb, maxlen=max_length, padding='post', truncating='post')\n\nprint(f\"Training data shape: {X_train_padded.shape}\")\nprint(f\"Test data shape: {X_test_padded.shape}\")\n\n# Show what padding looks like\nprint(\"\\nExample of padded sequence:\")\nsample_review = X_train_padded[0]\nprint(f\"Original length: {len(X_train_imdb[0])}\")\nprint(f\"Padded length: {len(sample_review)}\")\nprint(f\"First 20 words: {sample_review[:20]}\")\nprint(f\"Last 20 words: {sample_review[-20:]}\")\nprint(\"(Note: 0s are padding tokens)\")\n\n# Visualize the impact of padding\nplt.figure(figsize=(12, 4))\n\n# Before padding\nplt.subplot(1, 2, 1)\noriginal_lengths = [len(x) for x in X_train_imdb[:1000]]\nplt.hist(original_lengths, bins=30, alpha=0.7, color='blue')\nplt.title('Original Sequence Lengths')\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.axvline(max_length, color='red', linestyle='--', label=f'Max Length: {max_length}')\nplt.legend()\n\n# After padding\nplt.subplot(1, 2, 2)\npadded_lengths = [np.count_nonzero(x) for x in X_train_padded[:1000]]  # Count non-zero (non-padding)\nplt.hist(padded_lengths, bins=30, alpha=0.7, color='green')\nplt.title('Actual Content Lengths (After Padding)')\nplt.xlabel('Content Length')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate padding statistics\nactual_content_lengths = [np.count_nonzero(x) for x in X_train_padded]\ntruncated_samples = sum(1 for length in [len(x) for x in X_train_imdb] if length > max_length)\npadding_ratio = 1 - (np.mean(actual_content_lengths) / max_length)\n\nprint(f\"\\nüìä Padding Impact:\")\nprint(f\"  Samples truncated: {truncated_samples} ({truncated_samples/len(X_train_imdb)*100:.1f}%)\")\nprint(f\"  Average padding ratio: {padding_ratio:.2f} ({padding_ratio*100:.1f}% padding)\")\nprint(f\"  Efficiency: {(1-padding_ratio)*100:.1f}% of data is actual content\")\n\nprint(\"\\n‚úÖ Data Preparation Complete:\")\nprint(\"  ‚Ä¢ Sequences padded to uniform length\")\nprint(\"  ‚Ä¢ Truncated long sequences\")\nprint(\"  ‚Ä¢ Ready for RNN processing\")\nprint(\"  ‚Ä¢ Each word is represented by an integer ID\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Building a Simple RNN:\")\nprint(\"=\" * 50)\n\n# Build Simple RNN model\nsimple_rnn_model = keras.Sequential([\n    # Embedding layer converts word IDs to dense vectors\n    layers.Embedding(input_dim=10000, output_dim=32, input_length=max_length),\n    \n    # Simple RNN layer\n    layers.SimpleRNN(32, dropout=0.2),\n    \n    # Dense layer for classification\n    layers.Dense(1, activation='sigmoid')\n], name='Simple_RNN')\n\nsimple_rnn_model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Simple RNN Architecture:\")\nsimple_rnn_model.summary()\n\n# Build the model to enable parameter counting\nsimple_rnn_model.build(input_shape=(None, max_length))\nprint(f\"\\nTotal parameters: {simple_rnn_model.count_params():,}\")\n\nprint(\"\\nüß† How Simple RNN Works:\")\nprint(\"  1. Embedding: Converts word IDs ‚Üí dense vectors (32-dim)\")\nprint(\"  2. SimpleRNN: Processes sequence step by step\")\nprint(\"     - Maintains hidden state (memory)\")\nprint(\"     - Updates state with each word\")\nprint(\"     - Outputs final hidden state\")\nprint(\"  3. Dense: Maps final state ‚Üí sentiment probability\")\n\n# Train the Simple RNN\nprint(\"\\nTraining Simple RNN (this will take a few minutes...)\")\nsimple_rnn_history = simple_rnn_model.fit(\n    X_train_padded[:5000],  # Use subset for faster training\n    y_train_imdb[:5000],\n    validation_split=0.2,\n    epochs=3,\n    batch_size=128,\n    verbose=1\n)\n\n# Evaluate\nsimple_rnn_loss, simple_rnn_accuracy = simple_rnn_model.evaluate(\n    X_test_padded[:1000], y_test_imdb[:1000], verbose=0\n)\n\nprint(f\"\\nSimple RNN Results:\")\nprint(f\"  Test Accuracy: {simple_rnn_accuracy:.4f}\")\nprint(f\"  Test Loss: {simple_rnn_loss:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Preparing Sequential Data\\n\\nBefore we can use RNNs, we need to prepare our sequential data:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Understanding Sequential Data Problems:\")\nprint(\"=\" * 50)\n\n# Load IMDB movie review dataset for sentiment analysis\nprint(\"Loading IMDB movie reviews dataset...\")\n(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(num_words=10000)\n\nprint(f\"Training samples: {len(X_train_imdb)}\")\nprint(f\"Test samples: {len(X_test_imdb)}\")\nprint(f\"Classes: {len(set(y_train_imdb))} (0=negative, 1=positive)\")\n\n# Look at some sample reviews\nword_index = imdb.get_word_index()\nreverse_word_index = {value: key for key, value in word_index.items()}\n\ndef decode_review(encoded_review):\n    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n\nprint(\"\\nSample Reviews:\")\nprint(\"-\" * 30)\nfor i in range(3):\n    review_text = decode_review(X_train_imdb[i])\n    sentiment = \"POSITIVE\" if y_train_imdb[i] == 1 else \"NEGATIVE\"\n    print(f\"\\nReview {i+1} ({sentiment}):\")\n    print(f\"Length: {len(X_train_imdb[i])} words\")\n    print(f\"Text: {review_text[:200]}...\")\n\n# Show the sequence length problem\nsequence_lengths = [len(x) for x in X_train_imdb]\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(sequence_lengths, bins=50, alpha=0.7)\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\nplt.axvline(np.mean(sequence_lengths), color='red', linestyle='--', \n           label=f'Mean: {np.mean(sequence_lengths):.0f}')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(sequence_lengths[:1000])), sorted(sequence_lengths[:1000]))\nplt.title('Sequence Lengths (First 1000 samples)')\nplt.xlabel('Sample Index')\nplt.ylabel('Number of Words')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Sequence Statistics:\")\nprint(f\"  Shortest review: {min(sequence_lengths)} words\")\nprint(f\"  Longest review: {max(sequence_lengths)} words\")\nprint(f\"  Average length: {np.mean(sequence_lengths):.1f} words\")\nprint(f\"  Standard deviation: {np.std(sequence_lengths):.1f} words\")\n\nprint(\"\\n‚ùó Problems for Dense Networks:\")\nprint(\"  ‚Ä¢ Variable length inputs (can't handle different sizes)\")\nprint(\"  ‚Ä¢ Would need to pad/truncate all sequences\")\nprint(\"  ‚Ä¢ No understanding of word order\")\nprint(\"  ‚Ä¢ Can't capture long-range dependencies\")\nprint(\"  ‚Ä¢ 'movie great' vs 'great movie' treated identically\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab 4.4: Introduction to Recurrent Neural Networks\n",
      "============================================================\n",
      "TensorFlow version: 2.20.0\n",
      "\n",
      "üéØ What are sequences?\n",
      "  ‚Ä¢ Text: 'I love deep learning' ‚Üí words have order!\n",
      "  ‚Ä¢ Time series: Stock prices over time\n",
      "  ‚Ä¢ Audio: Sound waves over time\n",
      "  ‚Ä¢ Video: Frames in temporal order\n",
      "\n",
      "‚ùì Why can't we use Dense or CNN layers?\n",
      "  ‚Ä¢ Dense: Treats all inputs independently\n",
      "  ‚Ä¢ CNN: Good for spatial patterns, not temporal\n",
      "  ‚Ä¢ We need memory to understand context!\n",
      "\n",
      "üìù Sequence Examples (word order matters!):\n",
      "  1. 'The movie was not bad'\n",
      "  2. 'The movie was bad'\n",
      "  3. 'Not the best movie'\n",
      "  4. 'The best movie'\n",
      "\n",
      "üß† RNNs can understand context and sequence dependencies!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Lab 4.4: Introduction to Recurrent Neural Networks\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "print(\"\\nüéØ What are sequences?\")\n",
    "print(\"  ‚Ä¢ Text: 'I love deep learning' ‚Üí words have order!\")\n",
    "print(\"  ‚Ä¢ Time series: Stock prices over time\")\n",
    "print(\"  ‚Ä¢ Audio: Sound waves over time\")\n",
    "print(\"  ‚Ä¢ Video: Frames in temporal order\")\n",
    "\n",
    "print(\"\\n‚ùì Why can't we use Dense or CNN layers?\")\n",
    "print(\"  ‚Ä¢ Dense: Treats all inputs independently\")\n",
    "print(\"  ‚Ä¢ CNN: Good for spatial patterns, not temporal\")\n",
    "print(\"  ‚Ä¢ We need memory to understand context!\")\n",
    "\n",
    "# Example of sequence importance\n",
    "examples = [\n",
    "    \"The movie was not bad\" ,  # Positive\n",
    "    \"The movie was bad\",      # Negative\n",
    "    \"Not the best movie\",     # Negative  \n",
    "    \"The best movie\",         # Positive\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Sequence Examples (word order matters!):\")\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"  {i}. '{example}'\")\n",
    "    \n",
    "print(\"\\nüß† RNNs can understand context and sequence dependencies!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}