{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.4: Introduction to Recurrent Neural Networks\n",
    "\n",
    "## Duration: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand why RNNs are perfect for sequence data\n",
    "- Build your first RNN using TensorFlow/Keras\n",
    "- Compare RNN performance with regular Dense networks on sequence tasks\n",
    "- Use SimpleRNN, LSTM, and GRU layers\n",
    "- Apply RNNs to text classification and time series problems\n",
    "\n",
    "## Prerequisites\n",
    "- **Labs 4.1, 4.2, & 4.3 completed** (TensorFlow basics, deep networks, CNNs)\n",
    "- Understanding of neural networks\n",
    "- Basic knowledge of sequences and time series\n",
    "\n",
    "## Key Concepts\n",
    "- **Sequences**: Data where order matters (text, time series, audio)\n",
    "- **Memory**: How RNNs remember information from previous steps\n",
    "- **LSTM**: Long Short-Term Memory for long sequences\n",
    "- **GRU**: Gated Recurrent Unit (simpler than LSTM)\n",
    "- **Text Processing**: Converting words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"Lab 4.4: Introduction to Recurrent Neural Networks\")\nprint(\"=\" * 60)\nprint(f\"TensorFlow version: {tf.__version__}\")\n\nprint(\"\\nüéØ What are sequences?\")\nprint(\"  ‚Ä¢ Text: 'I love deep learning' ‚Üí words have order!\")\nprint(\"  ‚Ä¢ Time series: Stock prices over time\")\nprint(\"  ‚Ä¢ Audio: Sound waves over time\")\nprint(\"  ‚Ä¢ Video: Frames in temporal order\")\n\nprint(\"\\n‚ùì Why can't we use Dense or CNN layers?\")\nprint(\"  ‚Ä¢ Dense: Treats all inputs independently\")\nprint(\"  ‚Ä¢ CNN: Good for spatial patterns, not temporal\")\nprint(\"  ‚Ä¢ We need memory to understand context!\")\n\n# Example of sequence importance\nexamples = [\n    \"The movie was not bad\",  # Positive\n    \"The movie was bad\",      # Negative\n    \"Not the best movie\",     # Negative  \n    \"The best movie\",         # Positive\n]\n\nprint(\"\\nüìù Sequence Examples (word order matters!):\")\nfor i, example in enumerate(examples, 1):\n    print(f\"  {i}. '{example}'\")\n    \nprint(\"\\nüß† RNNs can understand context and sequence dependencies!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: The Problem - Dense Networks Can't Handle Sequences\n\nLet's first see why regular networks fail on sequential data:"
  },
  {
   "cell_type": "code",
   "source": "## Step 4: Comparing RNN Types - LSTM vs GRU vs SimpleRNN\n\nNow let's build LSTM and GRU models and compare their performance with our SimpleRNN:",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Comparing Different RNN Architectures:\")\nprint(\"=\" * 60)\n\n# Build LSTM model\nlstm_model = keras.Sequential([\n    layers.Embedding(input_dim=10000, output_dim=32, input_length=max_length),\n    layers.LSTM(32, dropout=0.2),\n    layers.Dense(1, activation='sigmoid')\n], name='LSTM_Model')\n\n# Build GRU model\ngru_model = keras.Sequential([\n    layers.Embedding(input_dim=10000, output_dim=32, input_length=max_length),\n    layers.GRU(32, dropout=0.2),\n    layers.Dense(1, activation='sigmoid')\n], name='GRU_Model')\n\n# Compile models\nfor model in [lstm_model, gru_model]:\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\nprint(\"üîç Model Architecture Comparison:\")\nprint(\"-\" * 40)\n\nmodels = {\n    'SimpleRNN': simple_rnn_model,\n    'LSTM': lstm_model,\n    'GRU': gru_model\n}\n\nfor name, model in models.items():\n    if name != 'SimpleRNN':  # SimpleRNN already built\n        model.build(input_shape=(None, max_length))\n    \n    params = model.count_params()\n    rnn_layer = next(layer for layer in model.layers if 'rnn' in layer.__class__.__name__.lower() or \n                     'lstm' in layer.__class__.__name__.lower() or \n                     'gru' in layer.__class__.__name__.lower())\n    \n    print(f\"\\n{name}:\")\n    print(f\"  ‚Ä¢ Total parameters: {params:,}\")\n    print(f\"  ‚Ä¢ RNN layer type: {rnn_layer.__class__.__name__}\")\n    print(f\"  ‚Ä¢ Hidden units: {rnn_layer.units}\")\n\nprint(\"\\nüß† Understanding the Differences:\")\nprint(\"-\" * 40)\nprint(\"SimpleRNN:\")\nprint(\"  ‚úÖ Simple and fast\")\nprint(\"  ‚ùå Vanishing gradient problem\")\nprint(\"  ‚ùå Poor long-term memory\")\n\nprint(\"\\nLSTM (Long Short-Term Memory):\")\nprint(\"  ‚úÖ Solves vanishing gradient\")\nprint(\"  ‚úÖ Excellent long-term memory\")\nprint(\"  ‚ùå More complex, slower\")\n\nprint(\"\\nGRU (Gated Recurrent Unit):\")\nprint(\"  ‚úÖ Good long-term memory\")\nprint(\"  ‚úÖ Simpler than LSTM\")\nprint(\"  ‚úÖ Faster than LSTM\")\n\n# Train LSTM model\nprint(\"\\nüöÄ Training LSTM model...\")\nlstm_history = lstm_model.fit(\n    X_train_padded[:5000], y_train_imdb[:5000],\n    validation_split=0.2,\n    epochs=3,\n    batch_size=128,\n    verbose=0\n)\n\n# Train GRU model\nprint(\"üöÄ Training GRU model...\")\ngru_history = gru_model.fit(\n    X_train_padded[:5000], y_train_imdb[:5000],\n    validation_split=0.2,\n    epochs=3,\n    batch_size=128,\n    verbose=0\n)\n\n# Evaluate all models\nprint(\"\\nüìä Performance Comparison:\")\nprint(\"-\" * 30)\n\nresults = {}\nfor name, model in models.items():\n    loss, accuracy = model.evaluate(X_test_padded[:1000], y_test_imdb[:1000], verbose=0)\n    results[name] = {'accuracy': accuracy, 'loss': loss}\n    print(f\"{name:>10}: {accuracy:.4f} accuracy\")\n\n# Find best model\nbest_model = max(results.items(), key=lambda x: x[1]['accuracy'])\nprint(f\"\\nüèÜ Best performing model: {best_model[0]} ({best_model[1]['accuracy']:.4f})\")\n\n# Visualize training curves\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(simple_rnn_history.history['val_accuracy'], label='SimpleRNN', linewidth=2)\nplt.plot(lstm_history.history['val_accuracy'], label='LSTM', linewidth=2)\nplt.plot(gru_history.history['val_accuracy'], label='GRU', linewidth=2)\nplt.title('Validation Accuracy Comparison')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 2)\nmodel_names = list(results.keys())\naccuracies = [results[name]['accuracy'] for name in model_names]\ncolors = ['orange', 'blue', 'green']\nbars = plt.bar(model_names, accuracies, color=colors, alpha=0.7)\nplt.title('Final Test Accuracy')\nplt.ylabel('Accuracy')\nfor bar, acc in zip(bars, accuracies):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n             f'{acc:.3f}', ha='center', fontweight='bold')\nplt.grid(axis='y', alpha=0.3)\n\nplt.subplot(1, 3, 3)\nparam_counts = [results_model.count_params() for results_model in models.values()]\nplt.bar(model_names, param_counts, color=colors, alpha=0.7)\nplt.title('Parameter Count')\nplt.ylabel('Parameters')\nfor i, (bar, count) in enumerate(zip(plt.gca().patches, param_counts)):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,\n             f'{count:,}', ha='center', fontweight='bold', rotation=45 if count > 50000 else 0)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Key Insights:\")\nprint(\"  ‚Ä¢ LSTM/GRU typically outperform SimpleRNN\")\nprint(\"  ‚Ä¢ GRU often matches LSTM performance with fewer parameters\")\nprint(\"  ‚Ä¢ All RNNs handle sequences better than Dense networks\")\nprint(\"  ‚Ä¢ Choice depends on your specific task and computational budget\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Building Your First SimpleRNN\n\nNow let's create a Simple RNN to understand the basics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Preparing Sequential Data for RNNs:\")\nprint(\"=\" * 50)\n\n# Pad sequences to make them the same length\nmax_length = 200  # Reasonable length that captures most reviews\n\nprint(f\"Padding sequences to length {max_length}...\")\nX_train_padded = pad_sequences(X_train_imdb, maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test_imdb, maxlen=max_length, padding='post', truncating='post')\n\nprint(f\"Training data shape: {X_train_padded.shape}\")\nprint(f\"Test data shape: {X_test_padded.shape}\")\n\n# Show what padding looks like\nprint(\"\\nExample of padded sequence:\")\nsample_review = X_train_padded[0]\nprint(f\"Original length: {len(X_train_imdb[0])}\")\nprint(f\"Padded length: {len(sample_review)}\")\nprint(f\"First 20 words: {sample_review[:20]}\")\nprint(f\"Last 20 words: {sample_review[-20:]}\")\nprint(\"(Note: 0s are padding tokens)\")\n\n# Visualize the impact of padding\nplt.figure(figsize=(12, 4))\n\n# Before padding\nplt.subplot(1, 2, 1)\noriginal_lengths = [len(x) for x in X_train_imdb[:1000]]\nplt.hist(original_lengths, bins=30, alpha=0.7, color='blue')\nplt.title('Original Sequence Lengths')\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.axvline(max_length, color='red', linestyle='--', label=f'Max Length: {max_length}')\nplt.legend()\n\n# After padding\nplt.subplot(1, 2, 2)\npadded_lengths = [np.count_nonzero(x) for x in X_train_padded[:1000]]  # Count non-zero (non-padding)\nplt.hist(padded_lengths, bins=30, alpha=0.7, color='green')\nplt.title('Actual Content Lengths (After Padding)')\nplt.xlabel('Content Length')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate padding statistics\nactual_content_lengths = [np.count_nonzero(x) for x in X_train_padded]\ntruncated_samples = sum(1 for length in [len(x) for x in X_train_imdb] if length > max_length)\npadding_ratio = 1 - (np.mean(actual_content_lengths) / max_length)\n\nprint(f\"\\nüìä Padding Impact:\")\nprint(f\"  Samples truncated: {truncated_samples} ({truncated_samples/len(X_train_imdb)*100:.1f}%)\")\nprint(f\"  Average padding ratio: {padding_ratio:.2f} ({padding_ratio*100:.1f}% padding)\")\nprint(f\"  Efficiency: {(1-padding_ratio)*100:.1f}% of data is actual content\")\n\nprint(\"\\n‚úÖ Data Preparation Complete:\")\nprint(\"  ‚Ä¢ Sequences padded to uniform length\")\nprint(\"  ‚Ä¢ Truncated long sequences\")\nprint(\"  ‚Ä¢ Ready for RNN processing\")\nprint(\"  ‚Ä¢ Each word is represented by an integer ID\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "print(\"Preparing Sequential Data for RNNs:\")\nprint(\"=\" * 50)\n\n# Load IMDB movie review dataset for sentiment analysis\nprint(\"Loading IMDB movie reviews dataset...\")\n(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(num_words=10000)\n\nprint(f\"Training samples: {len(X_train_imdb)}\")\nprint(f\"Test samples: {len(X_test_imdb)}\")\nprint(f\"Classes: {len(set(y_train_imdb))} (0=negative, 1=positive)\")\n\n# Look at some sample reviews\nword_index = imdb.get_word_index()\nreverse_word_index = {value: key for key, value in word_index.items()}\n\ndef decode_review(encoded_review):\n    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n\nprint(\"\\nSample Reviews:\")\nprint(\"-\" * 30)\nfor i in range(3):\n    review_text = decode_review(X_train_imdb[i])\n    sentiment = \"POSITIVE\" if y_train_imdb[i] == 1 else \"NEGATIVE\"\n    print(f\"\\nReview {i+1} ({sentiment}):\")\n    print(f\"Length: {len(X_train_imdb[i])} words\")\n    print(f\"Text: {review_text[:200]}...\")\n\n# Show the sequence length problem\nsequence_lengths = [len(x) for x in X_train_imdb]\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(sequence_lengths, bins=50, alpha=0.7)\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\nplt.axvline(np.mean(sequence_lengths), color='red', linestyle='--', \n           label=f'Mean: {np.mean(sequence_lengths):.0f}')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(sequence_lengths[:1000])), sorted(sequence_lengths[:1000]))\nplt.title('Sequence Lengths (First 1000 samples)')\nplt.xlabel('Sample Index')\nplt.ylabel('Number of Words')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Sequence Statistics:\")\nprint(f\"  Shortest review: {min(sequence_lengths)} words\")\nprint(f\"  Longest review: {max(sequence_lengths)} words\")\nprint(f\"  Average length: {np.mean(sequence_lengths):.1f} words\")\nprint(f\"  Standard deviation: {np.std(sequence_lengths):.1f} words\")\n\nprint(\"\\n‚ùó Problems for Dense Networks:\")\nprint(\"  ‚Ä¢ Variable length inputs (can't handle different sizes)\")\nprint(\"  ‚Ä¢ Would need to pad/truncate all sequences\")\nprint(\"  ‚Ä¢ No understanding of word order\")\nprint(\"  ‚Ä¢ Can't capture long-range dependencies\")\nprint(\"  ‚Ä¢ 'movie great' vs 'great movie' treated identically\")\n\n# Pad sequences to make them the same length\nmax_length = 200  # Reasonable length that captures most reviews\n\nprint(f\"\\nüîß Padding sequences to length {max_length}...\")\nX_train_padded = pad_sequences(X_train_imdb, maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test_imdb, maxlen=max_length, padding='post', truncating='post')\n\nprint(f\"Training data shape: {X_train_padded.shape}\")\nprint(f\"Test data shape: {X_test_padded.shape}\")\n\n# Show what padding looks like\nprint(\"\\nExample of padded sequence:\")\nsample_review = X_train_padded[0]\nprint(f\"Original length: {len(X_train_imdb[0])}\")\nprint(f\"Padded length: {len(sample_review)}\")\nprint(f\"First 20 words: {sample_review[:20]}\")\nprint(f\"Last 20 words: {sample_review[-20:]}\")\nprint(\"(Note: 0s are padding tokens)\")\n\nprint(\"\\n‚úÖ Data Preparation Complete:\")\nprint(\"  ‚Ä¢ Sequences padded to uniform length\")\nprint(\"  ‚Ä¢ Ready for RNN processing\")\nprint(\"  ‚Ä¢ Each word is represented by an integer ID\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Preparing Sequential Data\n\nBefore we can use RNNs, we need to prepare our sequential data:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}