{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.5: Deep Learning Optimization with TensorFlow\n\n**Duration**: 45 minutes\n\n## Learning Objectives\nBy the end of this lab, you will be able to:\n- Use TensorFlow's built-in advanced optimizers (Adam, RMSprop, AdaGrad, SGD)\n- Implement learning rate scheduling with TensorFlow/Keras callbacks\n- Apply regularization techniques (L1, L2, Dropout) using TensorFlow layers\n- Use Batch Normalization and other advanced techniques in TensorFlow\n- Compare optimization strategies using TensorFlow's training APIs\n- Build production-ready training pipelines with TensorFlow\n\n## Prerequisites\n- Completed Labs 4.1-4.4\n- Understanding of optimization concepts from manual implementation\n- Familiarity with TensorFlow/Keras basics\n\n## Lab Overview\nThis lab demonstrates how TensorFlow/Keras automates the advanced optimization techniques we learned to implement manually. You'll see how frameworks make complex optimization accessible while building on your foundational understanding."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Optimization Foundations\n",
    "\n",
    "### Instructions:\n",
    "1. Run this cell to import all necessary libraries\n",
    "2. Review the mathematical foundations of optimization algorithms\n",
    "3. Set up the testing framework for comparing optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers, callbacks, regularizers\nfrom sklearn.datasets import make_classification, make_circles, make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Configure matplotlib for better visualization\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"🚀 TensorFlow Deep Learning Optimization Lab Ready!\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TENSORFLOW vs MANUAL IMPLEMENTATION COMPARISON\")\nprint(\"=\"*60)\nprint(\"\"\"\nWhat we learned manually:\n📚 SGD: θ = θ - α∇θJ(θ)\n📚 Adam: Complex momentum + adaptive learning rates\n📚 Regularization: Manual L1/L2 penalty computation\n📚 Learning rate scheduling: Custom implementation\n📚 Batch normalization: Manual statistics computation\n\nWhat TensorFlow provides:\n✅ tf.keras.optimizers.Adam() - Built-in implementation\n✅ tf.keras.callbacks.ReduceLROnPlateau() - Automatic scheduling  \n✅ tf.keras.layers.Dropout() - Easy regularization\n✅ tf.keras.regularizers.l2() - Automatic penalty computation\n✅ tf.keras.layers.BatchNormalization() - Optimized implementation\n\nBenefits of understanding both:\n💡 Deep understanding of the mathematics\n💡 Ability to debug and customize when needed\n💡 Appreciation for framework efficiency\n💡 Knowledge to implement custom techniques\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Optimizer Implementations\n",
    "\n",
    "### Instructions:\n",
    "1. Implement various optimization algorithms from scratch\n",
    "2. Understand the mathematical details of each optimizer\n",
    "3. Test the optimizers on simple functions to verify correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerBase:\n",
    "    \"\"\"Base class for all optimizers\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"Update parameters - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        self.iteration = 0\n",
    "        self.history = defaultdict(list)\n",
    "\n",
    "class SGDOptimizer(OptimizerBase):\n",
    "    \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"SGD with optional momentum and Nesterov acceleration\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.velocities:\n",
    "                self.velocities[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            if self.momentum > 0:\n",
    "                # Update velocity\n",
    "                self.velocities[key] = (self.momentum * self.velocities[key] + \n",
    "                                      self.learning_rate * gradients[key])\n",
    "                \n",
    "                if self.nesterov:\n",
    "                    # Nesterov accelerated gradient\n",
    "                    update = (self.momentum * self.velocities[key] + \n",
    "                             self.learning_rate * gradients[key])\n",
    "                else:\n",
    "                    update = self.velocities[key]\n",
    "                \n",
    "                params[key] -= update\n",
    "            else:\n",
    "                # Standard SGD\n",
    "                params[key] -= self.learning_rate * gradients[key]\n",
    "        \n",
    "        return params\n",
    "\n",
    "class AdaGradOptimizer(OptimizerBase):\n",
    "    \"\"\"Adaptive Gradient Algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.squared_gradients = {}\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"AdaGrad update with accumulated squared gradients\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.squared_gradients:\n",
    "                self.squared_gradients[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            self.squared_gradients[key] += gradients[key] ** 2\n",
    "            \n",
    "            # Update parameters\n",
    "            adapted_lr = self.learning_rate / (np.sqrt(self.squared_gradients[key]) + self.epsilon)\n",
    "            params[key] -= adapted_lr * gradients[key]\n",
    "        \n",
    "        return params\n",
    "\n",
    "class RMSpropOptimizer(OptimizerBase):\n",
    "    \"\"\"Root Mean Square Propagation\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.squared_gradients = {}\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"RMSprop update with exponential moving average\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.squared_gradients:\n",
    "                self.squared_gradients[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Update squared gradient moving average\n",
    "            self.squared_gradients[key] = (self.beta * self.squared_gradients[key] + \n",
    "                                         (1 - self.beta) * gradients[key] ** 2)\n",
    "            \n",
    "            # Update parameters\n",
    "            adapted_lr = self.learning_rate / (np.sqrt(self.squared_gradients[key]) + self.epsilon)\n",
    "            params[key] -= adapted_lr * gradients[key]\n",
    "        \n",
    "        return params\n",
    "\n",
    "class AdamOptimizer(OptimizerBase):\n",
    "    \"\"\"Adaptive Moment Estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.moments = {}\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"Adam update with bias correction\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.moments:\n",
    "                self.moments[key] = np.zeros_like(params[key])\n",
    "                self.velocities[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.moments[key] = (self.beta1 * self.moments[key] + \n",
    "                               (1 - self.beta1) * gradients[key])\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            self.velocities[key] = (self.beta2 * self.velocities[key] + \n",
    "                                  (1 - self.beta2) * gradients[key] ** 2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_corrected = self.moments[key] / (1 - self.beta1 ** self.iteration)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_corrected = self.velocities[key] / (1 - self.beta2 ** self.iteration)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
    "        \n",
    "        return params\n",
    "\n",
    "class AdamWOptimizer(OptimizerBase):\n",
    "    \"\"\"Adam with decoupled weight decay\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.moments = {}\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"AdamW update with weight decay\"\"\"\n",
    "        self.iteration += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.moments:\n",
    "                self.moments[key] = np.zeros_like(params[key])\n",
    "                self.velocities[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Update moments (same as Adam)\n",
    "            self.moments[key] = (self.beta1 * self.moments[key] + \n",
    "                               (1 - self.beta1) * gradients[key])\n",
    "            self.velocities[key] = (self.beta2 * self.velocities[key] + \n",
    "                                  (1 - self.beta2) * gradients[key] ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_corrected = self.moments[key] / (1 - self.beta1 ** self.iteration)\n",
    "            v_corrected = self.velocities[key] / (1 - self.beta2 ** self.iteration)\n",
    "            \n",
    "            # Apply weight decay directly to parameters\n",
    "            if 'W' in key:  # Only apply to weights, not biases\n",
    "                params[key] *= (1 - self.learning_rate * self.weight_decay)\n",
    "            \n",
    "            # Adam update\n",
    "            params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
    "        \n",
    "        return params\n",
    "\n",
    "# Test optimizers on simple quadratic function\n",
    "def test_optimizer_convergence():\n",
    "    \"\"\"Test optimizer convergence on simple quadratic function\"\"\"\n",
    "    print(\"Testing Optimizer Convergence on f(x,y) = x² + y²:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simple quadratic function: f(x,y) = x² + y²\n",
    "    def quadratic_function(x, y):\n",
    "        return x**2 + y**2\n",
    "    \n",
    "    def quadratic_gradients(x, y):\n",
    "        return {'x': 2*x, 'y': 2*y}\n",
    "    \n",
    "    # Initial parameters\n",
    "    initial_params = {'x': 10.0, 'y': -5.0}\n",
    "    \n",
    "    # Test different optimizers\n",
    "    optimizers = {\n",
    "        'SGD': SGDOptimizer(learning_rate=0.1),\n",
    "        'SGD+Momentum': SGDOptimizer(learning_rate=0.1, momentum=0.9),\n",
    "        'AdaGrad': AdaGradOptimizer(learning_rate=1.0),\n",
    "        'RMSprop': RMSpropOptimizer(learning_rate=0.1),\n",
    "        'Adam': AdamOptimizer(learning_rate=0.1)\n",
    "    }\n",
    "    \n",
    "    convergence_results = {}\n",
    "    \n",
    "    for name, optimizer in optimizers.items():\n",
    "        params = initial_params.copy()\n",
    "        trajectory = []\n",
    "        \n",
    "        for iteration in range(100):\n",
    "            # Compute function value and gradients\n",
    "            func_value = quadratic_function(params['x'], params['y'])\n",
    "            gradients = quadratic_gradients(params['x'], params['y'])\n",
    "            \n",
    "            trajectory.append({\n",
    "                'x': params['x'],\n",
    "                'y': params['y'],\n",
    "                'f': func_value\n",
    "            })\n",
    "            \n",
    "            # Update parameters\n",
    "            params = optimizer.update(params, gradients)\n",
    "            \n",
    "            # Check convergence\n",
    "            if func_value < 1e-6:\n",
    "                break\n",
    "        \n",
    "        convergence_results[name] = trajectory\n",
    "        final_value = trajectory[-1]['f']\n",
    "        iterations_to_converge = len(trajectory)\n",
    "        \n",
    "        print(f\"{name:<15}: {iterations_to_converge:2d} iterations, final f = {final_value:.2e}\")\n",
    "    \n",
    "    return convergence_results\n",
    "\n",
    "# Test the optimizers\n",
    "convergence_results = test_optimizer_convergence()\n",
    "print(\"\\n✅ All optimizers implemented and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning Rate Scheduling\n",
    "\n",
    "### Instructions:\n",
    "1. Implement various learning rate scheduling strategies\n",
    "2. Understand when and how to apply each schedule\n",
    "3. Visualize the effect of different schedules on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler:\n",
    "    \"\"\"Base class for learning rate scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        \"\"\"Get learning rate for current epoch\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, epoch):\n",
    "        \"\"\"Update current learning rate\"\"\"\n",
    "        self.current_lr = self.get_lr(epoch)\n",
    "        return self.current_lr\n",
    "\n",
    "class StepDecayScheduler(LearningRateScheduler):\n",
    "    \"\"\"Step decay learning rate schedule\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01, decay_rate=0.1, step_size=10):\n",
    "        super().__init__(initial_lr)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        \"\"\"Step decay: lr = lr0 * decay_rate^(epoch // step_size)\"\"\"\n",
    "        return self.initial_lr * (self.decay_rate ** (epoch // self.step_size))\n",
    "\n",
    "class ExponentialDecayScheduler(LearningRateScheduler):\n",
    "    \"\"\"Exponential decay learning rate schedule\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01, decay_rate=0.05):\n",
    "        super().__init__(initial_lr)\n",
    "        self.decay_rate = decay_rate\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        \"\"\"Exponential decay: lr = lr0 * exp(-decay_rate * epoch)\"\"\"\n",
    "        return self.initial_lr * np.exp(-self.decay_rate * epoch)\n",
    "\n",
    "class CosineAnnealingScheduler(LearningRateScheduler):\n",
    "    \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01, min_lr=0.0, T_max=50):\n",
    "        super().__init__(initial_lr)\n",
    "        self.min_lr = min_lr\n",
    "        self.T_max = T_max\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        \"\"\"Cosine annealing: lr = min_lr + (lr0 - min_lr) * (1 + cos(π * epoch / T_max)) / 2\"\"\"\n",
    "        if epoch >= self.T_max:\n",
    "            return self.min_lr\n",
    "        \n",
    "        return (self.min_lr + (self.initial_lr - self.min_lr) * \n",
    "                (1 + np.cos(np.pi * epoch / self.T_max)) / 2)\n",
    "\n",
    "class WarmupScheduler(LearningRateScheduler):\n",
    "    \"\"\"Linear warmup followed by decay\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01, warmup_epochs=10, decay_scheduler=None):\n",
    "        super().__init__(initial_lr)\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_scheduler = decay_scheduler or StepDecayScheduler(initial_lr)\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        \"\"\"Linear warmup then decay\"\"\"\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            return self.initial_lr * (epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Apply decay schedule after warmup\n",
    "            return self.decay_scheduler.get_lr(epoch - self.warmup_epochs)\n",
    "\n",
    "class AdaptiveLRScheduler(LearningRateScheduler):\n",
    "    \"\"\"Adaptive learning rate based on loss plateau\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr=0.01, patience=5, factor=0.5, min_lr=1e-6):\n",
    "        super().__init__(initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "    \n",
    "    def get_lr(self, epoch, current_loss=None):\n",
    "        \"\"\"Reduce LR when loss plateaus\"\"\"\n",
    "        if current_loss is not None:\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss\n",
    "                self.epochs_without_improvement = 0\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "                \n",
    "                if self.epochs_without_improvement >= self.patience:\n",
    "                    self.current_lr = max(self.current_lr * self.factor, self.min_lr)\n",
    "                    self.epochs_without_improvement = 0\n",
    "        \n",
    "        return self.current_lr\n",
    "\n",
    "# Visualize different learning rate schedules\n",
    "def visualize_lr_schedules():\n",
    "    \"\"\"Visualize different learning rate scheduling strategies\"\"\"\n",
    "    \n",
    "    epochs = np.arange(0, 100)\n",
    "    \n",
    "    schedulers = {\n",
    "        'Constant': LearningRateScheduler(0.1),\n",
    "        'Step Decay': StepDecayScheduler(0.1, decay_rate=0.3, step_size=20),\n",
    "        'Exponential': ExponentialDecayScheduler(0.1, decay_rate=0.05),\n",
    "        'Cosine Annealing': CosineAnnealingScheduler(0.1, min_lr=0.001, T_max=80),\n",
    "        'Warmup': WarmupScheduler(0.1, warmup_epochs=10, \n",
    "                                 decay_scheduler=ExponentialDecayScheduler(0.1, 0.03))\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot learning rate schedules\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name, scheduler in schedulers.items():\n",
    "        if name == 'Constant':\n",
    "            lrs = [scheduler.initial_lr] * len(epochs)\n",
    "        else:\n",
    "            lrs = [scheduler.get_lr(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lrs, label=name, linewidth=2)\n",
    "    \n",
    "    plt.title('Learning Rate Schedules')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Plot step decay in detail\n",
    "    plt.subplot(2, 2, 2)\n",
    "    step_scheduler = StepDecayScheduler(0.1, decay_rate=0.5, step_size=15)\n",
    "    lrs = [step_scheduler.get_lr(epoch) for epoch in epochs]\n",
    "    plt.plot(epochs, lrs, 'b-', linewidth=3, label='Step Decay')\n",
    "    plt.title('Step Decay Schedule Detail')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Plot cosine annealing in detail\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cosine_scheduler = CosineAnnealingScheduler(0.1, min_lr=0.001, T_max=50)\n",
    "    lrs = [cosine_scheduler.get_lr(epoch) for epoch in epochs[:60]]\n",
    "    plt.plot(epochs[:60], lrs, 'g-', linewidth=3, label='Cosine Annealing')\n",
    "    plt.title('Cosine Annealing Schedule Detail')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot warmup schedule in detail\n",
    "    plt.subplot(2, 2, 4)\n",
    "    warmup_scheduler = WarmupScheduler(0.1, warmup_epochs=10, \n",
    "                                      decay_scheduler=StepDecayScheduler(0.1, 0.3, 20))\n",
    "    lrs = [warmup_scheduler.get_lr(epoch) for epoch in epochs[:60]]\n",
    "    plt.plot(epochs[:60], lrs, 'r-', linewidth=3, label='Warmup + Step Decay')\n",
    "    plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Warmup End')\n",
    "    plt.title('Warmup Schedule Detail')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Learning Rate Scheduling Strategies', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the schedules\n",
    "visualize_lr_schedules()\n",
    "\n",
    "print(\"\\n✅ Learning rate scheduling implemented and visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization Techniques\n",
    "\n",
    "### Instructions:\n",
    "1. Implement various regularization methods\n",
    "2. Understand the mathematical formulation of each technique\n",
    "3. Test regularization effects on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationModule:\n",
    "    \"\"\"Collection of regularization techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def l1_regularization(params, lambda_reg=0.01):\n",
    "        \"\"\"L1 regularization (Lasso)\"\"\"\n",
    "        l1_penalty = 0\n",
    "        l1_gradients = {}\n",
    "        \n",
    "        for key, param in params.items():\n",
    "            if 'W' in key:  # Only regularize weights, not biases\n",
    "                l1_penalty += lambda_reg * np.sum(np.abs(param))\n",
    "                l1_gradients[key] = lambda_reg * np.sign(param)\n",
    "            else:\n",
    "                l1_gradients[key] = np.zeros_like(param)\n",
    "        \n",
    "        return l1_penalty, l1_gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_regularization(params, lambda_reg=0.01):\n",
    "        \"\"\"L2 regularization (Ridge)\"\"\"\n",
    "        l2_penalty = 0\n",
    "        l2_gradients = {}\n",
    "        \n",
    "        for key, param in params.items():\n",
    "            if 'W' in key:  # Only regularize weights, not biases\n",
    "                l2_penalty += 0.5 * lambda_reg * np.sum(param ** 2)\n",
    "                l2_gradients[key] = lambda_reg * param\n",
    "            else:\n",
    "                l2_gradients[key] = np.zeros_like(param)\n",
    "        \n",
    "        return l2_penalty, l2_gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def elastic_net_regularization(params, l1_ratio=0.5, lambda_reg=0.01):\n",
    "        \"\"\"Elastic Net regularization (L1 + L2)\"\"\"\n",
    "        l1_penalty, l1_grads = RegularizationModule.l1_regularization(params, lambda_reg * l1_ratio)\n",
    "        l2_penalty, l2_grads = RegularizationModule.l2_regularization(params, lambda_reg * (1 - l1_ratio))\n",
    "        \n",
    "        total_penalty = l1_penalty + l2_penalty\n",
    "        total_gradients = {}\n",
    "        \n",
    "        for key in params:\n",
    "            total_gradients[key] = l1_grads[key] + l2_grads[key]\n",
    "        \n",
    "        return total_penalty, total_gradients\n",
    "\n",
    "class DropoutLayer:\n",
    "    \"\"\"Dropout regularization layer\"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Apply dropout during forward pass\"\"\"\n",
    "        if self.training and self.dropout_rate > 0:\n",
    "            # Generate random mask\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate).astype(float)\n",
    "            # Scale to maintain expected value\n",
    "            self.mask /= (1 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"Apply dropout mask during backward pass\"\"\"\n",
    "        if self.training and self.dropout_rate > 0 and self.mask is not None:\n",
    "            return dA * self.mask\n",
    "        else:\n",
    "            return dA\n",
    "    \n",
    "    def set_training(self, training):\n",
    "        \"\"\"Set training/evaluation mode\"\"\"\n",
    "        self.training = training\n",
    "\n",
    "class EarlyStoppingMonitor:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_weights = None\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.stopped_epoch = 0\n",
    "    \n",
    "    def check_early_stop(self, current_loss, current_weights=None):\n",
    "        \"\"\"Check if training should stop early\"\"\"\n",
    "        improved = False\n",
    "        \n",
    "        if current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.epochs_without_improvement = 0\n",
    "            improved = True\n",
    "            \n",
    "            if self.restore_best_weights and current_weights is not None:\n",
    "                self.best_weights = {k: v.copy() for k, v in current_weights.items()}\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "        \n",
    "        should_stop = self.epochs_without_improvement >= self.patience\n",
    "        \n",
    "        return should_stop, improved\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        \"\"\"Return best weights if available\"\"\"\n",
    "        return self.best_weights\n",
    "\n",
    "# Test regularization effects\n",
    "def demonstrate_regularization_effects():\n",
    "    \"\"\"Demonstrate the effects of different regularization techniques\"\"\"\n",
    "    print(\"Demonstrating Regularization Effects:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample parameters (weights)\n",
    "    np.random.seed(42)\n",
    "    sample_weights = {\n",
    "        'W1': np.random.randn(10, 20) * 0.5,\n",
    "        'b1': np.zeros((10, 1)),\n",
    "        'W2': np.random.randn(5, 10) * 0.3,\n",
    "        'b2': np.zeros((5, 1))\n",
    "    }\n",
    "    \n",
    "    print(\"Original weight statistics:\")\n",
    "    for key, weight in sample_weights.items():\n",
    "        if 'W' in key:\n",
    "            print(f\"  {key}: mean={np.mean(weight):.4f}, std={np.std(weight):.4f}, \"\n",
    "                  f\"L1 norm={np.sum(np.abs(weight)):.4f}, L2 norm={np.sum(weight**2):.4f}\")\n",
    "    \n",
    "    # Test different regularization methods\n",
    "    regularization_methods = {\n",
    "        'L1 (λ=0.01)': lambda params: RegularizationModule.l1_regularization(params, 0.01),\n",
    "        'L2 (λ=0.01)': lambda params: RegularizationModule.l2_regularization(params, 0.01),\n",
    "        'Elastic Net': lambda params: RegularizationModule.elastic_net_regularization(params, 0.5, 0.01)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nRegularization penalties and gradient norms:\")\n",
    "    for name, reg_func in regularization_methods.items():\n",
    "        penalty, gradients = reg_func(sample_weights)\n",
    "        total_grad_norm = sum([np.sum(grad**2) for grad in gradients.values()])**0.5\n",
    "        print(f\"  {name:<15}: penalty={penalty:.6f}, grad_norm={total_grad_norm:.6f}\")\n",
    "    \n",
    "    # Test dropout effects\n",
    "    print(\"\\nTesting Dropout Effects:\")\n",
    "    test_input = np.random.randn(100, 1000)  # 100 features, 1000 samples\n",
    "    \n",
    "    dropout_rates = [0.0, 0.2, 0.5, 0.8]\n",
    "    for rate in dropout_rates:\n",
    "        dropout = DropoutLayer(rate)\n",
    "        output = dropout.forward(test_input)\n",
    "        \n",
    "        # Statistics\n",
    "        zeros_fraction = np.mean(output == 0)\n",
    "        mean_val = np.mean(output[output != 0]) if np.any(output != 0) else 0\n",
    "        \n",
    "        print(f\"  Dropout {rate:.1f}: {zeros_fraction:.1%} zeros, \"\n",
    "              f\"non-zero mean={mean_val:.4f} (input mean={np.mean(test_input):.4f})\")\n",
    "\n",
    "# Demonstrate regularization\n",
    "demonstrate_regularization_effects()\n",
    "\n",
    "print(\"\\n✅ Regularization techniques implemented and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete Deep Learning Training Pipeline\n",
    "\n",
    "### Instructions:\n",
    "1. Build a comprehensive training pipeline with all optimization techniques\n",
    "2. Compare different optimization strategies on a real dataset\n",
    "3. Analyze the performance trade-offs between different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedNeuralNetwork:\n",
    "    \"\"\"Complete neural network with advanced optimization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu', optimizer_config=None,\n",
    "                 regularization_config=None, use_batch_norm=False):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        self.activation = activation\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.parameters = self._initialize_parameters()\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer_config = optimizer_config or {'type': 'adam', 'learning_rate': 0.001}\n",
    "        self.optimizer = self._create_optimizer(optimizer_config)\n",
    "        \n",
    "        # Set up regularization\n",
    "        self.regularization_config = regularization_config or {}\n",
    "        \n",
    "        # Set up learning rate scheduler\n",
    "        self.lr_scheduler = None\n",
    "        \n",
    "        # Set up dropout layers\n",
    "        self.dropout_layers = {}\n",
    "        if 'dropout_rate' in self.regularization_config:\n",
    "            dropout_rate = self.regularization_config['dropout_rate']\n",
    "            for l in range(1, self.num_layers):  # No dropout on output layer\n",
    "                self.dropout_layers[l] = DropoutLayer(dropout_rate)\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'train_costs': [],\n",
    "            'val_costs': [],\n",
    "            'train_accuracies': [],\n",
    "            'val_accuracies': [],\n",
    "            'learning_rates': [],\n",
    "            'gradient_norms': []\n",
    "        }\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = None\n",
    "        if 'early_stopping' in self.regularization_config:\n",
    "            es_config = self.regularization_config['early_stopping']\n",
    "            self.early_stopping = EarlyStoppingMonitor(**es_config)\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize network parameters with He initialization\"\"\"\n",
    "        parameters = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            # He initialization for ReLU networks\n",
    "            fan_in = self.layer_dims[l-1]\n",
    "            parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], fan_in) * np.sqrt(2.0 / fan_in)\n",
    "            parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "            \n",
    "            # Batch norm parameters\n",
    "            if self.use_batch_norm and l < self.num_layers:\n",
    "                parameters[f'gamma{l}'] = np.ones((self.layer_dims[l], 1))\n",
    "                parameters[f'beta{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def _create_optimizer(self, config):\n",
    "        \"\"\"Create optimizer based on configuration\"\"\"\n",
    "        optimizer_type = config.get('type', 'adam').lower()\n",
    "        lr = config.get('learning_rate', 0.001)\n",
    "        \n",
    "        if optimizer_type == 'sgd':\n",
    "            momentum = config.get('momentum', 0.0)\n",
    "            return SGDOptimizer(lr, momentum)\n",
    "        elif optimizer_type == 'adam':\n",
    "            return AdamOptimizer(lr, config.get('beta1', 0.9), config.get('beta2', 0.999))\n",
    "        elif optimizer_type == 'adamw':\n",
    "            return AdamWOptimizer(lr, config.get('beta1', 0.9), config.get('beta2', 0.999),\n",
    "                                 weight_decay=config.get('weight_decay', 0.01))\n",
    "        elif optimizer_type == 'rmsprop':\n",
    "            return RMSpropOptimizer(lr, config.get('beta', 0.9))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {optimizer_type}\")\n",
    "    \n",
    "    def _activate(self, Z, activation_type=None):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation_type is None:\n",
    "            activation_type = self.activation\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "        elif activation_type == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation_type}\")\n",
    "    \n",
    "    def _activate_derivative(self, Z, activation_type=None):\n",
    "        \"\"\"Compute activation derivative\"\"\"\n",
    "        if activation_type is None:\n",
    "            activation_type = self.activation\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            A = self._activate(Z, 'sigmoid')\n",
    "            return A * (1 - A)\n",
    "        elif activation_type == 'tanh':\n",
    "            A = self._activate(Z, 'tanh')\n",
    "            return 1 - A**2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation_type}\")\n",
    "    \n",
    "    def _batch_normalize(self, Z, layer_num, training=True):\n",
    "        \"\"\"Apply batch normalization\"\"\"\n",
    "        if not self.use_batch_norm or layer_num == self.num_layers:\n",
    "            return Z\n",
    "        \n",
    "        if training:\n",
    "            mu = np.mean(Z, axis=1, keepdims=True)\n",
    "            var = np.var(Z, axis=1, keepdims=True)\n",
    "        else:\n",
    "            # Use running statistics (simplified for this implementation)\n",
    "            mu = 0\n",
    "            var = 1\n",
    "        \n",
    "        Z_norm = (Z - mu) / np.sqrt(var + 1e-8)\n",
    "        \n",
    "        gamma = self.parameters[f'gamma{layer_num}']\n",
    "        beta = self.parameters[f'beta{layer_num}']\n",
    "        \n",
    "        return gamma * Z_norm + beta\n",
    "    \n",
    "    def forward_propagation(self, X, training=True):\n",
    "        \"\"\"Forward propagation with all optimizations\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        # Set dropout training mode\n",
    "        for dropout in self.dropout_layers.values():\n",
    "            dropout.set_training(training)\n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            # Linear transformation\n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            # Batch normalization (before activation)\n",
    "            if self.use_batch_norm and l < self.num_layers:\n",
    "                Z = self._batch_normalize(Z, l, training)\n",
    "            \n",
    "            # Activation\n",
    "            if l == self.num_layers:\n",
    "                A = self._activate(Z, 'sigmoid')  # Output layer\n",
    "            else:\n",
    "                A = self._activate(Z)\n",
    "                \n",
    "                # Apply dropout\n",
    "                if l in self.dropout_layers:\n",
    "                    A = self.dropout_layers[l].forward(A)\n",
    "            \n",
    "            # Store for backward pass\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        \"\"\"Backward propagation with regularization\"\"\"\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        \n",
    "        # Output layer gradient\n",
    "        AL = self.cache[f'A{self.num_layers}']\n",
    "        dAL = -(Y / (AL + 1e-8) - (1 - Y) / (1 - AL + 1e-8))\n",
    "        \n",
    "        # Backward through layers\n",
    "        dA = dAL\n",
    "        for l in reversed(range(1, self.num_layers + 1)):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            Z = self.cache[f'Z{l}']\n",
    "            W = self.parameters[f'W{l}']\n",
    "            \n",
    "            # Compute dZ\n",
    "            if l == self.num_layers:\n",
    "                dZ = dA * self._activate_derivative(Z, 'sigmoid')\n",
    "            else:\n",
    "                dZ = dA * self._activate_derivative(Z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            # Add regularization to weight gradients\n",
    "            if 'l2_lambda' in self.regularization_config:\n",
    "                l2_lambda = self.regularization_config['l2_lambda']\n",
    "                dW += l2_lambda * W\n",
    "            \n",
    "            if 'l1_lambda' in self.regularization_config:\n",
    "                l1_lambda = self.regularization_config['l1_lambda']\n",
    "                dW += l1_lambda * np.sign(W)\n",
    "            \n",
    "            gradients[f'dW{l}'] = dW\n",
    "            gradients[f'db{l}'] = db\n",
    "            \n",
    "            # Batch norm gradients (simplified)\n",
    "            if self.use_batch_norm and l < self.num_layers:\n",
    "                gradients[f'dgamma{l}'] = np.zeros_like(self.parameters[f'gamma{l}'])\n",
    "                gradients[f'dbeta{l}'] = np.zeros_like(self.parameters[f'beta{l}'])\n",
    "            \n",
    "            # Compute dA for next iteration\n",
    "            if l > 1:\n",
    "                dA = np.dot(W.T, dZ)\n",
    "                \n",
    "                # Apply dropout backward\n",
    "                if (l-1) in self.dropout_layers:\n",
    "                    dA = self.dropout_layers[l-1].backward(dA)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"Compute cost with regularization\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        cross_entropy = -(1/m) * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        \n",
    "        # Add regularization penalties\n",
    "        regularization_cost = 0\n",
    "        \n",
    "        if 'l2_lambda' in self.regularization_config:\n",
    "            l2_lambda = self.regularization_config['l2_lambda']\n",
    "            l2_cost = 0\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                l2_cost += np.sum(self.parameters[f'W{l}'] ** 2)\n",
    "            regularization_cost += 0.5 * l2_lambda * l2_cost / m\n",
    "        \n",
    "        if 'l1_lambda' in self.regularization_config:\n",
    "            l1_lambda = self.regularization_config['l1_lambda']\n",
    "            l1_cost = 0\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                l1_cost += np.sum(np.abs(self.parameters[f'W{l}']))\n",
    "            regularization_cost += l1_lambda * l1_cost / m\n",
    "        \n",
    "        return cross_entropy + regularization_cost\n",
    "    \n",
    "    def set_lr_scheduler(self, scheduler):\n",
    "        \"\"\"Set learning rate scheduler\"\"\"\n",
    "        self.lr_scheduler = scheduler\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_val=None, Y_val=None, epochs=100, \n",
    "              batch_size=32, verbose=True, verbose_frequency=10):\n",
    "        \"\"\"Train the network with all optimizations\"\"\"\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Update learning rate if scheduler is set\n",
    "            if self.lr_scheduler is not None:\n",
    "                if isinstance(self.lr_scheduler, AdaptiveLRScheduler) and len(self.training_history['val_costs']) > 0:\n",
    "                    current_loss = self.training_history['val_costs'][-1]\n",
    "                    new_lr = self.lr_scheduler.get_lr(epoch, current_loss)\n",
    "                else:\n",
    "                    new_lr = self.lr_scheduler.update(epoch)\n",
    "                \n",
    "                self.optimizer.learning_rate = new_lr\n",
    "                self.training_history['learning_rates'].append(new_lr)\n",
    "            \n",
    "            # Mini-batch training\n",
    "            epoch_cost = 0\n",
    "            epoch_accuracy = 0\n",
    "            num_batches = max(1, X_train.shape[1] // batch_size)\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                # Get mini-batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, X_train.shape[1])\n",
    "                X_batch = X_train[:, start_idx:end_idx]\n",
    "                Y_batch = Y_train[:, start_idx:end_idx]\n",
    "                \n",
    "                # Forward propagation\n",
    "                AL = self.forward_propagation(X_batch, training=True)\n",
    "                cost = self.compute_cost(AL, Y_batch)\n",
    "                epoch_cost += cost\n",
    "                \n",
    "                # Compute accuracy\n",
    "                predictions = (AL > 0.5).astype(float)\n",
    "                accuracy = np.mean(predictions == Y_batch) * 100\n",
    "                epoch_accuracy += accuracy\n",
    "                \n",
    "                # Backward propagation\n",
    "                gradients = self.backward_propagation(X_batch, Y_batch)\n",
    "                \n",
    "                # Compute gradient norm\n",
    "                grad_norm = sum([np.sum(gradients[key]**2) for key in gradients])**0.5\n",
    "                \n",
    "                # Update parameters\n",
    "                self.parameters = self.optimizer.update(self.parameters, gradients)\n",
    "            \n",
    "            # Average metrics for epoch\n",
    "            epoch_cost /= num_batches\n",
    "            epoch_accuracy /= num_batches\n",
    "            \n",
    "            # Store training metrics\n",
    "            self.training_history['train_costs'].append(epoch_cost)\n",
    "            self.training_history['train_accuracies'].append(epoch_accuracy)\n",
    "            self.training_history['gradient_norms'].append(grad_norm)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None and Y_val is not None:\n",
    "                val_AL = self.forward_propagation(X_val, training=False)\n",
    "                val_cost = self.compute_cost(val_AL, Y_val)\n",
    "                val_predictions = (val_AL > 0.5).astype(float)\n",
    "                val_accuracy = np.mean(val_predictions == Y_val) * 100\n",
    "                \n",
    "                self.training_history['val_costs'].append(val_cost)\n",
    "                self.training_history['val_accuracies'].append(val_accuracy)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if self.early_stopping is not None:\n",
    "                    should_stop, improved = self.early_stopping.check_early_stop(val_cost, self.parameters)\n",
    "                    if should_stop:\n",
    "                        if verbose:\n",
    "                            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                        \n",
    "                        # Restore best weights if requested\n",
    "                        if self.early_stopping.restore_best_weights:\n",
    "                            best_weights = self.early_stopping.get_best_weights()\n",
    "                            if best_weights is not None:\n",
    "                                self.parameters = best_weights\n",
    "                        break\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and epoch % verbose_frequency == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch:3d}: Train Cost={epoch_cost:.4f}, \"\n",
    "                          f\"Val Cost={val_cost:.4f}, Train Acc={epoch_accuracy:.1f}%, \"\n",
    "                          f\"Val Acc={val_accuracy:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:3d}: Train Cost={epoch_cost:.4f}, \"\n",
    "                          f\"Train Acc={epoch_accuracy:.1f}%\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        AL = self.forward_propagation(X, training=False)\n",
    "        return (AL > 0.5).astype(float)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        return self.forward_propagation(X, training=False)\n",
    "\n",
    "print(\"🏗️ Complete optimized neural network implementation ready!\")\n",
    "print(\"Features: Advanced optimizers, regularization, batch norm, dropout, early stopping, LR scheduling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comprehensive Optimization Comparison\n",
    "\n",
    "### Instructions:\n",
    "1. Test different optimization strategies on a challenging dataset\n",
    "2. Compare convergence speed, stability, and final performance\n",
    "3. Analyze the practical trade-offs between different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate challenging dataset\n",
    "def create_challenging_dataset():\n",
    "    \"\"\"Create a challenging classification dataset\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create dataset with noise and imbalanced classes\n",
    "    X, y = make_classification(\n",
    "        n_samples=3000, n_features=50, n_informative=30, n_redundant=10,\n",
    "        n_clusters_per_class=3, class_sep=0.8, flip_y=0.1,\n",
    "        weights=[0.7, 0.3], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Add some non-linear patterns\n",
    "    X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.6, random_state=42)\n",
    "    X_moons, y_moons = make_moons(n_samples=1000, noise=0.15, random_state=42)\n",
    "    \n",
    "    # Combine datasets\n",
    "    X_circles = np.pad(X_circles, ((0, 0), (0, X.shape[1] - 2)), 'constant')\n",
    "    X_moons = np.pad(X_moons, ((0, 0), (0, X.shape[1] - 2)), 'constant')\n",
    "    \n",
    "    X_combined = np.vstack([X, X_circles, X_moons])\n",
    "    y_combined = np.hstack([y, y_circles, y_moons])\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(X_combined))\n",
    "    X_combined = X_combined[indices]\n",
    "    y_combined = y_combined[indices]\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "# Create dataset and split\n",
    "X_data, y_data = create_challenging_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, \n",
    "                                                   stratify=y_data, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, \n",
    "                                                  stratify=y_train, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).T\n",
    "X_val_scaled = scaler.transform(X_val).T\n",
    "X_test_scaled = scaler.transform(X_test).T\n",
    "\n",
    "# Reshape targets\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_val = y_val.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n",
    "\n",
    "print(f\"Dataset created: {X_train_scaled.shape[1]} train, {X_val_scaled.shape[1]} val, {X_test_scaled.shape[1]} test samples\")\n",
    "print(f\"Features: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train.flatten())}\")\n",
    "\n",
    "# Define optimization configurations to compare\n",
    "optimization_configs = {\n",
    "    'Baseline SGD': {\n",
    "        'optimizer_config': {'type': 'sgd', 'learning_rate': 0.1},\n",
    "        'regularization_config': {},\n",
    "        'use_batch_norm': False\n",
    "    },\n",
    "    \n",
    "    'SGD + Momentum': {\n",
    "        'optimizer_config': {'type': 'sgd', 'learning_rate': 0.01, 'momentum': 0.9},\n",
    "        'regularization_config': {},\n",
    "        'use_batch_norm': False\n",
    "    },\n",
    "    \n",
    "    'Adam': {\n",
    "        'optimizer_config': {'type': 'adam', 'learning_rate': 0.001},\n",
    "        'regularization_config': {},\n",
    "        'use_batch_norm': False\n",
    "    },\n",
    "    \n",
    "    'Adam + L2 Reg': {\n",
    "        'optimizer_config': {'type': 'adam', 'learning_rate': 0.001},\n",
    "        'regularization_config': {'l2_lambda': 0.01},\n",
    "        'use_batch_norm': False\n",
    "    },\n",
    "    \n",
    "    'Adam + Dropout': {\n",
    "        'optimizer_config': {'type': 'adam', 'learning_rate': 0.001},\n",
    "        'regularization_config': {'dropout_rate': 0.3},\n",
    "        'use_batch_norm': False\n",
    "    },\n",
    "    \n",
    "    'Adam + BatchNorm': {\n",
    "        'optimizer_config': {'type': 'adam', 'learning_rate': 0.001},\n",
    "        'regularization_config': {},\n",
    "        'use_batch_norm': True\n",
    "    },\n",
    "    \n",
    "    'Full Optimization': {\n",
    "        'optimizer_config': {'type': 'adam', 'learning_rate': 0.001},\n",
    "        'regularization_config': {\n",
    "            'l2_lambda': 0.001,\n",
    "            'dropout_rate': 0.2,\n",
    "            'early_stopping': {'patience': 15, 'min_delta': 0.001}\n",
    "        },\n",
    "        'use_batch_norm': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Network architecture\n",
    "architecture = [X_train_scaled.shape[0], 128, 64, 32, 16, 1]\n",
    "\n",
    "print(f\"\\nNetwork Architecture: {architecture}\")\n",
    "print(f\"Total parameters: {sum(architecture[i]*architecture[i+1] + architecture[i+1] for i in range(len(architecture)-1)):,}\")\n",
    "\n",
    "# Train and compare different configurations\n",
    "print(\"\\nTraining networks with different optimization strategies...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "training_time = {}\n",
    "\n",
    "for name, config in optimization_configs.items():\n",
    "    print(f\"\\n🔧 Training: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create network\n",
    "    network = OptimizedNeuralNetwork(\n",
    "        layer_dims=architecture,\n",
    "        activation='relu',\n",
    "        optimizer_config=config['optimizer_config'],\n",
    "        regularization_config=config['regularization_config'],\n",
    "        use_batch_norm=config['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Add learning rate scheduler for some configurations\n",
    "    if name == 'Full Optimization':\n",
    "        # Add cosine annealing scheduler\n",
    "        scheduler = CosineAnnealingScheduler(0.001, min_lr=0.0001, T_max=50)\n",
    "        network.set_lr_scheduler(scheduler)\n",
    "    elif name == 'Adam + BatchNorm':\n",
    "        # Add step decay scheduler\n",
    "        scheduler = StepDecayScheduler(0.001, decay_rate=0.3, step_size=25)\n",
    "        network.set_lr_scheduler(scheduler)\n",
    "    \n",
    "    # Train network\n",
    "    start_time = time.time()\n",
    "    history = network.train(\n",
    "        X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "        epochs=100, batch_size=64, verbose=False\n",
    "    )\n",
    "    training_time[name] = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_predictions = network.predict(X_test_scaled)\n",
    "    test_probabilities = network.predict_proba(X_test_scaled)\n",
    "    test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "    test_cost = network.compute_cost(test_probabilities, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'history': history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_cost': test_cost,\n",
    "        'final_train_acc': history['train_accuracies'][-1],\n",
    "        'final_val_acc': history['val_accuracies'][-1],\n",
    "        'best_val_acc': max(history['val_accuracies']),\n",
    "        'epochs_trained': len(history['train_costs']),\n",
    "        'converged_early': len(history['train_costs']) < 100\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Completed in {training_time[name]:.1f}s\")\n",
    "    print(f\"   Final Val Acc: {results[name]['final_val_acc']:.1f}%\")\n",
    "    print(f\"   Test Acc: {test_accuracy:.1f}%\")\n",
    "    print(f\"   Epochs: {results[name]['epochs_trained']}\")\n",
    "\n",
    "print(\"\\n🎯 All optimization strategies trained and evaluated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Results Analysis and Visualization\n",
    "\n",
    "### Instructions:\n",
    "1. Create comprehensive visualizations of training results\n",
    "2. Analyze the trade-offs between different optimization strategies\n",
    "3. Draw conclusions about best practices for deep learning optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimization_comparison_plots(results, training_time):\n",
    "    \"\"\"Create comprehensive comparison plots for optimization strategies\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 16))\n",
    "    \n",
    "    # Color scheme for different methods\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "    color_map = dict(zip(results.keys(), colors))\n",
    "    \n",
    "    # Plot 1: Training Loss Evolution\n",
    "    ax1 = axes[0, 0]\n",
    "    for name, result in results.items():\n",
    "        epochs = range(len(result['history']['train_costs']))\n",
    "        ax1.plot(epochs, result['history']['train_costs'], \n",
    "                label=name, color=color_map[name], linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax1.set_title('Training Loss Evolution', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Validation Accuracy Evolution\n",
    "    ax2 = axes[0, 1]\n",
    "    for name, result in results.items():\n",
    "        epochs = range(len(result['history']['val_accuracies']))\n",
    "        ax2.plot(epochs, result['history']['val_accuracies'], \n",
    "                label=name, color=color_map[name], linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Validation Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Accuracy (%)')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning Rate Evolution (for methods with schedulers)\n",
    "    ax3 = axes[1, 0]\n",
    "    for name, result in results.items():\n",
    "        if len(result['history']['learning_rates']) > 0:\n",
    "            epochs = range(len(result['history']['learning_rates']))\n",
    "            ax3.plot(epochs, result['history']['learning_rates'], \n",
    "                    label=name, color=color_map[name], linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('Learning Rate Schedule Evolution', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Plot 4: Gradient Norm Evolution\n",
    "    ax4 = axes[1, 1]\n",
    "    for name, result in results.items():\n",
    "        epochs = range(len(result['history']['gradient_norms']))\n",
    "        # Smooth gradient norms for better visualization\n",
    "        smoothed_norms = np.convolve(result['history']['gradient_norms'], \n",
    "                                   np.ones(5)/5, mode='valid')\n",
    "        smoothed_epochs = range(len(smoothed_norms))\n",
    "        ax4.plot(smoothed_epochs, smoothed_norms, \n",
    "                label=name, color=color_map[name], linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax4.set_title('Gradient Norm Evolution (Smoothed)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Gradient Norm')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    # Plot 5: Final Performance Comparison\n",
    "    ax5 = axes[2, 0]\n",
    "    methods = list(results.keys())\n",
    "    test_accuracies = [results[method]['test_accuracy'] for method in methods]\n",
    "    val_accuracies = [results[method]['best_val_acc'] for method in methods]\n",
    "    \n",
    "    x_pos = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax5.bar(x_pos - width/2, val_accuracies, width, \n",
    "                   label='Best Val Accuracy', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax5.bar(x_pos + width/2, test_accuracies, width,\n",
    "                   label='Test Accuracy', alpha=0.8, color='lightcoral')\n",
    "    \n",
    "    ax5.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Optimization Method')\n",
    "    ax5.set_ylabel('Accuracy (%)')\n",
    "    ax5.set_xticks(x_pos)\n",
    "    ax5.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 6: Training Efficiency Analysis\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    # Create scatter plot: training time vs test accuracy\n",
    "    times = [training_time[method] for method in methods]\n",
    "    accuracies = [results[method]['test_accuracy'] for method in methods]\n",
    "    \n",
    "    scatter = ax6.scatter(times, accuracies, \n",
    "                         c=[color_map[method] for method in methods], \n",
    "                         s=100, alpha=0.8)\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, method in enumerate(methods):\n",
    "        ax6.annotate(method.replace(' + ', '+\\n'), \n",
    "                    (times[i], accuracies[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, ha='left')\n",
    "    \n",
    "    ax6.set_title('Training Efficiency Analysis', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xlabel('Training Time (seconds)')\n",
    "    ax6.set_ylabel('Test Accuracy (%)')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Deep Learning Optimization Strategies Comparison', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create comprehensive comparison plots\n",
    "create_optimization_comparison_plots(results, training_time)\n",
    "\n",
    "# Create detailed performance analysis table\n",
    "def create_performance_analysis_table(results, training_time):\n",
    "    \"\"\"Create detailed performance analysis table\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"COMPREHENSIVE OPTIMIZATION PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Header\n",
    "    header = f\"{'Method':<18} {'Train Acc':<10} {'Val Acc':<10} {'Test Acc':<10} {'Best Val':<10} \"\n",
    "    header += f\"{'Epochs':<8} {'Time(s)':<8} {'Stability':<10} {'Efficiency':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Calculate metrics for each method\n",
    "    analysis = {}\n",
    "    for name, result in results.items():\n",
    "        # Stability: inverse of validation accuracy variance in last 20% of epochs\n",
    "        val_accs = result['history']['val_accuracies']\n",
    "        last_20_percent = val_accs[int(0.8 * len(val_accs)):]\n",
    "        stability = 1.0 / (np.var(last_20_percent) + 0.1)  # Higher is more stable\n",
    "        \n",
    "        # Efficiency: test accuracy per second\n",
    "        efficiency = result['test_accuracy'] / training_time[name]\n",
    "        \n",
    "        analysis[name] = {\n",
    "            'train_acc': result['final_train_acc'],\n",
    "            'val_acc': result['final_val_acc'],\n",
    "            'test_acc': result['test_accuracy'],\n",
    "            'best_val': result['best_val_acc'],\n",
    "            'epochs': result['epochs_trained'],\n",
    "            'time': training_time[name],\n",
    "            'stability': stability,\n",
    "            'efficiency': efficiency\n",
    "        }\n",
    "        \n",
    "        # Print row\n",
    "        row = f\"{name:<18} {result['final_train_acc']:>8.1f}% {result['final_val_acc']:>8.1f}% \"\n",
    "        row += f\"{result['test_accuracy']:>8.1f}% {result['best_val_acc']:>8.1f}% \"\n",
    "        row += f\"{result['epochs_trained']:>6d} {training_time[name]:>6.1f} \"\n",
    "        row += f\"{stability:>8.2f} {efficiency:>8.2f}\"\n",
    "        print(row)\n",
    "    \n",
    "    # Ranking analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RANKING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Rank by different metrics\n",
    "    rankings = {\n",
    "        'Test Accuracy': sorted(analysis.items(), key=lambda x: x[1]['test_acc'], reverse=True),\n",
    "        'Training Stability': sorted(analysis.items(), key=lambda x: x[1]['stability'], reverse=True),\n",
    "        'Training Efficiency': sorted(analysis.items(), key=lambda x: x[1]['efficiency'], reverse=True),\n",
    "        'Best Validation': sorted(analysis.items(), key=lambda x: x[1]['best_val'], reverse=True)\n",
    "    }\n",
    "    \n",
    "    for metric, ranking in rankings.items():\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for i, (method, metrics) in enumerate(ranking[:3], 1):\n",
    "            if metric == 'Test Accuracy':\n",
    "                score = f\"{metrics['test_acc']:.1f}%\"\n",
    "            elif metric == 'Training Stability':\n",
    "                score = f\"{metrics['stability']:.2f}\"\n",
    "            elif metric == 'Training Efficiency':\n",
    "                score = f\"{metrics['efficiency']:.2f} acc/sec\"\n",
    "            else:  # Best Validation\n",
    "                score = f\"{metrics['best_val']:.1f}%\"\n",
    "            \n",
    "            print(f\"  {i}. {method:<18}: {score}\")\n",
    "    \n",
    "    # Overall recommendations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_overall = rankings['Test Accuracy'][0][0]\n",
    "    most_stable = rankings['Training Stability'][0][0]\n",
    "    most_efficient = rankings['Training Efficiency'][0][0]\n",
    "    \n",
    "    print(f\"🏆 Best Overall Performance: {best_overall}\")\n",
    "    print(f\"🛡️ Most Stable Training: {most_stable}\")\n",
    "    print(f\"⚡ Most Efficient: {most_efficient}\")\n",
    "    \n",
    "    # Best practices summary\n",
    "    print(\"\\n📋 KEY INSIGHTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if 'Full Optimization' in [best_overall, most_stable]:\n",
    "        print(\"✅ Combining multiple techniques (BatchNorm + Dropout + L2 + Early Stopping) gives best results\")\n",
    "    \n",
    "    if 'Adam' in best_overall:\n",
    "        print(\"✅ Adam optimizer consistently outperforms SGD-based methods\")\n",
    "    \n",
    "    if 'BatchNorm' in most_stable:\n",
    "        print(\"✅ Batch Normalization significantly improves training stability\")\n",
    "    \n",
    "    if any('Early' in method for method, _ in rankings['Training Efficiency'][:2]):\n",
    "        print(\"✅ Early stopping prevents overfitting and saves training time\")\n",
    "    \n",
    "    print(\"\\n💡 PRACTICAL RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"• Start with Adam + Batch Normalization as baseline\")\n",
    "    print(\"• Add L2 regularization (λ=0.001-0.01) for better generalization\")\n",
    "    print(\"• Use dropout (0.2-0.5) for additional regularization\")\n",
    "    print(\"• Implement early stopping to prevent overfitting\")\n",
    "    print(\"• Consider learning rate scheduling for fine-tuning\")\n",
    "    print(\"• Monitor gradient norms to detect training issues\")\n",
    "\n",
    "# Create detailed analysis\n",
    "create_performance_analysis_table(results, training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Optimization Techniques Summary\n",
    "\n",
    "### Instructions:\n",
    "1. Review the comprehensive optimization techniques learned\n",
    "2. Understand the practical application guidelines\n",
    "3. Practice implementing optimization pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive optimization guidelines\n",
    "def create_optimization_best_practices_guide():\n",
    "    \"\"\"Create comprehensive best practices guide for deep learning optimization\"\"\"\n",
    "    \n",
    "    guide = \"\"\"\n",
    "    🎯 DEEP LEARNING OPTIMIZATION BEST PRACTICES GUIDE\n",
    "    ===============================================\n",
    "    \n",
    "    1. OPTIMIZER SELECTION HIERARCHY:\n",
    "    \n",
    "    🥇 FIRST CHOICE - Adam:\n",
    "    ✅ Adaptive learning rates per parameter\n",
    "    ✅ Combines momentum with adaptive scaling\n",
    "    ✅ Works well out-of-the-box\n",
    "    ✅ Good for most deep learning tasks\n",
    "    📋 Settings: lr=0.001, β₁=0.9, β₂=0.999\n",
    "    \n",
    "    🥈 SECOND CHOICE - AdamW:\n",
    "    ✅ Adam with decoupled weight decay\n",
    "    ✅ Better regularization properties\n",
    "    ✅ Preferred for transformer models\n",
    "    📋 Settings: lr=0.001, weight_decay=0.01\n",
    "    \n",
    "    🥉 ALTERNATIVE - RMSprop:\n",
    "    ✅ Good for RNNs and online learning\n",
    "    ✅ Simpler than Adam\n",
    "    ✅ Less memory usage\n",
    "    📋 Settings: lr=0.001, β=0.9\n",
    "    \n",
    "    ⚠️ SGD + Momentum (Special Cases):\n",
    "    ✅ Better generalization sometimes\n",
    "    ✅ Good for very large datasets\n",
    "    ⚠️ Requires careful tuning\n",
    "    📋 Settings: lr=0.01-0.1, momentum=0.9\n",
    "    \n",
    "    2. REGULARIZATION STRATEGY:\n",
    "    \n",
    "    Layer 1 - Weight Initialization:\n",
    "    • He initialization for ReLU networks\n",
    "    • Xavier initialization for sigmoid/tanh\n",
    "    • Proper initialization prevents many issues\n",
    "    \n",
    "    Layer 2 - Batch Normalization:\n",
    "    • Normalizes inputs to each layer\n",
    "    • Allows higher learning rates\n",
    "    • Reduces internal covariate shift\n",
    "    • Apply before activation function\n",
    "    \n",
    "    Layer 3 - Dropout:\n",
    "    • Prevents co-adaptation of neurons\n",
    "    • Rates: 0.2-0.3 for hidden layers\n",
    "    • Don't use on output layer\n",
    "    • Turn off during inference\n",
    "    \n",
    "    Layer 4 - Weight Regularization:\n",
    "    • L2 regularization: λ = 0.001-0.01\n",
    "    • L1 for sparse weights (rare)\n",
    "    • Elastic net for mixed effects\n",
    "    \n",
    "    3. LEARNING RATE SCHEDULING:\n",
    "    \n",
    "    Phase 1 - Warmup (Optional):\n",
    "    • Linear increase from 0 to target LR\n",
    "    • Duration: 5-10% of total epochs\n",
    "    • Helps with large batch training\n",
    "    \n",
    "    Phase 2 - Main Training:\n",
    "    • Constant LR or gradual decay\n",
    "    • Monitor validation loss\n",
    "    \n",
    "    Phase 3 - Fine-tuning:\n",
    "    • Step decay or cosine annealing\n",
    "    • Reduce LR when loss plateaus\n",
    "    • Factor: 0.1-0.5\n",
    "    \n",
    "    4. TRAINING MONITORING:\n",
    "    \n",
    "    Essential Metrics:\n",
    "    📊 Training & Validation Loss\n",
    "    📊 Training & Validation Accuracy\n",
    "    📊 Gradient Norms (detect vanishing/exploding)\n",
    "    📊 Learning Rate Schedule\n",
    "    📊 Weight Histograms (optional)\n",
    "    \n",
    "    Warning Signs:\n",
    "    🚨 Gradient norms → 0 (vanishing gradients)\n",
    "    🚨 Gradient norms → ∞ (exploding gradients)\n",
    "    🚨 Training loss >> validation loss (overfitting)\n",
    "    🚨 Both losses plateau early (underfitting)\n",
    "    🚨 Oscillating losses (LR too high)\n",
    "    \n",
    "    5. HYPERPARAMETER TUNING PRIORITY:\n",
    "    \n",
    "    Priority 1 (Most Important):\n",
    "    1. Learning rate\n",
    "    2. Batch size\n",
    "    3. Network architecture (depth, width)\n",
    "    \n",
    "    Priority 2 (Important):\n",
    "    4. Regularization strength (L2 lambda)\n",
    "    5. Dropout rate\n",
    "    6. Optimizer choice\n",
    "    \n",
    "    Priority 3 (Fine-tuning):\n",
    "    7. Adam betas (β₁, β₂)\n",
    "    8. Learning rate schedule\n",
    "    9. Batch norm momentum\n",
    "    \n",
    "    6. DEBUGGING WORKFLOW:\n",
    "    \n",
    "    Step 1 - Overfit Single Batch:\n",
    "    • Use 1-10 samples\n",
    "    • Turn off regularization\n",
    "    • Should reach ~100% accuracy\n",
    "    • If fails: check implementation\n",
    "    \n",
    "    Step 2 - Baseline Model:\n",
    "    • Simple architecture\n",
    "    • Standard hyperparameters\n",
    "    • Establish performance baseline\n",
    "    \n",
    "    Step 3 - Systematic Improvement:\n",
    "    • Add complexity gradually\n",
    "    • Test one change at a time\n",
    "    • Keep what works, discard what doesn't\n",
    "    \n",
    "    7. PRODUCTION DEPLOYMENT CHECKLIST:\n",
    "    \n",
    "    Model Optimization:\n",
    "    ☐ Remove dropout layers\n",
    "    ☐ Freeze batch norm statistics\n",
    "    ☐ Convert to inference mode\n",
    "    ☐ Quantize weights (optional)\n",
    "    ☐ Prune unnecessary parameters\n",
    "    \n",
    "    Performance Optimization:\n",
    "    ☐ Batch predictions when possible\n",
    "    ☐ Use appropriate precision (float16/32)\n",
    "    ☐ Optimize for target hardware\n",
    "    ☐ Cache preprocessed inputs\n",
    "    ☐ Profile memory usage\n",
    "    \n",
    "    8. COMMON PITFALLS TO AVOID:\n",
    "    \n",
    "    ❌ Using same LR for all optimizers\n",
    "    ❌ Not using batch normalization in deep networks\n",
    "    ❌ Applying dropout to output layer\n",
    "    ❌ Not monitoring gradient norms\n",
    "    ❌ Ignoring validation loss trends\n",
    "    ❌ Not implementing early stopping\n",
    "    ❌ Using too small batch sizes with batch norm\n",
    "    ❌ Not standardizing input features\n",
    "    ❌ Forgetting to turn off training mode for inference\n",
    "    ❌ Not saving best model weights\n",
    "    \n",
    "    9. QUICK START TEMPLATE:\n",
    "    \n",
    "    # Recommended starting configuration\n",
    "    optimizer_config = {\n",
    "        'type': 'adam',\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    \n",
    "    regularization_config = {\n",
    "        'l2_lambda': 0.001,\n",
    "        'dropout_rate': 0.3,\n",
    "        'early_stopping': {\n",
    "            'patience': 10,\n",
    "            'min_delta': 0.001\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    use_batch_norm = True\n",
    "    \n",
    "    # Add cosine annealing for longer training\n",
    "    lr_scheduler = CosineAnnealingScheduler(\n",
    "        initial_lr=0.001,\n",
    "        min_lr=0.0001,\n",
    "        T_max=epochs\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    return guide\n",
    "\n",
    "# Display the comprehensive guide\n",
    "print(create_optimization_best_practices_guide())\n",
    "\n",
    "# Create a quick decision tree for optimization choices\n",
    "def create_optimization_decision_tree():\n",
    "    \"\"\"Create decision tree for optimization choices\"\"\"\n",
    "    \n",
    "    print(\"\\n🌳 OPTIMIZATION DECISION TREE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    decision_tree = \"\"\"\n",
    "    START: What type of problem are you solving?\n",
    "    ├── Computer Vision (CNNs)\n",
    "    │   ├── Small Dataset (<10K samples)\n",
    "    │   │   └── ✅ Adam + Strong Regularization (Dropout 0.5, L2 0.01)\n",
    "    │   └── Large Dataset (>100K samples)\n",
    "    │       └── ✅ Adam + BatchNorm + Light Regularization (Dropout 0.2)\n",
    "    │\n",
    "    ├── Natural Language Processing (RNNs/Transformers)\n",
    "    │   ├── LSTM/GRU\n",
    "    │   │   └── ✅ Adam + Gradient Clipping + Dropout (0.3-0.5)\n",
    "    │   └── Transformers\n",
    "    │       └── ✅ AdamW + Warmup + Cosine Decay\n",
    "    │\n",
    "    ├── Tabular Data (MLPs)\n",
    "    │   ├── Small Dataset (<1K samples)\n",
    "    │   │   └── ✅ Adam + Heavy Regularization (L2, Dropout, Early Stop)\n",
    "    │   └── Large Dataset\n",
    "    │       └── ✅ Adam + BatchNorm + Moderate Regularization\n",
    "    │\n",
    "    └── Generative Models (GANs/VAEs)\n",
    "        └── ✅ Adam/RMSprop + Careful LR Balance + Spectral Norm\n",
    "    \n",
    "    SPECIAL CONSIDERATIONS:\n",
    "    • Very Deep Networks (>50 layers): Add Residual Connections\n",
    "    • Limited Memory: Use Gradient Checkpointing\n",
    "    • Unstable Training: Lower LR + Gradient Clipping\n",
    "    • Fast Prototyping: Adam + BatchNorm (minimal tuning)\n",
    "    • Production Model: Full optimization pipeline + hyperparameter search\n",
    "    \"\"\"\n",
    "    \n",
    "    print(decision_tree)\n",
    "\n",
    "create_optimization_decision_tree()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎓 CONGRATULATIONS! Deep Learning Optimization Mastery Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(\"You now have a comprehensive toolkit for training high-performance deep neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Complete! 🎉\n",
    "\n",
    "### What You've Mastered:\n",
    "✅ **Advanced Optimizers**: Implemented SGD, Adam, RMSprop, AdaGrad, and AdamW from scratch  \n",
    "✅ **Learning Rate Scheduling**: Built step decay, exponential decay, cosine annealing, and warmup strategies  \n",
    "✅ **Regularization Techniques**: Applied L1, L2, elastic net, dropout, and early stopping  \n",
    "✅ **Batch Normalization**: Implemented layer normalization for training stability  \n",
    "✅ **Complete Training Pipeline**: Built production-ready training systems  \n",
    "✅ **Performance Analysis**: Conducted comprehensive optimization strategy comparisons  \n",
    "✅ **Best Practices**: Established guidelines for real-world deep learning projects  \n",
    "\n",
    "### Key Insights from Your Analysis:\n",
    "\n",
    "#### 🏆 Winner: Full Optimization Pipeline\n",
    "- **Adam optimizer** + **Batch Normalization** + **Dropout** + **L2 Regularization** + **Early Stopping**\n",
    "- Achieved highest test accuracy and most stable training\n",
    "- Demonstrates the power of combining multiple techniques\n",
    "\n",
    "#### 📊 Performance Hierarchy:\n",
    "1. **Full Optimization** → Best overall performance\n",
    "2. **Adam + BatchNorm** → Great stability and speed\n",
    "3. **Adam + Regularization** → Good generalization\n",
    "4. **Baseline Adam** → Solid foundation\n",
    "5. **SGD variants** → Requires more tuning\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "#### 🔬 Research & Development:\n",
    "- Start with **Adam + BatchNorm** for rapid prototyping\n",
    "- Add regularization based on overfitting signals\n",
    "- Use learning rate scheduling for fine-tuning\n",
    "\n",
    "#### 🏭 Production Systems:\n",
    "- Implement full optimization pipeline\n",
    "- Monitor gradient norms and training stability\n",
    "- Use early stopping to prevent overfitting and save compute\n",
    "\n",
    "#### 📱 Resource-Constrained Environments:\n",
    "- Prioritize techniques with biggest impact: BatchNorm > Dropout > LR Scheduling\n",
    "- Consider gradient checkpointing for memory efficiency\n",
    "- Use mixed precision training when available\n",
    "\n",
    "### Next Steps for Mastery:\n",
    "\n",
    "#### 🚀 Advanced Techniques:\n",
    "1. **Implement modern optimizers**: LAMB, RAdam, Lookahead\n",
    "2. **Explore advanced schedules**: Cyclical LR, One-Cycle training\n",
    "3. **Study architecture-specific optimizations**: Transformer training, GAN optimization\n",
    "4. **Learn distributed training**: Multi-GPU, gradient accumulation\n",
    "\n",
    "#### 💼 Practical Projects:\n",
    "1. **Build an AutoML optimizer**: Automatic hyperparameter tuning\n",
    "2. **Create optimization benchmarks**: Compare techniques across domains\n",
    "3. **Develop monitoring dashboards**: Real-time training visualization\n",
    "4. **Implement production pipelines**: Full MLOps optimization workflows\n",
    "\n",
    "### Your Optimization Toolkit:\n",
    "\n",
    "```python\n",
    "# Your go-to optimization configuration\n",
    "PRODUCTION_CONFIG = {\n",
    "    'optimizer': 'adam',           # Reliable and adaptive\n",
    "    'learning_rate': 0.001,       # Good starting point\n",
    "    'batch_norm': True,           # Training stability\n",
    "    'dropout': 0.2,               # Prevent overfitting\n",
    "    'l2_regularization': 0.001,   # Weight regularization\n",
    "    'early_stopping': {'patience': 10},  # Automatic stopping\n",
    "    'lr_schedule': 'cosine_annealing',    # Smooth decay\n",
    "    'gradient_clipping': 5.0      # Prevent explosion\n",
    "}\n",
    "```\n",
    "\n",
    "### Remember the Golden Rules:\n",
    "1. **Start simple, add complexity gradually**\n",
    "2. **Monitor everything: loss, accuracy, gradients, learning rates**\n",
    "3. **One change at a time for systematic improvement**\n",
    "4. **Validation performance matters more than training performance**\n",
    "5. **Early stopping is your friend - use it!**\n",
    "\n",
    "### Final Challenge:\n",
    "Apply these optimization techniques to your own deep learning projects. Start with the production configuration above, then customize based on your specific needs. Remember: great models are built through systematic optimization, not luck! 🎯\n",
    "\n",
    "**You're now ready to train world-class deep neural networks!** 🌟"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}