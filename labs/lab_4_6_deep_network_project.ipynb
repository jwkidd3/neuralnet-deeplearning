{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6: Complete Deep Network Project with TensorFlow - Production Image Classification System\n\n**Duration**: 45 minutes\n\n## Learning Objectives\nBy the end of this lab, you will be able to:\n- Design and implement a complete production-ready deep neural network using TensorFlow/Keras\n- Apply all advanced TensorFlow techniques learned in previous labs\n- Build a comprehensive image classification system with modern TensorFlow practices\n- Implement advanced TensorFlow features: custom layers, callbacks, and training loops\n- Deploy and evaluate a TensorFlow model in production-ready format\n- Create comprehensive model documentation and performance analysis\n\n## Prerequisites\n- Completed Labs 4.1, 4.2, 4.3, 4.4, and 4.5\n- Understanding of TensorFlow/Keras architecture design\n- Familiarity with TensorFlow optimization and regularization techniques\n\n## Project Overview\nThis capstone lab combines everything learned in the TensorFlow deep learning module. You'll build a complete image classification system using TensorFlow/Keras with the CIFAR-10 dataset, implementing state-of-the-art architectures with modern TensorFlow practices including custom layers, advanced callbacks, model subclassing, and production deployment techniques."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Project Setup and Data Preparation with TensorFlow\n\n### Instructions:\n1. Set up the complete TensorFlow development environment\n2. Load and preprocess the CIFAR-10 dataset using tf.data\n3. Implement TensorFlow data augmentation techniques\n4. Create TensorFlow visualization utilities for analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers, callbacks, regularizers\nfrom tensorflow.keras.datasets import cifar10\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Configure matplotlib\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11\n\nprint(\"🚀 TensorFlow Deep Network Project Environment Ready!\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nif tf.config.list_physical_devices('GPU'):\n    print(\"✅ GPU detected - enabling GPU acceleration\")\n    # Configure GPU memory growth\n    for gpu in tf.config.list_physical_devices('GPU'):\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"💻 Using CPU - consider enabling GPU for better performance\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TENSORFLOW DEEP LEARNING PROJECT OVERVIEW\")\nprint(\"=\"*60)\nprint(\"\"\"\nThis capstone project demonstrates:\n✅ Complete TensorFlow/Keras ecosystem mastery\n✅ Advanced architecture design with custom layers\n✅ Modern training techniques and callbacks\n✅ Production deployment with TensorFlow Serving\n✅ Comprehensive model evaluation and monitoring\n✅ Integration of all previous lab concepts\n\nKey TensorFlow Features We'll Use:\n🔥 tf.data for efficient data pipelines\n🔥 tf.keras.Model subclassing for custom architectures  \n🔥 tf.keras.callbacks for advanced training control\n🔥 tf.keras.utils for model visualization and analysis\n🔥 tf.saved_model for production deployment\n🔥 tf.function for performance optimization\n🔥 Mixed precision training for efficiency\n🔥 TensorBoard for comprehensive monitoring\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load and prepare CIFAR-10 dataset using TensorFlow\nprint(\"Loading CIFAR-10 dataset using TensorFlow...\")\n\n# Load CIFAR-10 data\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# CIFAR-10 class names\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Training samples: {x_train.shape[0]:,}\")\nprint(f\"Test samples: {x_test.shape[0]:,}\")\nprint(f\"Image shape: {x_train.shape[1:]}\")\nprint(f\"Number of classes: {len(class_names)}\")\n\n# Display dataset information\nfig, axes = plt.subplots(2, 5, figsize=(15, 8))\naxes = axes.ravel()\n\nprint(f\"\\nVisualizing sample images from each class...\")\n\n# Show one sample from each class\nfor i in range(10):\n    # Find first occurrence of each class\n    class_idx = np.where(y_train.flatten() == i)[0][0]\n    \n    axes[i].imshow(x_train[class_idx])\n    axes[i].set_title(f'{class_names[i]}')\n    axes[i].axis('off')\n\nplt.suptitle('CIFAR-10 Dataset Sample Images', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Analyze class distribution\nunique, counts = np.unique(y_train, return_counts=True)\n\nplt.figure(figsize=(12, 5))\n\n# Bar plot\nplt.subplot(1, 2, 1)\nbars = plt.bar(class_names, counts, color=plt.cm.tab10(np.arange(10)))\nplt.title('CIFAR-10 Training Set Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Number of Samples')\nplt.xticks(rotation=45)\n\nfor bar, count in zip(bars, counts):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n            str(count), ha='center', va='bottom')\n\n# Pie chart\nplt.subplot(1, 2, 2)\nplt.pie(counts, labels=class_names, autopct='%1.1f%%')\nplt.title('Class Distribution (Percentage)')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nDataset Analysis:\")\nprint(f\"✅ Balanced dataset: {len(set(counts)) == 1}\")\nprint(f\"✅ Total training samples: {len(y_train):,}\")\nprint(f\"✅ Samples per class: {counts[0]:,}\")\n\nprint(\"\\n✅ CIFAR-10 dataset loaded and analyzed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create efficient TensorFlow data pipeline with tf.data\ndef create_data_pipeline(x_data, y_data, batch_size=32, shuffle=True, augment=False, cache=True):\n    \"\"\"\n    Create efficient TensorFlow data pipeline using tf.data\n    \n    Args:\n        x_data: Input images\n        y_data: Labels\n        batch_size: Batch size for training\n        shuffle: Whether to shuffle the data\n        augment: Whether to apply data augmentation\n        cache: Whether to cache the dataset in memory\n    \n    Returns:\n        tf.data.Dataset: Configured dataset\n    \"\"\"\n    # Convert to tf.data.Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    \n    # Cache dataset in memory for performance (if it fits)\n    if cache:\n        dataset = dataset.cache()\n    \n    # Shuffle if requested\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=1000)\n    \n    # Normalize pixel values to [0, 1]\n    def normalize_images(image, label):\n        return tf.cast(image, tf.float32) / 255.0, label\n    \n    dataset = dataset.map(normalize_images, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Apply data augmentation for training\n    if augment:\n        def augment_image(image, label):\n            # Random horizontal flip\n            image = tf.image.random_flip_left_right(image)\n            \n            # Random rotation\n            image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n            \n            # Random brightness and contrast\n            image = tf.image.random_brightness(image, max_delta=0.1)\n            image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n            \n            # Random saturation\n            image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n            \n            # Ensure values stay in [0, 1]\n            image = tf.clip_by_value(image, 0.0, 1.0)\n            \n            return image, label\n        \n        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Batch the data\n    dataset = dataset.batch(batch_size)\n    \n    # Prefetch for performance\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n# Split data into train/validation sets\nfrom sklearn.model_selection import train_test_split\n\nx_train_split, x_val, y_train_split, y_val = train_test_split(\n    x_train, y_train, test_size=0.1, stratify=y_train, random_state=42\n)\n\nprint(\"Creating TensorFlow data pipelines...\")\nprint(f\"Training set: {len(x_train_split):,} samples\")\nprint(f\"Validation set: {len(x_val):,} samples\") \nprint(f\"Test set: {len(x_test):,} samples\")\n\n# Create datasets with different configurations\nBATCH_SIZE = 32\n\n# Training dataset with augmentation\ntrain_dataset = create_data_pipeline(\n    x_train_split, y_train_split, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    augment=True, \n    cache=True\n)\n\n# Validation dataset (no augmentation)\nval_dataset = create_data_pipeline(\n    x_val, y_val, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    augment=False, \n    cache=True\n)\n\n# Test dataset (no augmentation)\ntest_dataset = create_data_pipeline(\n    x_test, y_test, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    augment=False, \n    cache=True\n)\n\nprint(\"✅ TensorFlow data pipelines created successfully!\")\n\n# Visualize augmented samples\ndef visualize_augmented_samples(dataset, class_names, num_samples=8):\n    \"\"\"Visualize samples from augmented dataset\"\"\"\n    \n    # Take one batch and visualize\n    for batch_x, batch_y in dataset.take(1):\n        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n        axes = axes.ravel()\n        \n        for i in range(num_samples):\n            if i < len(batch_x):\n                image = batch_x[i].numpy()\n                label = batch_y[i].numpy()[0]\n                \n                axes[i].imshow(image)\n                axes[i].set_title(f'{class_names[label]}')\n                axes[i].axis('off')\n        \n        plt.suptitle('Augmented Training Samples', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        break\n\nprint(\"\\nVisualizing augmented training samples:\")\nvisualize_augmented_samples(train_dataset, class_names)\n\n# Dataset performance analysis\ndef analyze_dataset_performance(dataset, name):\n    \"\"\"Analyze dataset loading performance\"\"\"\n    print(f\"\\n📊 Analyzing {name} dataset performance...\")\n    \n    start_time = time.time()\n    sample_count = 0\n    \n    # Time loading a few batches\n    for batch_x, batch_y in dataset.take(10):\n        sample_count += len(batch_x)\n    \n    end_time = time.time()\n    loading_time = end_time - start_time\n    \n    print(f\"   Loaded {sample_count} samples in {loading_time:.3f}s\")\n    print(f\"   Throughput: {sample_count/loading_time:.1f} samples/sec\")\n    print(f\"   Batch shape: {batch_x.shape}, Label shape: {batch_y.shape}\")\n\n# Analyze performance of all datasets\nanalyze_dataset_performance(train_dataset, \"Training\")\nanalyze_dataset_performance(val_dataset, \"Validation\")\nanalyze_dataset_performance(test_dataset, \"Test\")\n\nprint(\"\\n✅ Data pipeline optimization complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Advanced TensorFlow Model Architecture Design\n\n### Instructions:\n1. Design a deep neural network architecture using TensorFlow/Keras Model subclassing\n2. Implement custom layers and advanced techniques using TensorFlow\n3. Add modern techniques like residual connections and attention mechanisms\n4. Create modular and extensible TensorFlow architecture"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced TensorFlow model using Model subclassing and custom layers\n\nclass ResidualBlock(layers.Layer):\n    \"\"\"Custom Residual Block layer for TensorFlow\"\"\"\n    \n    def __init__(self, filters, kernel_size=3, stride=1, activation='relu', **kwargs):\n        super(ResidualBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.activation = activation\n        \n        # Main path layers\n        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride, padding='same',\n                                  kernel_initializer='he_normal')\n        self.bn1 = layers.BatchNormalization()\n        self.act1 = layers.Activation(activation)\n        \n        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1, padding='same',\n                                  kernel_initializer='he_normal')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Shortcut path (if needed)\n        self.shortcut_conv = None\n        self.shortcut_bn = None\n        \n        # Final activation\n        self.final_activation = layers.Activation(activation)\n    \n    def build(self, input_shape):\n        # Create shortcut path if dimensions don't match\n        if self.stride != 1 or input_shape[-1] != self.filters:\n            self.shortcut_conv = layers.Conv2D(self.filters, 1, strides=self.stride, \n                                             padding='same', kernel_initializer='he_normal')\n            self.shortcut_bn = layers.BatchNormalization()\n        \n        super(ResidualBlock, self).build(input_shape)\n    \n    def call(self, inputs, training=False):\n        # Main path\n        x = self.conv1(inputs)\n        x = self.bn1(x, training=training)\n        x = self.act1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        \n        # Shortcut path\n        shortcut = inputs\n        if self.shortcut_conv is not None:\n            shortcut = self.shortcut_conv(inputs)\n            shortcut = self.shortcut_bn(shortcut, training=training)\n        \n        # Add residual connection\n        x = layers.Add()([x, shortcut])\n        x = self.final_activation(x)\n        \n        return x\n    \n    def get_config(self):\n        config = super(ResidualBlock, self).get_config()\n        config.update({\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'stride': self.stride,\n            'activation': self.activation\n        })\n        return config\n\nclass AttentionBlock(layers.Layer):\n    \"\"\"Simple spatial attention mechanism\"\"\"\n    \n    def __init__(self, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n    \n    def build(self, input_shape):\n        # Create attention weights\n        self.attention_conv = layers.Conv2D(1, 1, activation='sigmoid', \n                                          kernel_initializer='he_normal')\n        super(AttentionBlock, self).build(input_shape)\n    \n    def call(self, inputs, training=False):\n        # Generate attention map\n        attention_map = self.attention_conv(inputs)\n        \n        # Apply attention weights\n        attended_features = layers.Multiply()([inputs, attention_map])\n        \n        return attended_features\n\nclass AdvancedCIFARModel(keras.Model):\n    \"\"\"Advanced CIFAR-10 classification model using TensorFlow/Keras\"\"\"\n    \n    def __init__(self, num_classes=10, dropout_rate=0.3, **kwargs):\n        super(AdvancedCIFARModel, self).__init__(**kwargs)\n        \n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n        \n        # Initial convolution\n        self.initial_conv = layers.Conv2D(64, 3, padding='same', \n                                        kernel_initializer='he_normal')\n        self.initial_bn = layers.BatchNormalization()\n        self.initial_activation = layers.Activation('relu')\n        \n        # Residual blocks\n        self.res_block1 = ResidualBlock(64, activation='relu')\n        self.res_block2 = ResidualBlock(128, stride=2, activation='relu')\n        self.res_block3 = ResidualBlock(128, activation='relu')\n        self.res_block4 = ResidualBlock(256, stride=2, activation='relu')\n        self.res_block5 = ResidualBlock(256, activation='relu')\n        \n        # Attention mechanism\n        self.attention = AttentionBlock()\n        \n        # Global average pooling\n        self.global_pool = layers.GlobalAveragePooling2D()\n        \n        # Dense layers with dropout\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dense1 = layers.Dense(512, activation='relu', \n                                  kernel_initializer='he_normal',\n                                  kernel_regularizer=regularizers.l2(0.001))\n        \n        self.dropout2 = layers.Dropout(dropout_rate)\n        self.dense2 = layers.Dense(256, activation='relu',\n                                  kernel_initializer='he_normal', \n                                  kernel_regularizer=regularizers.l2(0.001))\n        \n        # Output layer\n        self.classifier = layers.Dense(num_classes, activation='softmax',\n                                     kernel_initializer='glorot_uniform')\n        \n        # Build the model\n        self.build((None, 32, 32, 3))\n    \n    def call(self, inputs, training=False):\n        # Initial convolution\n        x = self.initial_conv(inputs)\n        x = self.initial_bn(x, training=training)\n        x = self.initial_activation(x)\n        \n        # Residual blocks\n        x = self.res_block1(x, training=training)\n        x = self.res_block2(x, training=training) \n        x = self.res_block3(x, training=training)\n        x = self.res_block4(x, training=training)\n        x = self.res_block5(x, training=training)\n        \n        # Apply attention\n        x = self.attention(x, training=training)\n        \n        # Global pooling\n        x = self.global_pool(x)\n        \n        # Dense layers\n        x = self.dropout1(x, training=training)\n        x = self.dense1(x)\n        \n        x = self.dropout2(x, training=training)\n        x = self.dense2(x)\n        \n        # Classification\n        outputs = self.classifier(x)\n        \n        return outputs\n    \n    def model_summary(self):\n        \"\"\"Print detailed model summary\"\"\"\n        print(\"🏗️ Advanced CIFAR-10 Model Architecture\")\n        print(\"=\" * 50)\n        \n        # Build model with sample input to get summary\n        sample_input = tf.random.normal((1, 32, 32, 3))\n        _ = self(sample_input)\n        \n        print(\"\\nModel Summary:\")\n        self.summary()\n        \n        # Count parameters\n        total_params = self.count_params()\n        trainable_params = sum([tf.reduce_prod(var.shape) for var in self.trainable_variables])\n        \n        print(f\"\\nParameter Count:\")\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        print(f\"Model size (MB): {total_params * 4 / 1024 / 1024:.2f}\")\n\n# Create the advanced model\nprint(\"🏗️ Building Advanced TensorFlow CIFAR-10 Model...\")\n\nmodel = AdvancedCIFARModel(num_classes=10, dropout_rate=0.3)\n\n# Display model information\nmodel.model_summary()\n\n# Visualize model architecture\nprint(\"\\n📊 Visualizing model architecture...\")\n\n# Create model plot\ntf.keras.utils.plot_model(\n    model, \n    to_file='cifar10_model_architecture.png',\n    show_shapes=True, \n    show_layer_names=True,\n    dpi=150\n)\n\nprint(\"✅ Model architecture saved as 'cifar10_model_architecture.png'\")\n\n# Test model with sample data\nprint(\"\\n🧪 Testing model with sample batch...\")\n\n# Get a sample batch\nfor sample_batch_x, sample_batch_y in train_dataset.take(1):\n    print(f\"Input shape: {sample_batch_x.shape}\")\n    print(f\"Label shape: {sample_batch_y.shape}\")\n    \n    # Forward pass\n    predictions = model(sample_batch_x, training=False)\n    print(f\"Output shape: {predictions.shape}\")\n    print(f\"Output sample: {predictions[0][:5].numpy()}\")  # Show first 5 predictions\n    \n    # Verify output is probability distribution\n    sample_probs = predictions[0].numpy()\n    print(f\"Sum of probabilities: {np.sum(sample_probs):.6f}\")\n    print(f\"Max probability: {np.max(sample_probs):.6f}\")\n\nprint(\"\\n✅ Advanced TensorFlow model created and tested successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Advanced TensorFlow Training System Implementation\n\n### Instructions:\n1. Implement a complete training system with TensorFlow's advanced features\n2. Add comprehensive monitoring using TensorBoard and Keras callbacks\n3. Implement custom training loops with tf.GradientTape\n4. Create advanced callback systems for training control"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced TensorFlow Training System with Custom Callbacks and Training Loops\n\nclass CustomMetricsCallback(callbacks.Callback):\n    \"\"\"Custom callback for advanced metrics logging\"\"\"\n    \n    def __init__(self, validation_data, class_names, **kwargs):\n        super(CustomMetricsCallback, self).__init__(**kwargs)\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.epoch_metrics = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate additional metrics\n        if logs is None:\n            logs = {}\n        \n        # Store epoch metrics\n        epoch_info = {\n            'epoch': epoch + 1,\n            'train_loss': logs.get('loss', 0),\n            'train_accuracy': logs.get('accuracy', 0),\n            'val_loss': logs.get('val_loss', 0),\n            'val_accuracy': logs.get('val_accuracy', 0),\n            'learning_rate': float(self.model.optimizer.learning_rate.numpy())\n        }\n        \n        self.epoch_metrics.append(epoch_info)\n        \n        # Print detailed progress every 10 epochs\n        if (epoch + 1) % 10 == 0:\n            print(f\"\\nEpoch {epoch + 1} Detailed Metrics:\")\n            print(f\"  Learning Rate: {epoch_info['learning_rate']:.6f}\")\n            print(f\"  Train Loss: {epoch_info['train_loss']:.4f}\")\n            print(f\"  Val Loss: {epoch_info['val_loss']:.4f}\")\n            print(f\"  Train Accuracy: {epoch_info['train_accuracy']:.1%}\")\n            print(f\"  Val Accuracy: {epoch_info['val_accuracy']:.1%}\")\n\nclass GradientMonitorCallback(callbacks.Callback):\n    \"\"\"Monitor gradient norms during training\"\"\"\n    \n    def __init__(self, monitor_frequency=5, **kwargs):\n        super(GradientMonitorCallback, self).__init__(**kwargs)\n        self.monitor_frequency = monitor_frequency\n        self.gradient_norms = []\n    \n    def on_batch_end(self, batch, logs=None):\n        if batch % self.monitor_frequency == 0:\n            # Calculate gradient norms\n            with tf.GradientTape() as tape:\n                # Get a batch of training data\n                sample_data = next(iter(train_dataset.take(1)))\n                x_sample, y_sample = sample_data\n                \n                predictions = self.model(x_sample, training=True)\n                loss = tf.keras.losses.sparse_categorical_crossentropy(y_sample, predictions)\n                loss = tf.reduce_mean(loss)\n            \n            # Compute gradients\n            gradients = tape.gradient(loss, self.model.trainable_variables)\n            \n            # Calculate gradient norm\n            grad_norm = tf.linalg.global_norm(gradients)\n            self.gradient_norms.append(float(grad_norm.numpy()))\n\ndef create_advanced_callbacks():\n    \"\"\"Create comprehensive callback system\"\"\"\n    \n    # Create logs directory\n    log_dir = f\"logs/cifar10_training_{int(time.time())}\"\n    os.makedirs(log_dir, exist_ok=True)\n    \n    callback_list = [\n        # Early Stopping\n        callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1,\n            mode='max'\n        ),\n        \n        # Learning Rate Reduction\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-7,\n            verbose=1,\n            mode='min'\n        ),\n        \n        # Model Checkpointing\n        callbacks.ModelCheckpoint(\n            filepath=f'{log_dir}/best_model.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=False,\n            verbose=1,\n            mode='max'\n        ),\n        \n        # TensorBoard Logging\n        callbacks.TensorBoard(\n            log_dir=log_dir,\n            histogram_freq=1,\n            write_graph=True,\n            write_images=True,\n            update_freq='epoch'\n        ),\n        \n        # Custom Metrics Callback\n        CustomMetricsCallback(val_dataset, class_names),\n        \n        # Gradient Monitoring\n        GradientMonitorCallback(monitor_frequency=10),\n        \n        # Learning Rate Scheduler\n        callbacks.LearningRateScheduler(\n            schedule=lambda epoch: 0.001 * 0.95 ** epoch,\n            verbose=0\n        )\n    ]\n    \n    return callback_list, log_dir\n\n# Create advanced training configuration\ndef create_training_system():\n    \"\"\"Set up comprehensive training system\"\"\"\n    \n    print(\"🚀 Setting up Advanced TensorFlow Training System...\")\n    \n    # Compile model with advanced configuration\n    model.compile(\n        optimizer=optimizers.Adam(\n            learning_rate=0.001,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=1e-7\n        ),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=[\n            'accuracy',\n            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n    \n    print(\"✅ Model compiled with advanced metrics\")\n    \n    # Create callbacks\n    callback_list, log_dir = create_advanced_callbacks()\n    \n    print(f\"✅ Callbacks created, logs will be saved to: {log_dir}\")\n    \n    return callback_list, log_dir\n\n# Set up training system\ncallback_list, log_dir = create_training_system()\n\n# Display training configuration\nprint(\"\\n📋 Training Configuration:\")\nprint(\"=\" * 50)\nprint(f\"Optimizer: Adam (lr=0.001)\")\nprint(f\"Loss Function: Sparse Categorical Crossentropy\")\nprint(f\"Metrics: Accuracy, Top-3 Accuracy, Precision, Recall\")\nprint(f\"Callbacks: {len(callback_list)} advanced callbacks\")\nprint(f\"Data Augmentation: Enabled on training set\")\nprint(f\"Regularization: L2, Dropout, Batch Normalization\")\n\nprint(f\"\\nDataset Configuration:\")\nprint(f\"Training batches: {len(train_dataset)}\")\nprint(f\"Validation batches: {len(val_dataset)}\")\nprint(f\"Test batches: {len(test_dataset)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\n\n# Custom Training Loop Implementation (Alternative approach)\n@tf.function\ndef train_step(x, y):\n    \"\"\"Custom training step with tf.GradientTape\"\"\"\n    with tf.GradientTape() as tape:\n        predictions = model(x, training=True)\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n        loss = tf.reduce_mean(loss)\n        \n        # Add regularization losses\n        regularization_loss = tf.add_n([l for l in model.losses])\n        total_loss = loss + regularization_loss\n    \n    # Compute gradients\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    \n    # Apply gradients\n    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    # Calculate accuracy\n    accuracy = tf.keras.metrics.sparse_categorical_accuracy(y, predictions)\n    accuracy = tf.reduce_mean(accuracy)\n    \n    return total_loss, accuracy\n\n@tf.function\ndef val_step(x, y):\n    \"\"\"Validation step\"\"\"\n    predictions = model(x, training=False)\n    loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n    loss = tf.reduce_mean(loss)\n    \n    accuracy = tf.keras.metrics.sparse_categorical_accuracy(y, predictions)\n    accuracy = tf.reduce_mean(accuracy)\n    \n    return loss, accuracy\n\ndef custom_training_loop(epochs=5):\n    \"\"\"Demonstrate custom training loop with tf.GradientTape\"\"\"\n    \n    print(f\"\\n🔧 Demonstrating Custom Training Loop ({epochs} epochs)...\")\n    \n    # Training metrics\n    train_loss_metric = tf.keras.metrics.Mean()\n    train_accuracy_metric = tf.keras.metrics.Mean()\n    val_loss_metric = tf.keras.metrics.Mean()\n    val_accuracy_metric = tf.keras.metrics.Mean()\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        \n        # Reset metrics\n        train_loss_metric.reset_states()\n        train_accuracy_metric.reset_states()\n        val_loss_metric.reset_states()\n        val_accuracy_metric.reset_states()\n        \n        # Training loop\n        for batch, (x, y) in enumerate(train_dataset):\n            loss, accuracy = train_step(x, y)\n            train_loss_metric.update_state(loss)\n            train_accuracy_metric.update_state(accuracy)\n            \n            if batch % 100 == 0:\n                print(f\"  Batch {batch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n        \n        # Validation loop\n        for x_val, y_val in val_dataset:\n            val_loss, val_accuracy = val_step(x_val, y_val)\n            val_loss_metric.update_state(val_loss)\n            val_accuracy_metric.update_state(val_accuracy)\n        \n        # Print epoch results\n        print(f\"  Train Loss: {train_loss_metric.result():.4f}\")\n        print(f\"  Train Accuracy: {train_accuracy_metric.result():.4f}\")\n        print(f\"  Val Loss: {val_loss_metric.result():.4f}\")\n        print(f\"  Val Accuracy: {val_accuracy_metric.result():.4f}\")\n\n# Demonstrate custom training loop\ncustom_training_loop(epochs=3)\n\nprint(\"\\n✅ Advanced TensorFlow training system ready!\")\nprint(f\"📊 TensorBoard logs: tensorboard --logdir {log_dir}\")\nprint(f\"🚀 Ready to train with model.fit() or custom training loops!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Complete TensorFlow Model Training and Analysis\n\n### Instructions:\n1. Train the complete TensorFlow deep neural network system\n2. Monitor training progress with TensorBoard and advanced metrics\n3. Analyze training dynamics and model performance\n4. Create comprehensive performance visualizations using TensorFlow tools"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete TensorFlow Model Training with Advanced Monitoring\n\nprint(\"🚀 Starting Complete TensorFlow Deep Learning Training...\")\nprint(\"This demonstrates the full power of TensorFlow for deep learning!\")\nprint(\"=\"*70)\n\n# Training parameters\nEPOCHS = 30  # Reduced for demonstration, increase for better results\n\n# Start training with all advanced features\nprint(f\"Beginning training for {EPOCHS} epochs...\")\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=callback_list,\n    verbose=1\n)\n\nprint(\"\\n✅ Training completed successfully!\")\n\n# Comprehensive Performance Analysis with TensorFlow\ndef analyze_tensorflow_training_results(history, model, test_dataset, class_names):\n    \"\"\"Comprehensive analysis of TensorFlow training results\"\"\"\n    \n    print(\"\\n🔍 COMPREHENSIVE TENSORFLOW MODEL ANALYSIS\")\n    print(\"=\" * 60)\n    \n    # Extract training history\n    train_acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(1, len(train_acc) + 1)\n    \n    # Create comprehensive visualization\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Plot 1: Training and Validation Accuracy\n    axes[0, 0].plot(epochs_range, train_acc, 'bo-', label='Training Accuracy', linewidth=2)\n    axes[0, 0].plot(epochs_range, val_acc, 'ro-', label='Validation Accuracy', linewidth=2)\n    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Plot 2: Training and Validation Loss\n    axes[0, 1].plot(epochs_range, train_loss, 'bo-', label='Training Loss', linewidth=2)\n    axes[0, 1].plot(epochs_range, val_loss, 'ro-', label='Validation Loss', linewidth=2)\n    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Plot 3: Learning Rate Schedule (if available)\n    if 'lr' in history.history:\n        axes[0, 2].plot(epochs_range, history.history['lr'], 'go-', linewidth=2)\n        axes[0, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n        axes[0, 2].set_xlabel('Epoch')\n        axes[0, 2].set_ylabel('Learning Rate')\n        axes[0, 2].set_yscale('log')\n        axes[0, 2].grid(True, alpha=0.3)\n    else:\n        axes[0, 2].text(0.5, 0.5, 'Learning Rate\\nSchedule\\nNot Recorded', \n                       ha='center', va='center', transform=axes[0, 2].transAxes,\n                       fontsize=12)\n        axes[0, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    test_results = model.evaluate(test_dataset, verbose=0)\n    test_loss, test_accuracy = test_results[0], test_results[1]\n    \n    print(f\"📊 Final Test Results:\")\n    print(f\"   Test Loss: {test_loss:.4f}\")\n    print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n    \n    # Get predictions for confusion matrix\n    print(\"Generating predictions for detailed analysis...\")\n    y_pred = []\n    y_true = []\n    \n    for batch_x, batch_y in test_dataset:\n        predictions = model.predict(batch_x, verbose=0)\n        y_pred.extend(np.argmax(predictions, axis=1))\n        y_true.extend(batch_y.numpy().flatten())\n    \n    y_pred = np.array(y_pred)\n    y_true = np.array(y_true)\n    \n    # Plot 4: Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    im = axes[1, 0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n    \n    # Add colorbar\n    plt.colorbar(im, ax=axes[1, 0])\n    \n    # Add labels\n    tick_marks = np.arange(len(class_names))\n    axes[1, 0].set_xticks(tick_marks)\n    axes[1, 0].set_yticks(tick_marks)\n    axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[1, 0].set_yticklabels(class_names)\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            axes[1, 0].text(j, i, format(cm[i, j], 'd'),\n                          ha=\"center\", va=\"center\",\n                          color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    axes[1, 0].set_xlabel('Predicted Label')\n    axes[1, 0].set_ylabel('True Label')\n    \n    # Plot 5: Per-Class Accuracy\n    class_accuracy = []\n    for i in range(len(class_names)):\n        class_mask = (y_true == i)\n        if np.sum(class_mask) > 0:\n            acc = np.mean(y_pred[class_mask] == y_true[class_mask])\n            class_accuracy.append(acc)\n        else:\n            class_accuracy.append(0)\n    \n    bars = axes[1, 1].bar(class_names, class_accuracy, color=plt.cm.tab10(np.arange(10)))\n    axes[1, 1].set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Class')\n    axes[1, 1].set_ylabel('Accuracy')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    \n    # Add value labels on bars\n    for bar, acc in zip(bars, class_accuracy):\n        height = bar.get_height()\n        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{acc:.3f}', ha='center', va='bottom')\n    \n    # Plot 6: Training Summary Stats\n    axes[1, 2].axis('off')\n    \n    # Create summary statistics\n    final_train_acc = train_acc[-1]\n    final_val_acc = val_acc[-1]\n    best_val_acc = max(val_acc)\n    best_val_epoch = val_acc.index(best_val_acc) + 1\n    \n    summary_text = f\"\"\"\n    🏆 TRAINING SUMMARY\n    \n    Final Training Accuracy: {final_train_acc:.1%}\n    Final Validation Accuracy: {final_val_acc:.1%}\n    Best Validation Accuracy: {best_val_acc:.1%}\n    Best Epoch: {best_val_epoch}\n    \n    Test Set Performance:\n    Test Accuracy: {test_accuracy:.1%}\n    Test Loss: {test_loss:.4f}\n    \n    Generalization Gap:\n    Train-Test: {abs(final_train_acc - test_accuracy):.1%}\n    \n    Model Complexity:\n    Parameters: {model.count_params():,}\n    Layers: {len(model.layers)}\n    \"\"\"\n    \n    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,\n                   fontsize=11, verticalalignment='top', fontfamily='monospace',\n                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.7))\n    \n    plt.suptitle('Complete TensorFlow Deep Learning Analysis', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Detailed Classification Report\n    print(\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n    print(\"-\" * 60)\n    report = classification_report(y_true, y_pred, target_names=class_names, \n                                 output_dict=True)\n    \n    # Print formatted report\n    print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support'}\")\n    print(\"-\" * 60)\n    \n    for class_name in class_names:\n        metrics = report[class_name]\n        print(f\"{class_name:<12} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n              f\"{metrics['f1-score']:<10.3f} {int(metrics['support'])}\")\n    \n    print(\"-\" * 60)\n    macro_avg = report['macro avg']\n    weighted_avg = report['weighted avg']\n    \n    print(f\"{'Macro Avg':<12} {macro_avg['precision']:<10.3f} {macro_avg['recall']:<10.3f} \"\n          f\"{macro_avg['f1-score']:<10.3f} {int(macro_avg['support'])}\")\n    print(f\"{'Weighted Avg':<12} {weighted_avg['precision']:<10.3f} {weighted_avg['recall']:<10.3f} \"\n          f\"{weighted_avg['f1-score']:<10.3f} {int(weighted_avg['support'])}\")\n    \n    return {\n        'test_accuracy': test_accuracy,\n        'test_loss': test_loss,\n        'predictions': y_pred,\n        'true_labels': y_true,\n        'classification_report': report,\n        'history': history.history\n    }\n\n# Perform comprehensive analysis\nanalysis_results = analyze_tensorflow_training_results(history, model, test_dataset, class_names)\n\n# Display TensorBoard information\nprint(f\"\\n📊 TensorBoard Analysis:\")\nprint(f\"   Launch TensorBoard with: tensorboard --logdir {log_dir}\")\nprint(f\"   Then open: http://localhost:6006\")\n\nprint(f\"\\n✅ Complete TensorFlow training and analysis finished!\")\nprint(f\"🎯 Final Results Summary:\")\nprint(f\"   • Test Accuracy: {analysis_results['test_accuracy']:.1%}\")\nprint(f\"   • Training completed in {len(history.history['loss'])} epochs\")\nprint(f\"   • Model successfully demonstrates all TensorFlow capabilities!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5: Production Deployment with TensorFlow\n\n### Instructions:\n1. Prepare the TensorFlow model for production deployment\n2. Implement TensorFlow Serving and SavedModel format\n3. Create inference APIs and model monitoring systems\n4. Generate comprehensive project documentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Production-Ready TensorFlow Model Deployment\n\nclass TensorFlowProductionModel:\n    \"\"\"Production-ready wrapper for TensorFlow models\"\"\"\n    \n    def __init__(self, model, class_names, model_metadata=None):\n        self.model = model\n        self.class_names = class_names\n        self.model_metadata = model_metadata or self._create_metadata()\n        \n        print(\"🚀 TensorFlow Production Model Initialized\")\n        self._print_model_info()\n    \n    def _create_metadata(self):\n        \"\"\"Create comprehensive model metadata\"\"\"\n        return {\n            'model_name': 'Advanced CIFAR-10 Classifier',\n            'model_version': '1.0.0',\n            'framework': 'TensorFlow',\n            'framework_version': tf.__version__,\n            'model_type': 'Image Classification CNN',\n            'dataset': 'CIFAR-10',\n            'input_shape': (32, 32, 3),\n            'output_classes': len(self.class_names),\n            'class_names': self.class_names,\n            'architecture_features': [\n                'Residual Connections', 'Batch Normalization', \n                'Dropout Regularization', 'Attention Mechanism',\n                'Global Average Pooling', 'L2 Regularization'\n            ],\n            'preprocessing': 'Pixel normalization to [0,1]',\n            'total_parameters': self.model.count_params(),\n            'created_date': time.strftime('%Y-%m-%d %H:%M:%S')\n        }\n    \n    def _print_model_info(self):\n        \"\"\"Print model information\"\"\"\n        print(\"\\n📋 Production Model Information:\")\n        print(\"=\" * 50)\n        print(f\"Model: {self.model_metadata['model_name']}\")\n        print(f\"Version: {self.model_metadata['model_version']}\")\n        print(f\"Framework: {self.model_metadata['framework']} {self.model_metadata['framework_version']}\")\n        print(f\"Parameters: {self.model_metadata['total_parameters']:,}\")\n        print(f\"Classes: {self.model_metadata['output_classes']}\")\n        print(\"=\" * 50)\n    \n    def save_for_production(self, save_path):\n        \"\"\"Save model in production formats\"\"\"\n        \n        print(f\"💾 Saving production-ready model...\")\n        \n        # Create directory structure\n        os.makedirs(save_path, exist_ok=True)\n        \n        # 1. Save complete model in SavedModel format (TensorFlow Serving)\n        savedmodel_path = os.path.join(save_path, 'savedmodel')\n        tf.saved_model.save(self.model, savedmodel_path)\n        print(f\"✅ SavedModel saved to: {savedmodel_path}\")\n        \n        # 2. Save model weights and architecture separately\n        weights_path = os.path.join(save_path, 'model_weights.h5')\n        self.model.save_weights(weights_path)\n        print(f\"✅ Model weights saved to: {weights_path}\")\n        \n        # 3. Save complete model in H5 format\n        h5_path = os.path.join(save_path, 'complete_model.h5')\n        self.model.save(h5_path)\n        print(f\"✅ Complete model saved to: {h5_path}\")\n        \n        # 4. Save model metadata\n        metadata_path = os.path.join(save_path, 'model_metadata.json')\n        import json\n        with open(metadata_path, 'w') as f:\n            json.dump(self.model_metadata, f, indent=2, default=str)\n        print(f\"✅ Model metadata saved to: {metadata_path}\")\n        \n        # 5. Create model signature for TensorFlow Serving\n        self._create_serving_signature(savedmodel_path)\n        \n        # Calculate total size\n        total_size = sum(os.path.getsize(os.path.join(save_path, f)) \n                        for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f)))\n        total_size_mb = total_size / (1024 * 1024)\n        \n        print(f\"📊 Total model size: {total_size_mb:.2f} MB\")\n        \n        return {\n            'savedmodel_path': savedmodel_path,\n            'weights_path': weights_path,\n            'h5_path': h5_path,\n            'metadata_path': metadata_path,\n            'total_size_mb': total_size_mb\n        }\n    \n    def _create_serving_signature(self, savedmodel_path):\n        \"\"\"Create serving signature for TensorFlow Serving\"\"\"\n        \n        # Load the saved model and add signature\n        loaded_model = tf.saved_model.load(savedmodel_path)\n        \n        @tf.function\n        def serving_default(input_image):\n            # Ensure input is in correct format\n            input_image = tf.cast(input_image, tf.float32)\n            input_image = tf.ensure_shape(input_image, [None, 32, 32, 3])\n            \n            # Make prediction\n            predictions = self.model(input_image, training=False)\n            \n            return {\n                'predictions': predictions,\n                'predicted_class': tf.argmax(predictions, axis=1),\n                'confidence': tf.reduce_max(predictions, axis=1)\n            }\n        \n        # Save with signature\n        tf.saved_model.save(\n            self.model,\n            savedmodel_path,\n            signatures={'serving_default': serving_default}\n        )\n        \n        print(f\"✅ TensorFlow Serving signature created\")\n    \n    @staticmethod\n    def load_production_model(save_path, model_format='savedmodel'):\n        \"\"\"Load production model from saved files\"\"\"\n        \n        if model_format == 'savedmodel':\n            model_path = os.path.join(save_path, 'savedmodel')\n            model = tf.saved_model.load(model_path)\n        elif model_format == 'h5':\n            model_path = os.path.join(save_path, 'complete_model.h5')\n            model = tf.keras.models.load_model(model_path)\n        else:\n            raise ValueError(f\"Unsupported format: {model_format}\")\n        \n        # Load metadata\n        metadata_path = os.path.join(save_path, 'model_metadata.json')\n        import json\n        with open(metadata_path, 'r') as f:\n            metadata = json.load(f)\n        \n        print(f\"✅ Model loaded from: {model_path}\")\n        return model, metadata\n    \n    def predict_single(self, image, return_top_k=5):\n        \"\"\"Make prediction on single image with detailed output\"\"\"\n        \n        # Ensure image is in correct format\n        if len(image.shape) == 3:\n            image = tf.expand_dims(image, 0)\n        \n        # Normalize if needed\n        if tf.reduce_max(image) > 1.0:\n            image = tf.cast(image, tf.float32) / 255.0\n        \n        # Make prediction\n        predictions = self.model(image, training=False)\n        probabilities = tf.nn.softmax(predictions).numpy()[0]\n        \n        # Get top-k predictions\n        top_indices = np.argsort(probabilities)[::-1][:return_top_k]\n        \n        result = {\n            'predicted_class': top_indices[0],\n            'predicted_label': self.class_names[top_indices[0]],\n            'confidence': float(probabilities[top_indices[0]]),\n            'top_predictions': [\n                {\n                    'class': int(idx),\n                    'label': self.class_names[idx],\n                    'probability': float(probabilities[idx])\n                }\n                for idx in top_indices\n            ]\n        }\n        \n        return result\n    \n    def batch_predict(self, images, batch_size=32):\n        \"\"\"Efficient batch prediction\"\"\"\n        \n        # Create dataset for efficient batch processing\n        dataset = tf.data.Dataset.from_tensor_slices(images)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n        predictions = []\n        for batch in dataset:\n            batch_preds = self.model(batch, training=False)\n            predictions.append(batch_preds.numpy())\n        \n        return np.concatenate(predictions, axis=0)\n    \n    def evaluate_performance(self, test_dataset):\n        \"\"\"Comprehensive performance evaluation\"\"\"\n        \n        print(\"📊 Evaluating production model performance...\")\n        \n        # Evaluate using TensorFlow's built-in evaluate\n        results = self.model.evaluate(test_dataset, verbose=0)\n        \n        # Get detailed metrics\n        test_loss = results[0]\n        test_accuracy = results[1] if len(results) > 1 else None\n        \n        # Performance metrics\n        start_time = time.time()\n        sample_count = 0\n        \n        # Time inference on test set\n        for batch_x, batch_y in test_dataset.take(10):\n            _ = self.model.predict(batch_x, verbose=0)\n            sample_count += len(batch_x)\n        \n        inference_time = time.time() - start_time\n        throughput = sample_count / inference_time\n        \n        performance_metrics = {\n            'test_loss': float(test_loss),\n            'test_accuracy': float(test_accuracy) if test_accuracy else None,\n            'inference_throughput': throughput,\n            'avg_inference_time_per_sample': inference_time / sample_count,\n            'model_size_mb': self.model_metadata['total_parameters'] * 4 / 1024 / 1024\n        }\n        \n        print(f\"   Test Accuracy: {performance_metrics['test_accuracy']:.1%}\")\n        print(f\"   Inference Throughput: {performance_metrics['inference_throughput']:.1f} samples/sec\")\n        print(f\"   Avg Inference Time: {performance_metrics['avg_inference_time_per_sample']*1000:.1f} ms/sample\")\n        \n        return performance_metrics\n\n# Create production model\nprint(\"🏭 Creating Production-Ready TensorFlow Model...\")\n\nproduction_model = TensorFlowProductionModel(\n    model=model,\n    class_names=class_names,\n    model_metadata={\n        'test_accuracy': analysis_results['test_accuracy'],\n        'training_epochs': len(history.history['loss']),\n        'best_val_accuracy': max(history.history['val_accuracy'])\n    }\n)\n\n# Save model for production\nsave_paths = production_model.save_for_production('production_model')\n\n# Test production model functionality\nprint(\"\\n🧪 Testing Production Model Functionality...\")\n\n# Test single prediction\nsample_batch = next(iter(test_dataset.take(1)))\nsample_image = sample_batch[0][0]\nsample_label = sample_batch[1][0].numpy()\n\nprint(f\"\\nTesting single prediction...\")\nprint(f\"True label: {class_names[sample_label]}\")\n\nprediction_result = production_model.predict_single(sample_image, return_top_k=3)\n\nprint(f\"Predicted: {prediction_result['predicted_label']} (confidence: {prediction_result['confidence']:.3f})\")\nprint(f\"Top 3 predictions:\")\nfor pred in prediction_result['top_predictions']:\n    print(f\"  {pred['label']}: {pred['probability']:.3f}\")\n\n# Test batch prediction efficiency\nprint(f\"\\n🚀 Testing batch prediction performance...\")\nsample_images = sample_batch[0][:8]  # Test with 8 images\n\nstart_time = time.time()\nbatch_predictions = production_model.batch_predict(sample_images)\nbatch_time = time.time() - start_time\n\nprint(f\"Batch prediction time: {batch_time:.3f}s for {len(sample_images)} images\")\nprint(f\"Throughput: {len(sample_images)/batch_time:.1f} images/sec\")\n\n# Evaluate production model performance\nperformance_metrics = production_model.evaluate_performance(test_dataset)\n\n# Test model loading (to verify saves worked correctly)\nprint(f\"\\n🔄 Testing Model Loading...\")\ntry:\n    loaded_model, loaded_metadata = TensorFlowProductionModel.load_production_model(\n        'production_model', model_format='h5'\n    )\n    print(f\"✅ Model loaded successfully!\")\n    print(f\"   Loaded model version: {loaded_metadata.get('model_version', 'Unknown')}\")\nexcept Exception as e:\n    print(f\"❌ Error loading model: {e}\")\n\nprint(f\"\\n✅ Production TensorFlow model created and tested successfully!\")\nprint(f\"🚀 Model ready for deployment with TensorFlow Serving!\")\n\n# Production deployment instructions\ndeployment_instructions = f\"\"\"\n🚀 TENSORFLOW PRODUCTION DEPLOYMENT INSTRUCTIONS\n===============================================\n\n1. TensorFlow Serving Deployment:\n   docker run -p 8501:8501 \\\\\n     --mount type=bind,source={os.path.abspath(save_paths['savedmodel_path'])},target=/models/cifar10/1 \\\\\n     -e MODEL_NAME=cifar10 -t tensorflow/serving\n\n2. REST API Endpoint:\n   POST http://localhost:8501/v1/models/cifar10:predict\n   \n3. Model Monitoring:\n   - Monitor inference latency and throughput\n   - Track prediction confidence distributions  \n   - Log model performance metrics\n   - Set up alerts for model drift\n\n4. Model Versioning:\n   - Use model version directories (1/, 2/, etc.)\n   - Implement A/B testing between versions\n   - Maintain rollback capabilities\n\n5. Scaling Considerations:\n   - Use TensorFlow Serving with load balancer\n   - Consider TensorFlow Lite for mobile deployment\n   - Implement batch prediction endpoints for efficiency\n\"\"\"\n\nprint(deployment_instructions)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🎉 Lab Complete! Comprehensive TensorFlow Deep Learning Mastery Achieved!\n\n### What You've Accomplished - A Complete TensorFlow Production System:\n\n#### 🏗️ **Advanced TensorFlow Architecture**:\n✅ **Custom Model Subclassing**: Built advanced models with tf.keras.Model  \n✅ **Custom Layers**: Implemented ResidualBlock and AttentionBlock layers  \n✅ **Modern Techniques**: Batch normalization, dropout, residual connections  \n✅ **TensorFlow Best Practices**: Proper initialization, regularization, and optimization  \n\n#### 🚀 **TensorFlow Training Excellence**:\n✅ **Advanced Callbacks**: Custom metrics, gradient monitoring, learning rate scheduling  \n✅ **TensorBoard Integration**: Comprehensive training visualization and monitoring  \n✅ **Custom Training Loops**: tf.GradientTape for maximum control and flexibility  \n✅ **Data Pipeline Optimization**: tf.data for efficient training and augmentation  \n\n#### 📊 **Outstanding Performance with TensorFlow**:\n✅ **High Accuracy**: Achieved excellent classification performance on CIFAR-10  \n✅ **Stable Training**: Smooth convergence with proper regularization  \n✅ **Production Quality**: Professional-grade model performance and reliability  \n✅ **Comprehensive Analysis**: Detailed performance metrics and visualizations  \n\n#### 💼 **Production Deployment Mastery**:\n✅ **TensorFlow Serving**: SavedModel format with serving signatures  \n✅ **Multiple Export Formats**: H5, SavedModel, weights-only for different use cases  \n✅ **Inference Optimization**: Batch processing and performance monitoring  \n✅ **Production APIs**: Ready for deployment with REST/gRPC endpoints  \n\n### 🎯 Key TensorFlow Technical Achievements:\n\n#### **TensorFlow Ecosystem Mastery**:\n- 🔥 **tf.data**: Efficient data pipelines with augmentation and caching\n- 🔥 **tf.keras**: Advanced model building with subclassing and custom layers  \n- 🔥 **tf.GradientTape**: Custom training loops with full gradient control\n- 🔥 **tf.saved_model**: Production deployment with TensorFlow Serving\n- 🔥 **tf.function**: Performance optimization with graph compilation\n\n#### **Advanced Deep Learning Techniques**:\n- ⚡ **Residual Connections**: Skip connections for better gradient flow\n- ⚡ **Attention Mechanisms**: Spatial attention for feature enhancement  \n- ⚡ **Batch Normalization**: Training stability and faster convergence\n- ⚡ **Advanced Regularization**: L2, dropout, and architectural constraints\n\n#### **Professional Training System**:\n- 🏭 **Custom Callbacks**: Advanced monitoring and training control\n- 🏭 **TensorBoard Logging**: Comprehensive training visualization\n- 🏭 **Performance Optimization**: GPU utilization and memory efficiency\n- 🏭 **Production Monitoring**: Inference performance and model quality tracking\n\n### 🌟 Real-World TensorFlow Applications:\n\n#### **Computer Vision Systems**:\n- 🎯 **Image Classification**: Production-ready CIFAR-10 classifier\n- 🎯 **Medical Imaging**: Diagnostic systems with attention mechanisms\n- 🎯 **Autonomous Vehicles**: Real-time object detection and classification\n- 🎯 **Quality Control**: Automated defect detection in manufacturing\n\n#### **Industry-Grade Capabilities**:\n- 💡 **Scalable Architecture**: Ready for large-scale deployment\n- 💡 **Model Versioning**: A/B testing and rollback capabilities\n- 💡 **Performance Monitoring**: Production-ready metrics and alerting\n- 💡 **API Integration**: REST/gRPC endpoints for web and mobile applications\n\n### 🚀 Advanced TensorFlow Skills Demonstrated:\n\n```python\nTENSORFLOW_MASTERY = {\n    'model_building': {\n        'subclassing': 'EXPERT',\n        'custom_layers': 'ADVANCED', \n        'architectures': 'PROFESSIONAL'\n    },\n    'training_systems': {\n        'callbacks': 'EXPERT',\n        'custom_loops': 'ADVANCED',\n        'monitoring': 'PROFESSIONAL'\n    },\n    'production_deployment': {\n        'serving': 'EXPERT',\n        'optimization': 'ADVANCED',\n        'monitoring': 'PROFESSIONAL'\n    },\n    'tensorflow_ecosystem': {\n        'tf_data': 'EXPERT',\n        'tf_function': 'ADVANCED', \n        'tensorboard': 'PROFESSIONAL'\n    }\n}\n```\n\n### 🎓 What Makes This Achievement Exceptional:\n\n#### **Complete TensorFlow Ecosystem Coverage**:\n- 📚 **Data Pipelines**: tf.data for production-grade data processing\n- 📚 **Model Development**: Advanced architectures with custom components\n- 📚 **Training Systems**: Professional-grade training with monitoring\n- 📚 **Production Deployment**: TensorFlow Serving with full MLOps pipeline\n\n#### **Industry-Ready Skills**:\n- 🔬 **Research Capabilities**: Custom layers and experimental architectures\n- 🏭 **Production Engineering**: Scalable deployment and monitoring systems  \n- 📊 **Performance Optimization**: Inference speed and resource efficiency\n- 💼 **Business Impact**: End-to-end solution from research to production\n\n### 🌟 Your TensorFlow Journey Achievements:\n\n#### **Technical Mastery Progression**:\n1. **Foundations** → TensorFlow basics and Keras APIs ✅\n2. **Architecture Design** → Custom models and advanced layers ✅  \n3. **Training Systems** → Advanced callbacks and monitoring ✅\n4. **Production Deployment** → TensorFlow Serving and optimization ✅\n5. **Complete System** → End-to-end MLOps with TensorFlow ✅\n\n#### **Professional Capabilities Unlocked**:\n- 🎯 **Lead TensorFlow projects** in production environments\n- 🎯 **Design custom architectures** for specific business problems\n- 🎯 **Implement MLOps pipelines** with TensorFlow ecosystem\n- 🎯 **Optimize model performance** for real-world constraints\n\n### 🏆 Final Project Excellence Summary:\n\n**You've successfully built a complete, production-ready deep learning system using TensorFlow that demonstrates:**\n\n- ✨ **Advanced Architecture Design** with custom layers and modern techniques\n- ✨ **Professional Training Systems** with comprehensive monitoring\n- ✨ **Production Deployment Excellence** with TensorFlow Serving\n- ✨ **Industry-Grade Performance** with optimization and scalability\n- ✨ **Complete MLOps Integration** from research to production\n\n### 🚀 Ready for TensorFlow Leadership!\n\n**You now possess the complete skill set to:**\n- 🌟 **Lead TensorFlow initiatives** in enterprise environments\n- 🌟 **Architect scalable ML systems** using TensorFlow ecosystem\n- 🌟 **Mentor teams** on TensorFlow best practices and advanced techniques\n- 🌟 **Drive innovation** with cutting-edge TensorFlow applications\n\n**Congratulations on achieving TensorFlow Deep Learning Mastery!** You're now equipped to build world-class AI systems that can handle real-world challenges at enterprise scale! 🎉🚀\n\n---\n\n### 🎯 **Your Next Challenge**: \nApply this comprehensive TensorFlow expertise to solve real-world problems in computer vision, NLP, or other domains. The foundation you've built here will serve as the launching pad for your next breakthrough in AI! ✨"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}