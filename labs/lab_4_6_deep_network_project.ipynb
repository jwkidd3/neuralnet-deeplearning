{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4.6: Complete Deep Network Project with TensorFlow - Production Image Classification System\n\n**Duration**: 45 minutes\n\n## Learning Objectives\nBy the end of this lab, you will be able to:\n- Design and implement a complete production-ready deep neural network using TensorFlow/Keras\n- Apply all advanced TensorFlow techniques learned in previous labs\n- Build a comprehensive image classification system with modern TensorFlow practices\n- Implement advanced TensorFlow features: custom layers, callbacks, and training loops\n- Deploy and evaluate a TensorFlow model in production-ready format\n- Create comprehensive model documentation and performance analysis\n\n## Prerequisites\n- Completed Labs 4.1, 4.2, 4.3, 4.4, and 4.5\n- Understanding of TensorFlow/Keras architecture design\n- Familiarity with TensorFlow optimization and regularization techniques\n\n## Project Overview\nThis capstone lab combines everything learned in the TensorFlow deep learning module. You'll build a complete image classification system using TensorFlow/Keras with the CIFAR-10 dataset, implementing state-of-the-art architectures with modern TensorFlow practices including custom layers, advanced callbacks, model subclassing, and production deployment techniques."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Project Setup and Data Preparation with TensorFlow\n\n### Instructions:\n1. Set up the complete TensorFlow development environment\n2. Load and preprocess the CIFAR-10 dataset using tf.data\n3. Implement TensorFlow data augmentation techniques\n4. Create TensorFlow visualization utilities for analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers, callbacks, regularizers\nfrom tensorflow.keras.datasets import cifar10\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Configure matplotlib\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11\n\nprint(\"üöÄ TensorFlow Deep Network Project Environment Ready!\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\nif tf.config.list_physical_devices('GPU'):\n    print(\"‚úÖ GPU detected - enabling GPU acceleration\")\n    # Configure GPU memory growth\n    for gpu in tf.config.list_physical_devices('GPU'):\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"üíª Using CPU - consider enabling GPU for better performance\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TENSORFLOW DEEP LEARNING PROJECT OVERVIEW\")\nprint(\"=\"*60)\nprint(\"\"\"\nThis capstone project demonstrates:\n‚úÖ Complete TensorFlow/Keras ecosystem mastery\n‚úÖ Advanced architecture design with custom layers\n‚úÖ Modern training techniques and callbacks\n‚úÖ Production deployment with TensorFlow Serving\n‚úÖ Comprehensive model evaluation and monitoring\n‚úÖ Integration of all previous lab concepts\n\nKey TensorFlow Features We'll Use:\nüî• tf.data for efficient data pipelines\nüî• tf.keras.Model subclassing for custom architectures  \nüî• tf.keras.callbacks for advanced training control\nüî• tf.keras.utils for model visualization and analysis\nüî• tf.saved_model for production deployment\nüî• tf.function for performance optimization\nüî• Mixed precision training for efficiency\nüî• TensorBoard for comprehensive monitoring\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load and prepare CIFAR-10 dataset using TensorFlow\nprint(\"Loading CIFAR-10 dataset using TensorFlow...\")\n\n# Load CIFAR-10 data\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# CIFAR-10 class names\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Training samples: {x_train.shape[0]:,}\")\nprint(f\"Test samples: {x_test.shape[0]:,}\")\nprint(f\"Image shape: {x_train.shape[1:]}\")\nprint(f\"Number of classes: {len(class_names)}\")\n\n# Display dataset information\nfig, axes = plt.subplots(2, 5, figsize=(15, 8))\naxes = axes.ravel()\n\nprint(f\"\\nVisualizing sample images from each class...\")\n\n# Show one sample from each class\nfor i in range(10):\n    # Find first occurrence of each class\n    class_idx = np.where(y_train.flatten() == i)[0][0]\n    \n    axes[i].imshow(x_train[class_idx])\n    axes[i].set_title(f'{class_names[i]}')\n    axes[i].axis('off')\n\nplt.suptitle('CIFAR-10 Dataset Sample Images', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Analyze class distribution\nunique, counts = np.unique(y_train, return_counts=True)\n\nplt.figure(figsize=(12, 5))\n\n# Bar plot\nplt.subplot(1, 2, 1)\nbars = plt.bar(class_names, counts, color=plt.cm.tab10(np.arange(10)))\nplt.title('CIFAR-10 Training Set Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Number of Samples')\nplt.xticks(rotation=45)\n\nfor bar, count in zip(bars, counts):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n            str(count), ha='center', va='bottom')\n\n# Pie chart\nplt.subplot(1, 2, 2)\nplt.pie(counts, labels=class_names, autopct='%1.1f%%')\nplt.title('Class Distribution (Percentage)')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nDataset Analysis:\")\nprint(f\"‚úÖ Balanced dataset: {len(set(counts)) == 1}\")\nprint(f\"‚úÖ Total training samples: {len(y_train):,}\")\nprint(f\"‚úÖ Samples per class: {counts[0]:,}\")\n\nprint(\"\\n‚úÖ CIFAR-10 dataset loaded and analyzed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create efficient TensorFlow data pipeline with tf.data\ndef create_data_pipeline(x_data, y_data, batch_size=32, shuffle=True, augment=False, cache=True):\n    \"\"\"\n    Create efficient TensorFlow data pipeline using tf.data\n    \n    Args:\n        x_data: Input images\n        y_data: Labels\n        batch_size: Batch size for training\n        shuffle: Whether to shuffle the data\n        augment: Whether to apply data augmentation\n        cache: Whether to cache the dataset in memory\n    \n    Returns:\n        tf.data.Dataset: Configured dataset\n    \"\"\"\n    # Convert to tf.data.Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    \n    # Cache dataset in memory for performance (if it fits)\n    if cache:\n        dataset = dataset.cache()\n    \n    # Shuffle if requested\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=1000)\n    \n    # Normalize pixel values to [0, 1]\n    def normalize_images(image, label):\n        return tf.cast(image, tf.float32) / 255.0, label\n    \n    dataset = dataset.map(normalize_images, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Apply data augmentation for training\n    if augment:\n        def augment_image(image, label):\n            # Random horizontal flip\n            image = tf.image.random_flip_left_right(image)\n            \n            # Random rotation\n            image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n            \n            # Random brightness and contrast\n            image = tf.image.random_brightness(image, max_delta=0.1)\n            image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n            \n            # Random saturation\n            image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n            \n            # Ensure values stay in [0, 1]\n            image = tf.clip_by_value(image, 0.0, 1.0)\n            \n            return image, label\n        \n        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Batch the data\n    dataset = dataset.batch(batch_size)\n    \n    # Prefetch for performance\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n# Split data into train/validation sets\nfrom sklearn.model_selection import train_test_split\n\nx_train_split, x_val, y_train_split, y_val = train_test_split(\n    x_train, y_train, test_size=0.1, stratify=y_train, random_state=42\n)\n\nprint(\"Creating TensorFlow data pipelines...\")\nprint(f\"Training set: {len(x_train_split):,} samples\")\nprint(f\"Validation set: {len(x_val):,} samples\") \nprint(f\"Test set: {len(x_test):,} samples\")\n\n# Create datasets with different configurations\nBATCH_SIZE = 32\n\n# Training dataset with augmentation\ntrain_dataset = create_data_pipeline(\n    x_train_split, y_train_split, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    augment=True, \n    cache=True\n)\n\n# Validation dataset (no augmentation)\nval_dataset = create_data_pipeline(\n    x_val, y_val, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    augment=False, \n    cache=True\n)\n\n# Test dataset (no augmentation)\ntest_dataset = create_data_pipeline(\n    x_test, y_test, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    augment=False, \n    cache=True\n)\n\nprint(\"‚úÖ TensorFlow data pipelines created successfully!\")\n\n# Visualize augmented samples\ndef visualize_augmented_samples(dataset, class_names, num_samples=8):\n    \"\"\"Visualize samples from augmented dataset\"\"\"\n    \n    # Take one batch and visualize\n    for batch_x, batch_y in dataset.take(1):\n        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n        axes = axes.ravel()\n        \n        for i in range(num_samples):\n            if i < len(batch_x):\n                image = batch_x[i].numpy()\n                label = batch_y[i].numpy()[0]\n                \n                axes[i].imshow(image)\n                axes[i].set_title(f'{class_names[label]}')\n                axes[i].axis('off')\n        \n        plt.suptitle('Augmented Training Samples', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        break\n\nprint(\"\\nVisualizing augmented training samples:\")\nvisualize_augmented_samples(train_dataset, class_names)\n\n# Dataset performance analysis\ndef analyze_dataset_performance(dataset, name):\n    \"\"\"Analyze dataset loading performance\"\"\"\n    print(f\"\\nüìä Analyzing {name} dataset performance...\")\n    \n    start_time = time.time()\n    sample_count = 0\n    \n    # Time loading a few batches\n    for batch_x, batch_y in dataset.take(10):\n        sample_count += len(batch_x)\n    \n    end_time = time.time()\n    loading_time = end_time - start_time\n    \n    print(f\"   Loaded {sample_count} samples in {loading_time:.3f}s\")\n    print(f\"   Throughput: {sample_count/loading_time:.1f} samples/sec\")\n    print(f\"   Batch shape: {batch_x.shape}, Label shape: {batch_y.shape}\")\n\n# Analyze performance of all datasets\nanalyze_dataset_performance(train_dataset, \"Training\")\nanalyze_dataset_performance(val_dataset, \"Validation\")\nanalyze_dataset_performance(test_dataset, \"Test\")\n\nprint(\"\\n‚úÖ Data pipeline optimization complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Advanced TensorFlow Model Architecture Design\n\n### Instructions:\n1. Design a deep neural network architecture using TensorFlow/Keras Model subclassing\n2. Implement custom layers and advanced techniques using TensorFlow\n3. Add modern techniques like residual connections and attention mechanisms\n4. Create modular and extensible TensorFlow architecture"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced TensorFlow model using Model subclassing and custom layers\n\nclass ResidualBlock(layers.Layer):\n    \"\"\"Custom Residual Block layer for TensorFlow\"\"\"\n    \n    def __init__(self, filters, kernel_size=3, stride=1, activation='relu', **kwargs):\n        super(ResidualBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.activation = activation\n        \n        # Main path layers\n        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride, padding='same',\n                                  kernel_initializer='he_normal')\n        self.bn1 = layers.BatchNormalization()\n        self.act1 = layers.Activation(activation)\n        \n        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1, padding='same',\n                                  kernel_initializer='he_normal')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Shortcut path (if needed)\n        self.shortcut_conv = None\n        self.shortcut_bn = None\n        \n        # Final activation\n        self.final_activation = layers.Activation(activation)\n    \n    def build(self, input_shape):\n        # Create shortcut path if dimensions don't match\n        if self.stride != 1 or input_shape[-1] != self.filters:\n            self.shortcut_conv = layers.Conv2D(self.filters, 1, strides=self.stride, \n                                             padding='same', kernel_initializer='he_normal')\n            self.shortcut_bn = layers.BatchNormalization()\n        \n        super(ResidualBlock, self).build(input_shape)\n    \n    def call(self, inputs, training=False):\n        # Main path\n        x = self.conv1(inputs)\n        x = self.bn1(x, training=training)\n        x = self.act1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        \n        # Shortcut path\n        shortcut = inputs\n        if self.shortcut_conv is not None:\n            shortcut = self.shortcut_conv(inputs)\n            shortcut = self.shortcut_bn(shortcut, training=training)\n        \n        # Add residual connection\n        x = layers.Add()([x, shortcut])\n        x = self.final_activation(x)\n        \n        return x\n    \n    def get_config(self):\n        config = super(ResidualBlock, self).get_config()\n        config.update({\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'stride': self.stride,\n            'activation': self.activation\n        })\n        return config\n\nclass AttentionBlock(layers.Layer):\n    \"\"\"Simple spatial attention mechanism\"\"\"\n    \n    def __init__(self, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n    \n    def build(self, input_shape):\n        # Create attention weights\n        self.attention_conv = layers.Conv2D(1, 1, activation='sigmoid', \n                                          kernel_initializer='he_normal')\n        super(AttentionBlock, self).build(input_shape)\n    \n    def call(self, inputs, training=False):\n        # Generate attention map\n        attention_map = self.attention_conv(inputs)\n        \n        # Apply attention weights\n        attended_features = layers.Multiply()([inputs, attention_map])\n        \n        return attended_features\n\nclass AdvancedCIFARModel(keras.Model):\n    \"\"\"Advanced CIFAR-10 classification model using TensorFlow/Keras\"\"\"\n    \n    def __init__(self, num_classes=10, dropout_rate=0.3, **kwargs):\n        super(AdvancedCIFARModel, self).__init__(**kwargs)\n        \n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n        \n        # Initial convolution\n        self.initial_conv = layers.Conv2D(64, 3, padding='same', \n                                        kernel_initializer='he_normal')\n        self.initial_bn = layers.BatchNormalization()\n        self.initial_activation = layers.Activation('relu')\n        \n        # Residual blocks\n        self.res_block1 = ResidualBlock(64, activation='relu')\n        self.res_block2 = ResidualBlock(128, stride=2, activation='relu')\n        self.res_block3 = ResidualBlock(128, activation='relu')\n        self.res_block4 = ResidualBlock(256, stride=2, activation='relu')\n        self.res_block5 = ResidualBlock(256, activation='relu')\n        \n        # Attention mechanism\n        self.attention = AttentionBlock()\n        \n        # Global average pooling\n        self.global_pool = layers.GlobalAveragePooling2D()\n        \n        # Dense layers with dropout\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dense1 = layers.Dense(512, activation='relu', \n                                  kernel_initializer='he_normal',\n                                  kernel_regularizer=regularizers.l2(0.001))\n        \n        self.dropout2 = layers.Dropout(dropout_rate)\n        self.dense2 = layers.Dense(256, activation='relu',\n                                  kernel_initializer='he_normal', \n                                  kernel_regularizer=regularizers.l2(0.001))\n        \n        # Output layer\n        self.classifier = layers.Dense(num_classes, activation='softmax',\n                                     kernel_initializer='glorot_uniform')\n        \n        # Build the model\n        self.build((None, 32, 32, 3))\n    \n    def call(self, inputs, training=False):\n        # Initial convolution\n        x = self.initial_conv(inputs)\n        x = self.initial_bn(x, training=training)\n        x = self.initial_activation(x)\n        \n        # Residual blocks\n        x = self.res_block1(x, training=training)\n        x = self.res_block2(x, training=training) \n        x = self.res_block3(x, training=training)\n        x = self.res_block4(x, training=training)\n        x = self.res_block5(x, training=training)\n        \n        # Apply attention\n        x = self.attention(x, training=training)\n        \n        # Global pooling\n        x = self.global_pool(x)\n        \n        # Dense layers\n        x = self.dropout1(x, training=training)\n        x = self.dense1(x)\n        \n        x = self.dropout2(x, training=training)\n        x = self.dense2(x)\n        \n        # Classification\n        outputs = self.classifier(x)\n        \n        return outputs\n    \n    def model_summary(self):\n        \"\"\"Print detailed model summary\"\"\"\n        print(\"üèóÔ∏è Advanced CIFAR-10 Model Architecture\")\n        print(\"=\" * 50)\n        \n        # Build model with sample input to get summary\n        sample_input = tf.random.normal((1, 32, 32, 3))\n        _ = self(sample_input)\n        \n        print(\"\\nModel Summary:\")\n        self.summary()\n        \n        # Count parameters\n        total_params = self.count_params()\n        trainable_params = sum([tf.reduce_prod(var.shape) for var in self.trainable_variables])\n        \n        print(f\"\\nParameter Count:\")\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        print(f\"Model size (MB): {total_params * 4 / 1024 / 1024:.2f}\")\n\n# Create the advanced model\nprint(\"üèóÔ∏è Building Advanced TensorFlow CIFAR-10 Model...\")\n\nmodel = AdvancedCIFARModel(num_classes=10, dropout_rate=0.3)\n\n# Display model information\nmodel.model_summary()\n\n# Visualize model architecture\nprint(\"\\nüìä Visualizing model architecture...\")\n\n# Create model plot\ntf.keras.utils.plot_model(\n    model, \n    to_file='cifar10_model_architecture.png',\n    show_shapes=True, \n    show_layer_names=True,\n    dpi=150\n)\n\nprint(\"‚úÖ Model architecture saved as 'cifar10_model_architecture.png'\")\n\n# Test model with sample data\nprint(\"\\nüß™ Testing model with sample batch...\")\n\n# Get a sample batch\nfor sample_batch_x, sample_batch_y in train_dataset.take(1):\n    print(f\"Input shape: {sample_batch_x.shape}\")\n    print(f\"Label shape: {sample_batch_y.shape}\")\n    \n    # Forward pass\n    predictions = model(sample_batch_x, training=False)\n    print(f\"Output shape: {predictions.shape}\")\n    print(f\"Output sample: {predictions[0][:5].numpy()}\")  # Show first 5 predictions\n    \n    # Verify output is probability distribution\n    sample_probs = predictions[0].numpy()\n    print(f\"Sum of probabilities: {np.sum(sample_probs):.6f}\")\n    print(f\"Max probability: {np.max(sample_probs):.6f}\")\n\nprint(\"\\n‚úÖ Advanced TensorFlow model created and tested successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Advanced TensorFlow Training System Implementation\n\n### Instructions:\n1. Implement a complete training system with TensorFlow's advanced features\n2. Add comprehensive monitoring using TensorBoard and Keras callbacks\n3. Implement custom training loops with tf.GradientTape\n4. Create advanced callback systems for training control"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced TensorFlow Training System with Custom Callbacks and Training Loops\n\nclass CustomMetricsCallback(callbacks.Callback):\n    \"\"\"Custom callback for advanced metrics logging\"\"\"\n    \n    def __init__(self, validation_data, class_names, **kwargs):\n        super(CustomMetricsCallback, self).__init__(**kwargs)\n        self.validation_data = validation_data\n        self.class_names = class_names\n        self.epoch_metrics = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # Calculate additional metrics\n        if logs is None:\n            logs = {}\n        \n        # Store epoch metrics\n        epoch_info = {\n            'epoch': epoch + 1,\n            'train_loss': logs.get('loss', 0),\n            'train_accuracy': logs.get('accuracy', 0),\n            'val_loss': logs.get('val_loss', 0),\n            'val_accuracy': logs.get('val_accuracy', 0),\n            'learning_rate': float(self.model.optimizer.learning_rate.numpy())\n        }\n        \n        self.epoch_metrics.append(epoch_info)\n        \n        # Print detailed progress every 10 epochs\n        if (epoch + 1) % 10 == 0:\n            print(f\"\\nEpoch {epoch + 1} Detailed Metrics:\")\n            print(f\"  Learning Rate: {epoch_info['learning_rate']:.6f}\")\n            print(f\"  Train Loss: {epoch_info['train_loss']:.4f}\")\n            print(f\"  Val Loss: {epoch_info['val_loss']:.4f}\")\n            print(f\"  Train Accuracy: {epoch_info['train_accuracy']:.1%}\")\n            print(f\"  Val Accuracy: {epoch_info['val_accuracy']:.1%}\")\n\nclass GradientMonitorCallback(callbacks.Callback):\n    \"\"\"Monitor gradient norms during training\"\"\"\n    \n    def __init__(self, monitor_frequency=5, **kwargs):\n        super(GradientMonitorCallback, self).__init__(**kwargs)\n        self.monitor_frequency = monitor_frequency\n        self.gradient_norms = []\n    \n    def on_batch_end(self, batch, logs=None):\n        if batch % self.monitor_frequency == 0:\n            # Calculate gradient norms\n            with tf.GradientTape() as tape:\n                # Get a batch of training data\n                sample_data = next(iter(train_dataset.take(1)))\n                x_sample, y_sample = sample_data\n                \n                predictions = self.model(x_sample, training=True)\n                loss = tf.keras.losses.sparse_categorical_crossentropy(y_sample, predictions)\n                loss = tf.reduce_mean(loss)\n            \n            # Compute gradients\n            gradients = tape.gradient(loss, self.model.trainable_variables)\n            \n            # Calculate gradient norm\n            grad_norm = tf.linalg.global_norm(gradients)\n            self.gradient_norms.append(float(grad_norm.numpy()))\n\ndef create_advanced_callbacks():\n    \"\"\"Create comprehensive callback system\"\"\"\n    \n    # Create logs directory\n    log_dir = f\"logs/cifar10_training_{int(time.time())}\"\n    os.makedirs(log_dir, exist_ok=True)\n    \n    callback_list = [\n        # Early Stopping\n        callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1,\n            mode='max'\n        ),\n        \n        # Learning Rate Reduction\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-7,\n            verbose=1,\n            mode='min'\n        ),\n        \n        # Model Checkpointing\n        callbacks.ModelCheckpoint(\n            filepath=f'{log_dir}/best_model.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=False,\n            verbose=1,\n            mode='max'\n        ),\n        \n        # TensorBoard Logging\n        callbacks.TensorBoard(\n            log_dir=log_dir,\n            histogram_freq=1,\n            write_graph=True,\n            write_images=True,\n            update_freq='epoch'\n        ),\n        \n        # Custom Metrics Callback\n        CustomMetricsCallback(val_dataset, class_names),\n        \n        # Gradient Monitoring\n        GradientMonitorCallback(monitor_frequency=10),\n        \n        # Learning Rate Scheduler\n        callbacks.LearningRateScheduler(\n            schedule=lambda epoch: 0.001 * 0.95 ** epoch,\n            verbose=0\n        )\n    ]\n    \n    return callback_list, log_dir\n\n# Create advanced training configuration\ndef create_training_system():\n    \"\"\"Set up comprehensive training system\"\"\"\n    \n    print(\"üöÄ Setting up Advanced TensorFlow Training System...\")\n    \n    # Compile model with advanced configuration\n    model.compile(\n        optimizer=optimizers.Adam(\n            learning_rate=0.001,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=1e-7\n        ),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=[\n            'accuracy',\n            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n    \n    print(\"‚úÖ Model compiled with advanced metrics\")\n    \n    # Create callbacks\n    callback_list, log_dir = create_advanced_callbacks()\n    \n    print(f\"‚úÖ Callbacks created, logs will be saved to: {log_dir}\")\n    \n    return callback_list, log_dir\n\n# Set up training system\ncallback_list, log_dir = create_training_system()\n\n# Display training configuration\nprint(\"\\nüìã Training Configuration:\")\nprint(\"=\" * 50)\nprint(f\"Optimizer: Adam (lr=0.001)\")\nprint(f\"Loss Function: Sparse Categorical Crossentropy\")\nprint(f\"Metrics: Accuracy, Top-3 Accuracy, Precision, Recall\")\nprint(f\"Callbacks: {len(callback_list)} advanced callbacks\")\nprint(f\"Data Augmentation: Enabled on training set\")\nprint(f\"Regularization: L2, Dropout, Batch Normalization\")\n\nprint(f\"\\nDataset Configuration:\")\nprint(f\"Training batches: {len(train_dataset)}\")\nprint(f\"Validation batches: {len(val_dataset)}\")\nprint(f\"Test batches: {len(test_dataset)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\n\n# Custom Training Loop Implementation (Alternative approach)\n@tf.function\ndef train_step(x, y):\n    \"\"\"Custom training step with tf.GradientTape\"\"\"\n    with tf.GradientTape() as tape:\n        predictions = model(x, training=True)\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n        loss = tf.reduce_mean(loss)\n        \n        # Add regularization losses\n        regularization_loss = tf.add_n([l for l in model.losses])\n        total_loss = loss + regularization_loss\n    \n    # Compute gradients\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    \n    # Apply gradients\n    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    # Calculate accuracy\n    accuracy = tf.keras.metrics.sparse_categorical_accuracy(y, predictions)\n    accuracy = tf.reduce_mean(accuracy)\n    \n    return total_loss, accuracy\n\n@tf.function\ndef val_step(x, y):\n    \"\"\"Validation step\"\"\"\n    predictions = model(x, training=False)\n    loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n    loss = tf.reduce_mean(loss)\n    \n    accuracy = tf.keras.metrics.sparse_categorical_accuracy(y, predictions)\n    accuracy = tf.reduce_mean(accuracy)\n    \n    return loss, accuracy\n\ndef custom_training_loop(epochs=5):\n    \"\"\"Demonstrate custom training loop with tf.GradientTape\"\"\"\n    \n    print(f\"\\nüîß Demonstrating Custom Training Loop ({epochs} epochs)...\")\n    \n    # Training metrics\n    train_loss_metric = tf.keras.metrics.Mean()\n    train_accuracy_metric = tf.keras.metrics.Mean()\n    val_loss_metric = tf.keras.metrics.Mean()\n    val_accuracy_metric = tf.keras.metrics.Mean()\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        \n        # Reset metrics\n        train_loss_metric.reset_states()\n        train_accuracy_metric.reset_states()\n        val_loss_metric.reset_states()\n        val_accuracy_metric.reset_states()\n        \n        # Training loop\n        for batch, (x, y) in enumerate(train_dataset):\n            loss, accuracy = train_step(x, y)\n            train_loss_metric.update_state(loss)\n            train_accuracy_metric.update_state(accuracy)\n            \n            if batch % 100 == 0:\n                print(f\"  Batch {batch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n        \n        # Validation loop\n        for x_val, y_val in val_dataset:\n            val_loss, val_accuracy = val_step(x_val, y_val)\n            val_loss_metric.update_state(val_loss)\n            val_accuracy_metric.update_state(val_accuracy)\n        \n        # Print epoch results\n        print(f\"  Train Loss: {train_loss_metric.result():.4f}\")\n        print(f\"  Train Accuracy: {train_accuracy_metric.result():.4f}\")\n        print(f\"  Val Loss: {val_loss_metric.result():.4f}\")\n        print(f\"  Val Accuracy: {val_accuracy_metric.result():.4f}\")\n\n# Demonstrate custom training loop\ncustom_training_loop(epochs=3)\n\nprint(\"\\n‚úÖ Advanced TensorFlow training system ready!\")\nprint(f\"üìä TensorBoard logs: tensorboard --logdir {log_dir}\")\nprint(f\"üöÄ Ready to train with model.fit() or custom training loops!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Complete TensorFlow Model Training and Analysis\n\n### Instructions:\n1. Train the complete TensorFlow deep neural network system\n2. Monitor training progress with TensorBoard and advanced metrics\n3. Analyze training dynamics and model performance\n4. Create comprehensive performance visualizations using TensorFlow tools"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete TensorFlow Model Training with Advanced Monitoring\n\nprint(\"üöÄ Starting Complete TensorFlow Deep Learning Training...\")\nprint(\"This demonstrates the full power of TensorFlow for deep learning!\")\nprint(\"=\"*70)\n\n# Training parameters\nEPOCHS = 30  # Reduced for demonstration, increase for better results\n\n# Start training with all advanced features\nprint(f\"Beginning training for {EPOCHS} epochs...\")\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=callback_list,\n    verbose=1\n)\n\nprint(\"\\n‚úÖ Training completed successfully!\")\n\n# Comprehensive Performance Analysis with TensorFlow\ndef analyze_tensorflow_training_results(history, model, test_dataset, class_names):\n    \"\"\"Comprehensive analysis of TensorFlow training results\"\"\"\n    \n    print(\"\\nüîç COMPREHENSIVE TENSORFLOW MODEL ANALYSIS\")\n    print(\"=\" * 60)\n    \n    # Extract training history\n    train_acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(1, len(train_acc) + 1)\n    \n    # Create comprehensive visualization\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Plot 1: Training and Validation Accuracy\n    axes[0, 0].plot(epochs_range, train_acc, 'bo-', label='Training Accuracy', linewidth=2)\n    axes[0, 0].plot(epochs_range, val_acc, 'ro-', label='Validation Accuracy', linewidth=2)\n    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Plot 2: Training and Validation Loss\n    axes[0, 1].plot(epochs_range, train_loss, 'bo-', label='Training Loss', linewidth=2)\n    axes[0, 1].plot(epochs_range, val_loss, 'ro-', label='Validation Loss', linewidth=2)\n    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Plot 3: Learning Rate Schedule (if available)\n    if 'lr' in history.history:\n        axes[0, 2].plot(epochs_range, history.history['lr'], 'go-', linewidth=2)\n        axes[0, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n        axes[0, 2].set_xlabel('Epoch')\n        axes[0, 2].set_ylabel('Learning Rate')\n        axes[0, 2].set_yscale('log')\n        axes[0, 2].grid(True, alpha=0.3)\n    else:\n        axes[0, 2].text(0.5, 0.5, 'Learning Rate\\nSchedule\\nNot Recorded', \n                       ha='center', va='center', transform=axes[0, 2].transAxes,\n                       fontsize=12)\n        axes[0, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    test_results = model.evaluate(test_dataset, verbose=0)\n    test_loss, test_accuracy = test_results[0], test_results[1]\n    \n    print(f\"üìä Final Test Results:\")\n    print(f\"   Test Loss: {test_loss:.4f}\")\n    print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n    \n    # Get predictions for confusion matrix\n    print(\"Generating predictions for detailed analysis...\")\n    y_pred = []\n    y_true = []\n    \n    for batch_x, batch_y in test_dataset:\n        predictions = model.predict(batch_x, verbose=0)\n        y_pred.extend(np.argmax(predictions, axis=1))\n        y_true.extend(batch_y.numpy().flatten())\n    \n    y_pred = np.array(y_pred)\n    y_true = np.array(y_true)\n    \n    # Plot 4: Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    im = axes[1, 0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n    \n    # Add colorbar\n    plt.colorbar(im, ax=axes[1, 0])\n    \n    # Add labels\n    tick_marks = np.arange(len(class_names))\n    axes[1, 0].set_xticks(tick_marks)\n    axes[1, 0].set_yticks(tick_marks)\n    axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[1, 0].set_yticklabels(class_names)\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            axes[1, 0].text(j, i, format(cm[i, j], 'd'),\n                          ha=\"center\", va=\"center\",\n                          color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    axes[1, 0].set_xlabel('Predicted Label')\n    axes[1, 0].set_ylabel('True Label')\n    \n    # Plot 5: Per-Class Accuracy\n    class_accuracy = []\n    for i in range(len(class_names)):\n        class_mask = (y_true == i)\n        if np.sum(class_mask) > 0:\n            acc = np.mean(y_pred[class_mask] == y_true[class_mask])\n            class_accuracy.append(acc)\n        else:\n            class_accuracy.append(0)\n    \n    bars = axes[1, 1].bar(class_names, class_accuracy, color=plt.cm.tab10(np.arange(10)))\n    axes[1, 1].set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Class')\n    axes[1, 1].set_ylabel('Accuracy')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    \n    # Add value labels on bars\n    for bar, acc in zip(bars, class_accuracy):\n        height = bar.get_height()\n        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{acc:.3f}', ha='center', va='bottom')\n    \n    # Plot 6: Training Summary Stats\n    axes[1, 2].axis('off')\n    \n    # Create summary statistics\n    final_train_acc = train_acc[-1]\n    final_val_acc = val_acc[-1]\n    best_val_acc = max(val_acc)\n    best_val_epoch = val_acc.index(best_val_acc) + 1\n    \n    summary_text = f\"\"\"\n    üèÜ TRAINING SUMMARY\n    \n    Final Training Accuracy: {final_train_acc:.1%}\n    Final Validation Accuracy: {final_val_acc:.1%}\n    Best Validation Accuracy: {best_val_acc:.1%}\n    Best Epoch: {best_val_epoch}\n    \n    Test Set Performance:\n    Test Accuracy: {test_accuracy:.1%}\n    Test Loss: {test_loss:.4f}\n    \n    Generalization Gap:\n    Train-Test: {abs(final_train_acc - test_accuracy):.1%}\n    \n    Model Complexity:\n    Parameters: {model.count_params():,}\n    Layers: {len(model.layers)}\n    \"\"\"\n    \n    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,\n                   fontsize=11, verticalalignment='top', fontfamily='monospace',\n                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.7))\n    \n    plt.suptitle('Complete TensorFlow Deep Learning Analysis', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Detailed Classification Report\n    print(\"\\nüìã DETAILED CLASSIFICATION REPORT:\")\n    print(\"-\" * 60)\n    report = classification_report(y_true, y_pred, target_names=class_names, \n                                 output_dict=True)\n    \n    # Print formatted report\n    print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support'}\")\n    print(\"-\" * 60)\n    \n    for class_name in class_names:\n        metrics = report[class_name]\n        print(f\"{class_name:<12} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n              f\"{metrics['f1-score']:<10.3f} {int(metrics['support'])}\")\n    \n    print(\"-\" * 60)\n    macro_avg = report['macro avg']\n    weighted_avg = report['weighted avg']\n    \n    print(f\"{'Macro Avg':<12} {macro_avg['precision']:<10.3f} {macro_avg['recall']:<10.3f} \"\n          f\"{macro_avg['f1-score']:<10.3f} {int(macro_avg['support'])}\")\n    print(f\"{'Weighted Avg':<12} {weighted_avg['precision']:<10.3f} {weighted_avg['recall']:<10.3f} \"\n          f\"{weighted_avg['f1-score']:<10.3f} {int(weighted_avg['support'])}\")\n    \n    return {\n        'test_accuracy': test_accuracy,\n        'test_loss': test_loss,\n        'predictions': y_pred,\n        'true_labels': y_true,\n        'classification_report': report,\n        'history': history.history\n    }\n\n# Perform comprehensive analysis\nanalysis_results = analyze_tensorflow_training_results(history, model, test_dataset, class_names)\n\n# Display TensorBoard information\nprint(f\"\\nüìä TensorBoard Analysis:\")\nprint(f\"   Launch TensorBoard with: tensorboard --logdir {log_dir}\")\nprint(f\"   Then open: http://localhost:6006\")\n\nprint(f\"\\n‚úÖ Complete TensorFlow training and analysis finished!\")\nprint(f\"üéØ Final Results Summary:\")\nprint(f\"   ‚Ä¢ Test Accuracy: {analysis_results['test_accuracy']:.1%}\")\nprint(f\"   ‚Ä¢ Training completed in {len(history.history['loss'])} epochs\")\nprint(f\"   ‚Ä¢ Model successfully demonstrates all TensorFlow capabilities!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5: Production Deployment with TensorFlow\n\n### Instructions:\n1. Prepare the TensorFlow model for production deployment\n2. Implement TensorFlow Serving and SavedModel format\n3. Create inference APIs and model monitoring systems\n4. Generate comprehensive project documentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Production-Ready TensorFlow Model Deployment\n\nclass TensorFlowProductionModel:\n    \"\"\"Production-ready wrapper for TensorFlow models\"\"\"\n    \n    def __init__(self, model, class_names, model_metadata=None):\n        self.model = model\n        self.class_names = class_names\n        self.model_metadata = model_metadata or self._create_metadata()\n        \n        print(\"üöÄ TensorFlow Production Model Initialized\")\n        self._print_model_info()\n    \n    def _create_metadata(self):\n        \"\"\"Create comprehensive model metadata\"\"\"\n        return {\n            'model_name': 'Advanced CIFAR-10 Classifier',\n            'model_version': '1.0.0',\n            'framework': 'TensorFlow',\n            'framework_version': tf.__version__,\n            'model_type': 'Image Classification CNN',\n            'dataset': 'CIFAR-10',\n            'input_shape': (32, 32, 3),\n            'output_classes': len(self.class_names),\n            'class_names': self.class_names,\n            'architecture_features': [\n                'Residual Connections', 'Batch Normalization', \n                'Dropout Regularization', 'Attention Mechanism',\n                'Global Average Pooling', 'L2 Regularization'\n            ],\n            'preprocessing': 'Pixel normalization to [0,1]',\n            'total_parameters': self.model.count_params(),\n            'created_date': time.strftime('%Y-%m-%d %H:%M:%S')\n        }\n    \n    def _print_model_info(self):\n        \"\"\"Print model information\"\"\"\n        print(\"\\nüìã Production Model Information:\")\n        print(\"=\" * 50)\n        print(f\"Model: {self.model_metadata['model_name']}\")\n        print(f\"Version: {self.model_metadata['model_version']}\")\n        print(f\"Framework: {self.model_metadata['framework']} {self.model_metadata['framework_version']}\")\n        print(f\"Parameters: {self.model_metadata['total_parameters']:,}\")\n        print(f\"Classes: {self.model_metadata['output_classes']}\")\n        print(\"=\" * 50)\n    \n    def save_for_production(self, save_path):\n        \"\"\"Save model in production formats\"\"\"\n        \n        print(f\"üíæ Saving production-ready model...\")\n        \n        # Create directory structure\n        os.makedirs(save_path, exist_ok=True)\n        \n        # 1. Save complete model in SavedModel format (TensorFlow Serving)\n        savedmodel_path = os.path.join(save_path, 'savedmodel')\n        tf.saved_model.save(self.model, savedmodel_path)\n        print(f\"‚úÖ SavedModel saved to: {savedmodel_path}\")\n        \n        # 2. Save model weights and architecture separately\n        weights_path = os.path.join(save_path, 'model_weights.h5')\n        self.model.save_weights(weights_path)\n        print(f\"‚úÖ Model weights saved to: {weights_path}\")\n        \n        # 3. Save complete model in H5 format\n        h5_path = os.path.join(save_path, 'complete_model.h5')\n        self.model.save(h5_path)\n        print(f\"‚úÖ Complete model saved to: {h5_path}\")\n        \n        # 4. Save model metadata\n        metadata_path = os.path.join(save_path, 'model_metadata.json')\n        import json\n        with open(metadata_path, 'w') as f:\n            json.dump(self.model_metadata, f, indent=2, default=str)\n        print(f\"‚úÖ Model metadata saved to: {metadata_path}\")\n        \n        # 5. Create model signature for TensorFlow Serving\n        self._create_serving_signature(savedmodel_path)\n        \n        # Calculate total size\n        total_size = sum(os.path.getsize(os.path.join(save_path, f)) \n                        for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f)))\n        total_size_mb = total_size / (1024 * 1024)\n        \n        print(f\"üìä Total model size: {total_size_mb:.2f} MB\")\n        \n        return {\n            'savedmodel_path': savedmodel_path,\n            'weights_path': weights_path,\n            'h5_path': h5_path,\n            'metadata_path': metadata_path,\n            'total_size_mb': total_size_mb\n        }\n    \n    def _create_serving_signature(self, savedmodel_path):\n        \"\"\"Create serving signature for TensorFlow Serving\"\"\"\n        \n        # Load the saved model and add signature\n        loaded_model = tf.saved_model.load(savedmodel_path)\n        \n        @tf.function\n        def serving_default(input_image):\n            # Ensure input is in correct format\n            input_image = tf.cast(input_image, tf.float32)\n            input_image = tf.ensure_shape(input_image, [None, 32, 32, 3])\n            \n            # Make prediction\n            predictions = self.model(input_image, training=False)\n            \n            return {\n                'predictions': predictions,\n                'predicted_class': tf.argmax(predictions, axis=1),\n                'confidence': tf.reduce_max(predictions, axis=1)\n            }\n        \n        # Save with signature\n        tf.saved_model.save(\n            self.model,\n            savedmodel_path,\n            signatures={'serving_default': serving_default}\n        )\n        \n        print(f\"‚úÖ TensorFlow Serving signature created\")\n    \n    @staticmethod\n    def load_production_model(save_path, model_format='savedmodel'):\n        \"\"\"Load production model from saved files\"\"\"\n        \n        if model_format == 'savedmodel':\n            model_path = os.path.join(save_path, 'savedmodel')\n            model = tf.saved_model.load(model_path)\n        elif model_format == 'h5':\n            model_path = os.path.join(save_path, 'complete_model.h5')\n            model = tf.keras.models.load_model(model_path)\n        else:\n            raise ValueError(f\"Unsupported format: {model_format}\")\n        \n        # Load metadata\n        metadata_path = os.path.join(save_path, 'model_metadata.json')\n        import json\n        with open(metadata_path, 'r') as f:\n            metadata = json.load(f)\n        \n        print(f\"‚úÖ Model loaded from: {model_path}\")\n        return model, metadata\n    \n    def predict_single(self, image, return_top_k=5):\n        \"\"\"Make prediction on single image with detailed output\"\"\"\n        \n        # Ensure image is in correct format\n        if len(image.shape) == 3:\n            image = tf.expand_dims(image, 0)\n        \n        # Normalize if needed\n        if tf.reduce_max(image) > 1.0:\n            image = tf.cast(image, tf.float32) / 255.0\n        \n        # Make prediction\n        predictions = self.model(image, training=False)\n        probabilities = tf.nn.softmax(predictions).numpy()[0]\n        \n        # Get top-k predictions\n        top_indices = np.argsort(probabilities)[::-1][:return_top_k]\n        \n        result = {\n            'predicted_class': top_indices[0],\n            'predicted_label': self.class_names[top_indices[0]],\n            'confidence': float(probabilities[top_indices[0]]),\n            'top_predictions': [\n                {\n                    'class': int(idx),\n                    'label': self.class_names[idx],\n                    'probability': float(probabilities[idx])\n                }\n                for idx in top_indices\n            ]\n        }\n        \n        return result\n    \n    def batch_predict(self, images, batch_size=32):\n        \"\"\"Efficient batch prediction\"\"\"\n        \n        # Create dataset for efficient batch processing\n        dataset = tf.data.Dataset.from_tensor_slices(images)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n        predictions = []\n        for batch in dataset:\n            batch_preds = self.model(batch, training=False)\n            predictions.append(batch_preds.numpy())\n        \n        return np.concatenate(predictions, axis=0)\n    \n    def evaluate_performance(self, test_dataset):\n        \"\"\"Comprehensive performance evaluation\"\"\"\n        \n        print(\"üìä Evaluating production model performance...\")\n        \n        # Evaluate using TensorFlow's built-in evaluate\n        results = self.model.evaluate(test_dataset, verbose=0)\n        \n        # Get detailed metrics\n        test_loss = results[0]\n        test_accuracy = results[1] if len(results) > 1 else None\n        \n        # Performance metrics\n        start_time = time.time()\n        sample_count = 0\n        \n        # Time inference on test set\n        for batch_x, batch_y in test_dataset.take(10):\n            _ = self.model.predict(batch_x, verbose=0)\n            sample_count += len(batch_x)\n        \n        inference_time = time.time() - start_time\n        throughput = sample_count / inference_time\n        \n        performance_metrics = {\n            'test_loss': float(test_loss),\n            'test_accuracy': float(test_accuracy) if test_accuracy else None,\n            'inference_throughput': throughput,\n            'avg_inference_time_per_sample': inference_time / sample_count,\n            'model_size_mb': self.model_metadata['total_parameters'] * 4 / 1024 / 1024\n        }\n        \n        print(f\"   Test Accuracy: {performance_metrics['test_accuracy']:.1%}\")\n        print(f\"   Inference Throughput: {performance_metrics['inference_throughput']:.1f} samples/sec\")\n        print(f\"   Avg Inference Time: {performance_metrics['avg_inference_time_per_sample']*1000:.1f} ms/sample\")\n        \n        return performance_metrics\n\n# Create production model\nprint(\"üè≠ Creating Production-Ready TensorFlow Model...\")\n\nproduction_model = TensorFlowProductionModel(\n    model=model,\n    class_names=class_names,\n    model_metadata={\n        'test_accuracy': analysis_results['test_accuracy'],\n        'training_epochs': len(history.history['loss']),\n        'best_val_accuracy': max(history.history['val_accuracy'])\n    }\n)\n\n# Save model for production\nsave_paths = production_model.save_for_production('production_model')\n\n# Test production model functionality\nprint(\"\\nüß™ Testing Production Model Functionality...\")\n\n# Test single prediction\nsample_batch = next(iter(test_dataset.take(1)))\nsample_image = sample_batch[0][0]\nsample_label = sample_batch[1][0].numpy()\n\nprint(f\"\\nTesting single prediction...\")\nprint(f\"True label: {class_names[sample_label]}\")\n\nprediction_result = production_model.predict_single(sample_image, return_top_k=3)\n\nprint(f\"Predicted: {prediction_result['predicted_label']} (confidence: {prediction_result['confidence']:.3f})\")\nprint(f\"Top 3 predictions:\")\nfor pred in prediction_result['top_predictions']:\n    print(f\"  {pred['label']}: {pred['probability']:.3f}\")\n\n# Test batch prediction efficiency\nprint(f\"\\nüöÄ Testing batch prediction performance...\")\nsample_images = sample_batch[0][:8]  # Test with 8 images\n\nstart_time = time.time()\nbatch_predictions = production_model.batch_predict(sample_images)\nbatch_time = time.time() - start_time\n\nprint(f\"Batch prediction time: {batch_time:.3f}s for {len(sample_images)} images\")\nprint(f\"Throughput: {len(sample_images)/batch_time:.1f} images/sec\")\n\n# Evaluate production model performance\nperformance_metrics = production_model.evaluate_performance(test_dataset)\n\n# Test model loading (to verify saves worked correctly)\nprint(f\"\\nüîÑ Testing Model Loading...\")\ntry:\n    loaded_model, loaded_metadata = TensorFlowProductionModel.load_production_model(\n        'production_model', model_format='h5'\n    )\n    print(f\"‚úÖ Model loaded successfully!\")\n    print(f\"   Loaded model version: {loaded_metadata.get('model_version', 'Unknown')}\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n\nprint(f\"\\n‚úÖ Production TensorFlow model created and tested successfully!\")\nprint(f\"üöÄ Model ready for deployment with TensorFlow Serving!\")\n\n# Production deployment instructions\ndeployment_instructions = f\"\"\"\nüöÄ TENSORFLOW PRODUCTION DEPLOYMENT INSTRUCTIONS\n===============================================\n\n1. TensorFlow Serving Deployment:\n   docker run -p 8501:8501 \\\\\n     --mount type=bind,source={os.path.abspath(save_paths['savedmodel_path'])},target=/models/cifar10/1 \\\\\n     -e MODEL_NAME=cifar10 -t tensorflow/serving\n\n2. REST API Endpoint:\n   POST http://localhost:8501/v1/models/cifar10:predict\n   \n3. Model Monitoring:\n   - Monitor inference latency and throughput\n   - Track prediction confidence distributions  \n   - Log model performance metrics\n   - Set up alerts for model drift\n\n4. Model Versioning:\n   - Use model version directories (1/, 2/, etc.)\n   - Implement A/B testing between versions\n   - Maintain rollback capabilities\n\n5. Scaling Considerations:\n   - Use TensorFlow Serving with load balancer\n   - Consider TensorFlow Lite for mobile deployment\n   - Implement batch prediction endpoints for efficiency\n\"\"\"\n\nprint(deployment_instructions)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéâ Lab Complete! Comprehensive TensorFlow Deep Learning Mastery Achieved!\n\n### What You've Accomplished - A Complete TensorFlow Production System:\n\n#### üèóÔ∏è **Advanced TensorFlow Architecture**:\n‚úÖ **Custom Model Subclassing**: Built advanced models with tf.keras.Model  \n‚úÖ **Custom Layers**: Implemented ResidualBlock and AttentionBlock layers  \n‚úÖ **Modern Techniques**: Batch normalization, dropout, residual connections  \n‚úÖ **TensorFlow Best Practices**: Proper initialization, regularization, and optimization  \n\n#### üöÄ **TensorFlow Training Excellence**:\n‚úÖ **Advanced Callbacks**: Custom metrics, gradient monitoring, learning rate scheduling  \n‚úÖ **TensorBoard Integration**: Comprehensive training visualization and monitoring  \n‚úÖ **Custom Training Loops**: tf.GradientTape for maximum control and flexibility  \n‚úÖ **Data Pipeline Optimization**: tf.data for efficient training and augmentation  \n\n#### üìä **Outstanding Performance with TensorFlow**:\n‚úÖ **High Accuracy**: Achieved excellent classification performance on CIFAR-10  \n‚úÖ **Stable Training**: Smooth convergence with proper regularization  \n‚úÖ **Production Quality**: Professional-grade model performance and reliability  \n‚úÖ **Comprehensive Analysis**: Detailed performance metrics and visualizations  \n\n#### üíº **Production Deployment Mastery**:\n‚úÖ **TensorFlow Serving**: SavedModel format with serving signatures  \n‚úÖ **Multiple Export Formats**: H5, SavedModel, weights-only for different use cases  \n‚úÖ **Inference Optimization**: Batch processing and performance monitoring  \n‚úÖ **Production APIs**: Ready for deployment with REST/gRPC endpoints  \n\n### üéØ Key TensorFlow Technical Achievements:\n\n#### **TensorFlow Ecosystem Mastery**:\n- üî• **tf.data**: Efficient data pipelines with augmentation and caching\n- üî• **tf.keras**: Advanced model building with subclassing and custom layers  \n- üî• **tf.GradientTape**: Custom training loops with full gradient control\n- üî• **tf.saved_model**: Production deployment with TensorFlow Serving\n- üî• **tf.function**: Performance optimization with graph compilation\n\n#### **Advanced Deep Learning Techniques**:\n- ‚ö° **Residual Connections**: Skip connections for better gradient flow\n- ‚ö° **Attention Mechanisms**: Spatial attention for feature enhancement  \n- ‚ö° **Batch Normalization**: Training stability and faster convergence\n- ‚ö° **Advanced Regularization**: L2, dropout, and architectural constraints\n\n#### **Professional Training System**:\n- üè≠ **Custom Callbacks**: Advanced monitoring and training control\n- üè≠ **TensorBoard Logging**: Comprehensive training visualization\n- üè≠ **Performance Optimization**: GPU utilization and memory efficiency\n- üè≠ **Production Monitoring**: Inference performance and model quality tracking\n\n### üåü Real-World TensorFlow Applications:\n\n#### **Computer Vision Systems**:\n- üéØ **Image Classification**: Production-ready CIFAR-10 classifier\n- üéØ **Medical Imaging**: Diagnostic systems with attention mechanisms\n- üéØ **Autonomous Vehicles**: Real-time object detection and classification\n- üéØ **Quality Control**: Automated defect detection in manufacturing\n\n#### **Industry-Grade Capabilities**:\n- üí° **Scalable Architecture**: Ready for large-scale deployment\n- üí° **Model Versioning**: A/B testing and rollback capabilities\n- üí° **Performance Monitoring**: Production-ready metrics and alerting\n- üí° **API Integration**: REST/gRPC endpoints for web and mobile applications\n\n### üöÄ Advanced TensorFlow Skills Demonstrated:\n\n```python\nTENSORFLOW_MASTERY = {\n    'model_building': {\n        'subclassing': 'EXPERT',\n        'custom_layers': 'ADVANCED', \n        'architectures': 'PROFESSIONAL'\n    },\n    'training_systems': {\n        'callbacks': 'EXPERT',\n        'custom_loops': 'ADVANCED',\n        'monitoring': 'PROFESSIONAL'\n    },\n    'production_deployment': {\n        'serving': 'EXPERT',\n        'optimization': 'ADVANCED',\n        'monitoring': 'PROFESSIONAL'\n    },\n    'tensorflow_ecosystem': {\n        'tf_data': 'EXPERT',\n        'tf_function': 'ADVANCED', \n        'tensorboard': 'PROFESSIONAL'\n    }\n}\n```\n\n### üéì What Makes This Achievement Exceptional:\n\n#### **Complete TensorFlow Ecosystem Coverage**:\n- üìö **Data Pipelines**: tf.data for production-grade data processing\n- üìö **Model Development**: Advanced architectures with custom components\n- üìö **Training Systems**: Professional-grade training with monitoring\n- üìö **Production Deployment**: TensorFlow Serving with full MLOps pipeline\n\n#### **Industry-Ready Skills**:\n- üî¨ **Research Capabilities**: Custom layers and experimental architectures\n- üè≠ **Production Engineering**: Scalable deployment and monitoring systems  \n- üìä **Performance Optimization**: Inference speed and resource efficiency\n- üíº **Business Impact**: End-to-end solution from research to production\n\n### üåü Your TensorFlow Journey Achievements:\n\n#### **Technical Mastery Progression**:\n1. **Foundations** ‚Üí TensorFlow basics and Keras APIs ‚úÖ\n2. **Architecture Design** ‚Üí Custom models and advanced layers ‚úÖ  \n3. **Training Systems** ‚Üí Advanced callbacks and monitoring ‚úÖ\n4. **Production Deployment** ‚Üí TensorFlow Serving and optimization ‚úÖ\n5. **Complete System** ‚Üí End-to-end MLOps with TensorFlow ‚úÖ\n\n#### **Professional Capabilities Unlocked**:\n- üéØ **Lead TensorFlow projects** in production environments\n- üéØ **Design custom architectures** for specific business problems\n- üéØ **Implement MLOps pipelines** with TensorFlow ecosystem\n- üéØ **Optimize model performance** for real-world constraints\n\n### üèÜ Final Project Excellence Summary:\n\n**You've successfully built a complete, production-ready deep learning system using TensorFlow that demonstrates:**\n\n- ‚ú® **Advanced Architecture Design** with custom layers and modern techniques\n- ‚ú® **Professional Training Systems** with comprehensive monitoring\n- ‚ú® **Production Deployment Excellence** with TensorFlow Serving\n- ‚ú® **Industry-Grade Performance** with optimization and scalability\n- ‚ú® **Complete MLOps Integration** from research to production\n\n### üöÄ Ready for TensorFlow Leadership!\n\n**You now possess the complete skill set to:**\n- üåü **Lead TensorFlow initiatives** in enterprise environments\n- üåü **Architect scalable ML systems** using TensorFlow ecosystem\n- üåü **Mentor teams** on TensorFlow best practices and advanced techniques\n- üåü **Drive innovation** with cutting-edge TensorFlow applications\n\n**Congratulations on achieving TensorFlow Deep Learning Mastery!** You're now equipped to build world-class AI systems that can handle real-world challenges at enterprise scale! üéâüöÄ\n\n---\n\n### üéØ **Your Next Challenge**: \nApply this comprehensive TensorFlow expertise to solve real-world problems in computer vision, NLP, or other domains. The foundation you've built here will serve as the launching pad for your next breakthrough in AI! ‚ú®"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}